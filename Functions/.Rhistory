Data[index,]$Response,
lag.max = length(Data[index,]$Response),
na.action = na.pass,
plot = FALSE
)
acf_Confidence <-
acf(
Data[index,]$Confidence,
lag.max = length(Data[index,]$Confidence),
na.action = na.pass,
plot = FALSE
)
Data$acf_History[index] = acf_History$acf
Data$acf_Stimulus[index] = acf_Stimulus$acf
Data$acf_Perception[index] = acf_Perception$acf
Data$acf_Confidence[index] = acf_Confidence$acf
if (sum(!is.na(Data[index,]$cRT)) > 0) {
acf_RT <-
acf(
Data[index,]$cRT,
lag.max = length(Data[index,]$cRT),
na.action = na.pass,
plot = FALSE
)
Data$acf_RT[index] = acf_RT$acf
} else {
Data$acf_RT[index] <- rep(NA, length(Data[index,]$cRT))
}
if (sum(!is.na(Data[index,]$cDifficulty)) > 0) {
acf_Difficulty <-
acf(
Data[index,]$cDifficulty,
lag.max = length(Data[index,]$cDifficulty),
na.action = na.pass,
plot = FALSE
)
Data$acf_Difficulty[index] = acf_Difficulty$acf
} else {
Data$acf_Difficulty[index] <- rep(NA, length(Data[index,]$cDifficulty))
}
if (sum(!is.na(Data[index,]$cCondition)) > 0) {
acf_Condition <-
acf(
Data[index,]$cCondition,
lag.max = length(Data[index,]$cCondition),
na.action = na.pass,
plot = FALSE
)
Data$acf_Condition[index] = acf_Condition$acf
} else {
Data$acf_Condition[index] <- rep(NA, length(Data[index,]$cCondition))
}
Permutations = data.frame()
for (perm_idx in c(1:n_permutations)){
addPerm = data.frame()
random_acf_Stimulus <-
acf(
sample(Data[index,]$Accuracy),
lag.max = length(Data[index,]$Accuracy),
na.action = na.pass,
plot = FALSE
)
random_acf_History <-
acf(
sample(Data[index,]$History),
lag.max = length(Data[index,]$History),
na.action = na.pass,
plot = FALSE
)
random_acf_Perception <-
acf(
sample(Data[index,]$Response),
lag.max = length(Data[index,]$Response),
na.action = na.pass,
plot = FALSE
)
random_acf_Confidence <-
acf(
sample(Data[index,]$Confidence),
lag.max = length(Data[index,]$Confidence),
na.action = na.pass,
plot = FALSE
)
addPerm = data.frame(
random_acf_Stimulus = random_acf_Stimulus$acf,
random_acf_History = random_acf_History$acf,
random_acf_Perception = random_acf_Perception$acf,
random_acf_Confidence = random_acf_Confidence$acf,
exceed_acf_Stimulus = as.integer(Data$acf_Stimulus[index] < random_acf_Stimulus$acf),
exceed_acf_History = as.integer(Data$acf_History[index] < random_acf_History$acf),
exceed_acf_Perception = as.integer(Data$acf_Perception[index] < random_acf_Perception$acf),
exceed_acf_Confidence = as.integer(Data$acf_Confidence[index] < random_acf_Confidence$acf)
)
if (sum(!is.na(Data[index,]$cRT)) > 0) {
random_acf_RT <-
acf(
sample(Data[index,]$cRT),
lag.max = length(Data[index,]$cRT),
na.action = na.pass,
plot = FALSE
)
addPerm$random_acf_RT = random_acf_RT$acf
addPerm$exceed_acf_RT = as.integer(Data$acf_RT[index] < random_acf_RT$acf)
} else {
addPerm$random_acf_RT = rep(NA, length(random_acf_Stimulus$acf))
addPerm$exceed_acf_RT = rep(NA, length(random_acf_Stimulus$acf))
}
if (sum(!is.na(Data[index,]$cDifficulty)) > 0) {
random_acf_Difficulty <-
acf(
sample(Data[index,]$cDifficulty),
lag.max = length(Data[index,]$cDifficulty),
na.action = na.pass,
plot = FALSE
)
addPerm$random_acf_Difficulty = random_acf_Difficulty$acf
addPerm$exceed_acf_Difficulty = as.integer(Data$acf_Difficulty[index] < random_acf_Difficulty$acf)
} else {
addPerm$random_acf_Difficulty = rep(NA, length(random_acf_Stimulus$acf))
addPerm$exceed_acf_Difficulty = rep(NA, length(random_acf_Stimulus$acf))
}
if (sum(!is.na(Data[index,]$cCondition)) > 0) {
random_acf_Condition <-
acf(
sample(Data[index,]$cCondition),
lag.max = length(Data[index,]$cCondition),
na.action = na.pass,
plot = FALSE
)
addPerm$random_acf_Condition = random_acf_Condition$acf
addPerm$exceed_acf_Condition = as.integer(Data$acf_Condition[index] < random_acf_Condition$acf)
} else {
addPerm$random_acf_Condition = rep(NA, length(random_acf_Stimulus$acf))
addPerm$exceed_acf_Condition = rep(NA, length(random_acf_Stimulus$acf))
}
addPerm$perm_idx = rep(perm_idx, length(random_acf_Stimulus$acf))
addPerm$Trial = c(1:nrow(addPerm))
Permutations = rbind(Permutations, addPerm)
}
Summ_Permutations <-  ddply(
Permutations[, ],
.(Trial),
summarise,
mean_random_acf_Stimulus = mean(random_acf_Stimulus, na.rm = TRUE),
mean_random_acf_History = mean(random_acf_History, na.rm = TRUE),
mean_random_acf_Perception = mean(random_acf_Perception, na.rm = TRUE),
mean_random_acf_Confidence = mean(random_acf_Confidence, na.rm = TRUE),
mean_random_acf_RT = mean(random_acf_RT, na.rm = TRUE),
mean_random_acf_Difficulty = mean(random_acf_Difficulty, na.rm = TRUE),
mean_random_acf_Condition = mean(random_acf_Condition, na.rm = TRUE),
sum_exceed_acf_Stimulus = sum(exceed_acf_Stimulus, na.rm = TRUE),
sum_exceed_acf_History = sum(exceed_acf_History, na.rm = TRUE),
sum_exceed_acf_Perception = sum(exceed_acf_Perception, na.rm = TRUE),
sum_exceed_acf_Confidence = sum(exceed_acf_Confidence, na.rm = TRUE),
sum_exceed_acf_RT = sum(exceed_acf_RT, na.rm = TRUE),
sum_exceed_acf_Difficulty = sum(exceed_acf_Difficulty, na.rm = TRUE),
sum_exceed_acf_Condition = sum(exceed_acf_Condition, na.rm = TRUE)
)
Data$random_acf_History[index] = Summ_Permutations$mean_random_acf_History
Data$random_acf_Stimulus[index] = Summ_Permutations$mean_random_acf_Stimulus
Data$random_acf_Perception[index] = Summ_Permutations$mean_random_acf_Perception
Data$random_acf_Confidence[index] = Summ_Permutations$mean_random_acf_Confidence
Data$random_acf_RT[index] = Summ_Permutations$mean_random_acf_RT
Data$random_acf_Difficulty[index] = Summ_Permutations$mean_random_acf_Difficulty
Data$random_acf_Condition[index] = Summ_Permutations$mean_random_acf_Condition
Data$exceed_acf_History[index] = Summ_Permutations$sum_exceed_acf_History
Data$exceed_acf_Stimulus[index] = Summ_Permutations$sum_exceed_acf_Stimulus
Data$exceed_acf_Perception[index] = Summ_Permutations$sum_exceed_acf_Perception
Data$exceed_acf_Confidence[index] = Summ_Permutations$sum_exceed_acf_Confidence
Data$exceed_acf_RT[index] = Summ_Permutations$sum_exceed_acf_RT
Data$exceed_acf_Difficulty[index] = Summ_Permutations$sum_exceed_acf_Difficulty
Data$exceed_acf_Condition[index] = Summ_Permutations$sum_exceed_acf_Condition
## collect participant data
subject_counter = subject_counter + 1
add_PwData = data.frame(
study_id = rep(study_id, sum(index)),
study_type = rep(Experiments$Category[study_id], sum(index)),
response_type = rep(
checkBinaryTrait(Data$Response, naVal = "NA"),
sum(index)
),
subject_id = rep(subject_counter, sum(index)),
Trial = Data$Trial[index],
Stimulus = Data$Stimulus[index],
Response = Data$Response[index],
Response_minus_1 = Data$Response_minus_1[index],
Accuracy = Data$Accuracy[index],
History = Data$History[index],
Confidence = Data$Confidence[index],
RT = Data$cRT[index],
Difficulty = Data$cDifficulty[index],
Condition = Data$cCondition[index],
acf_Stimulus = Data$acf_Stimulus[index],
acf_History = Data$acf_History[index],
acf_Perception = Data$acf_Perception[index],
acf_Confidence = Data$acf_Confidence[index],
acf_RT = Data$acf_RT[index],
acf_Difficulty = Data$acf_Difficulty[index],
acf_Condition = Data$acf_Condition[index],
random_acf_Stimulus = Data$random_acf_Stimulus[index],
random_acf_History = Data$random_acf_History[index],
random_acf_Perception = Data$random_acf_Perception[index],
random_acf_Confidence = Data$random_acf_Confidence[index],
random_acf_RT = Data$random_acf_RT[index],
random_acf_Difficulty = Data$random_acf_Difficulty[index],
random_acf_Condition = Data$random_acf_Condition[index],
exceed_acf_Stimulus = Data$exceed_acf_Stimulus[index],
exceed_acf_History = Data$exceed_acf_History[index],
exceed_acf_Perception = Data$exceed_acf_Perception[index],
exceed_acf_Confidence = Data$exceed_acf_Confidence[index],
exceed_acf_RT = Data$exceed_acf_RT[index],
exceed_acf_Difficulty = Data$exceed_acf_Difficulty[index],
exceed_acf_Condition = Data$exceed_acf_Condition[index]
)
PwData <- rbind(PwData, add_PwData)
}
},
error = function(e){print("error")}
)
}
}
if (additional_autocorrelations) {
for (id in unique(PwData$subject_id)) {
index = PwData$subject_id == id
print(id)
# get trial numbers
PwData$HardEasy[index] <- round((sign(PwData$Difficulty[index]) + 1)/2)
# get autocorrelations
acf_HardEasy <-
acf(
PwData[index,]$HardEasy,
lag.max = length(PwData[index,]$HardEasy),
na.action = na.pass,
plot = FALSE
)
PwData$acf_HardEasy[index] <-  acf_HardEasy$acf
acf_External <-
acf(
PwData[index,]$Stimulus,
lag.max = length(PwData[index,]$Stimulus),
na.action = na.pass,
plot = FALSE
)
PwData$acf_External[index] <-  acf_External$acf
Permutations = data.frame()
for (perm_idx in c(1:n_permutations)){
addPerm = data.frame()
random_acf_HardEasy <-
acf(
sample(PwData[index,]$HardEasy),
lag.max = length(PwData[index,]$HardEasy),
na.action = na.pass,
plot = FALSE
)
random_acf_External <-
acf(
sample(PwData[index,]$Stimulus),
lag.max = length(PwData[index,]$Stimulus),
na.action = na.pass,
plot = FALSE
)
addPerm = data.frame(
random_acf_HardEasy = random_acf_HardEasy$acf,
exceed_acf_HardEasy = as.integer(PwData$acf_HardEasy[index] < random_acf_HardEasy$acf),
random_acf_External = random_acf_External$acf,
exceed_acf_External = as.integer(PwData$acf_External[index] < random_acf_External$acf)
)
addPerm$perm_idx = rep(perm_idx, length(random_acf_HardEasy$acf))
addPerm$Trial = c(1:nrow(addPerm))
Permutations = rbind(Permutations, addPerm)
}
Summ_Permutations <-  ddply(
Permutations[, ],
.(Trial),
summarise,
mean_random_acf_HardEasy = mean(random_acf_HardEasy, na.rm = TRUE),
sum_exceed_acf_HardEasy = sum(exceed_acf_HardEasy, na.rm = TRUE),
mean_random_acf_External = mean(random_acf_External, na.rm = TRUE),
sum_exceed_acf_External = sum(exceed_acf_External, na.rm = TRUE)
)
PwData$random_acf_HardEasy[index] = Summ_Permutations$mean_random_acf_HardEasy
PwData$exceed_acf_HardEasy[index] = Summ_Permutations$sum_exceed_acf_HardEasy
PwData$random_acf_External[index] = Summ_Permutations$mean_random_acf_External
PwData$exceed_acf_External[index] = Summ_Permutations$sum_exceed_acf_External
}
}
source("./Functions/f_compute_slider.R", local = knitr::knit_global())
## prepare data
if (compute_slider) {
library(ppcor)
for (id in unique(PwData$subject_id)) {
# for (id in c(1:1000)) {
index = PwData$subject_id == id
print(id)
PwData$History_slider[index] <- f_compute_slider(PwData$History[index], sliding_window)
PwData$Accuracy_slider[index] <- f_compute_slider(PwData$Accuracy[index], sliding_window)
PwData$RT_slider[index] <- f_compute_slider(PwData$RT[index], sliding_window)
PwData$Confidence_slider[index] <- f_compute_slider(PwData$Confidence[index], sliding_window)
}
}
if (load_data) {
#PwData <- read.csv(paste(root, "PwData_full_new_preproc_plus_slider_normalized_hardeasy_external.csv", sep = ""))
PwData <- read.csv(paste(root, "PwData_full_new_preproc_plus_slider_normalized_hardeasy_external_stimulus_history.csv", sep = ""))
}
##
## Add Stimulus History
##
# PwData$Stimulus_History = rep(NA, nrow(PwData))
# for (subject_id in unique(PwData$subject_id)) {
#
#       index = PwData$subject_id == subject_id
#       Data = PwData[index,]
#       Data$Stimulus_minus_1 <- c(NA, Data$Stimulus[1:nrow(Data) - 1])
#       Data$Stimulus_History = as.numeric(Data$Stimulus == Data$Stimulus_minus_1)
#
#       PwData[index]$Stimulus_History = Data$Stimulus_History
# }
## only evaluate perceptual studies with binary perceptual responses
PwData <- PwData[PwData$study_type == "Perception" & PwData$response_type == "bin",]
PwData[PwData == "NaN"] = NA
library(lme4)
library(afex)
Behav <-
ddply(
PwData,
.(subject_id, study_id),
summarise,
History = sum(History, na.rm = TRUE)/length(History)*100,
Min_History = min(History_slider, na.rm = TRUE)*100,
Max_History = max(History_slider, na.rm = TRUE)*100,
Accuracy = sum(Accuracy, na.rm = TRUE)/length(Accuracy)*100,
Min_Accuracy = min(Accuracy_slider, na.rm = TRUE)*100,
Max_Accuracy = max(Accuracy_slider, na.rm = TRUE)*100,
Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100
)
Behav_diff <-
ddply(
PwData[!is.na(PwData$Accuracy),],
.(subject_id, study_id, Accuracy),
summarise,
History = sum(History, na.rm = TRUE)/length(History)*100
)
##
## exclude studies with stimulus-congruence below chance level (unclear response labels?)
##
Study_Behav <-
ddply(Behav,
.(study_id),
summarise,
mean_Accuracy = mean(Accuracy, na.rm = TRUE),
mean_History = mean(History, na.rm = TRUE),
error_Accuracy = sd(Accuracy, na.rm = TRUE)/sqrt(length(Accuracy)),
error_History = sd(History, na.rm = TRUE)/sqrt(length(History)),
mean_id = mean(subject_id)
)
exclusion_list = Study_Behav[Study_Behav$mean_Accuracy < 50,]$study_id
Behav[Behav$study_id %in% exclusion_list,] = NA
Study_Behav[Study_Behav$study_id %in% exclusion_list,] = NA
Behav$study_id <- as.factor(Behav$study_id)
'%ni%' <- Negate('%in%')
PwData <- PwData[PwData$study_id %ni% exclusion_list,]
'%ni%' <- Negate('%in%')
PwData <- PwData[PwData$study_id %ni% exclusion_list,]
##
## STATS on History-congruent perception
##
library(lme4)
library(afex)
Behav$null_History = Behav$History - 50
Global_History_Accuracy <- lmer(null_History ~ Accuracy + (1|study_id), data = Behav)
true_Global_History_Accuracy <- lmer(History ~ Accuracy + (1|study_id), data = Behav)
STAT.Global_History_Accuracy = summary(Global_History_Accuracy)
diff_History_Accuracy <- lmer(History ~ Accuracy + (1|study_id), data = Behav_diff)
STAT.diff_History_Accuracy = summary(diff_History_Accuracy)
install.packages(rBayesianOptimization)
install.packages("rBayesianOptimization")
id = min(PwData$subject_id)
Input_Data = PwData[PwData$subject_id == id,c(6,7,12)]
list(H = c(0.01,0.99), prec = c(0,5), amp = (0,10), amp_LLR(0,10), frequency = c(1/40, 1/20), phase = (0, 2*pi - pi/10000) )
list(H = c(0.01,0.99), prec = c(0,5), amp = (0,10), amp_LLR(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000) )
list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000) )
fit_glaze_osc_Bayes(H, prec, amp, amp_LLR, frequency, phase, Input_Data),
source("./Functions/fit_glaze_osc_Bayes.R", local = knitr::knit_global())
add_O_Bayes = BayesianOptimization(
fit_glaze_osc_Bayes(H, prec, amp, amp_LLR, frequency, phase, Input_Data),
bounds = list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000) )
)
library(rBayesianOptimization)
add_O_Bayes = BayesianOptimization(
fit_glaze_osc_Bayes(H, prec, amp, amp_LLR, frequency, phase, Input_Data),
bounds = list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000) )
)
add_O_Bayes = BayesianOptimization(
fit_glaze_osc_Bayes(H, prec, amp, amp_LLR, frequency, phase, Input_Data),
bounds = list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000), n_iter = 20)
)
setwd("~/Public/Git/private/InProgress/Modes/Functions")
debugSource('~/Public/Git/private/InProgress/Modes/Functions/fit_glaze_osc_Bayes.R')
add_O_Bayes = BayesianOptimization(
fit_glaze_osc_Bayes(H, prec, amp, amp_LLR, frequency, phase, Input_Data),
bounds = list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000), n_iter = 20)
)
add_O_Bayes = BayesianOptimization(
fit_glaze_osc_Bayes,
bounds = list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000), n_iter = 2 )
)
bounds = list(H = c(0.01,0.99), prec = c(0,5), amp = c(0,10), amp_LLR = c(0,10), frequency = c(1/40, 1/20), phase = c(0, 2*pi - pi/10000)
y
?optimx
knitr::opts_chunk$set(echo = FALSE,
message = FALSE,
warning = FALSE)
options(scipen = -1, digits = 2)
## Global settings: what to do in R markdown (compute primary statistics vs. load data from disc)
## preprocessing / data collection
##
## ROOT
##
#root = "/home/veithweilnhammer/Desktop/Modes Data/"
root = "E:/Modes Data/"
#root = "/media/veithweilnhammer/My Passport/Modes Data/"
##
## Human Data
##
collect_data = FALSE
n_permutations = 100
compute_slider = FALSE
sliding_window = 10
additional_autocorrelations = FALSE
## load data
load_data = TRUE
## Compute (TRUE) or load (FALSE) time-comsuming data analyses
compute_logreg = FALSE
compute_Tw_LogReg = FALSE
compute_sine_wave_fit = FALSE
compute_power_spectra = FALSE
#compute_autocorr_corr = FALSE
#compute_fft = FALSE
compute_group_acf = FALSE
compute_RT_Conf_Accuracy_History = FALSE
compute_training_history = FALSE
##
## Mouse data
##
preprocess_mouse_data = FALSE
compute_slider_mouse_data = FALSE
filter_mouse_data = FALSE
compute_pretraining_data_mouse = FALSE
load_mouse_data = TRUE
apply_mouse_exclusion_criteria = FALSE
compute_mouse_group_acf = FALSE
compute_mouse_power_spectra = FALSE
compute_mouse_logreg = FALSE
compute_slider_Histora_Accuracy_lmer = FALSE
compute_mouse_Tw_LogReg = FALSE
mouse_compute_RT_Accuracy_History = FALSE
##
## Optimization
##
run_optim_human = TRUE
run_optim_mouse = TRUE
## generate output from fitted data
generate_output_human = TRUE
run_logreg_Confidence_mu_minus_1 = FALSE ## still missing from corrected modeling
generate_output_mouse = TRUE
run_mouse_logreg_mu_minus_1 = FALSE ## still missing from corrected modeling
##
## Simulation
##
run_simulation = FALSE
compute_power_spectra_simulation = FALSE
run_control_simulation = FALSE
compute_simulation_control_group_acf = FALSE
##
##
##
compute_metacognitive_sensitivity = FALSE
#### General Markdown Settings
library(pander)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)
library(knitcitations)
cleanbib()
cite_options(citation_format = "pandoc", check.entries = FALSE)
library(bibtex)
Optim <- read.csv("./Results/Optim_human_free_phase.csv")
mean(Optim$p1[1:100])
mean(Optim$p1[1:1000])
mean(Optim$p1[1:2000])
mean(Optim$p1[1:3000])
mean(Optim$p1)
hist(Optim$p1)
Optim <- Optim[Optim$subject_id %in% Behav$subject_id,]
Optim_Behav <-
ddply(
Optim,
.(subject_id),
summarise,
Hazard = mean(p1, na.rm = TRUE),
Precision =  mean(p2),
amp_prior = mean(p3),
amp_LLR = mean(p4),
frequency = mean(p5),
phase = mean(p6)
)
Optim_Behav
Optim_Behav$Hazard
mean(Optim_Behav$Hazard)
Optim <- read.csv("./Results/Optim_human_free_phase.csv")
head(Optim)
M_Optim <- read.csv("./Results/Optim_mouse_free_phase.csv")
head(M_Optim)
