---
bibliography: References/library.bib
csl: References/nature.csl
header-includes:
- \usepackage{setspace}\doublespacing
- \usepackage[fontsize=12pt]{scrextend}
- \usepackage[left]{lineno}
- \usepackage[labelformat=empty]{caption}
- \usepackage{longtable}
- \usepackage{booktabs}
always_allow_html: yes
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: cerulean
  word_document: default
---

```{r setup, include=FALSE}

##
## Latex smallest numbers
##
# smallest_value = .Machine$double.xmin
# replace p = \(0\) with p < \(\ensuremath{2.2\times 10^{-308}}\)

knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
options(scipen = -1, digits = 2)

##
## Global settings: what to do in R markdown (compute primary statistics vs. load data from disc)
##

##
## ROOT
##
#root = "/home/veithweilnhammer/Desktop/Modes Data/"
#root = "E:/Modes Data/"
root = "C:/Users/Veith Weilnhammer/Downloads/"

##
## Load/save fully preprocessed data
##
load_summary_data = FALSE
save_summary_data = FALSE

##
## Human Data
##
collect_data = FALSE
n_permutations = 100

compute_slider = FALSE
sliding_window = 10

additional_autocorrelations = FALSE

load_data = TRUE

compute_logreg = FALSE
compute_Tw_LogReg = FALSE
compute_sine_wave_fit = FALSE
compute_power_spectra = FALSE
compute_group_acf = FALSE
compute_training_history = FALSE
compute_metacognitive_sensitivity = FALSE

extract_additional_Slider_Data = FALSE
compute_sliding_log = FALSE


##
## Mouse data
##

preprocess_mouse_data = FALSE
compute_slider_mouse_data = FALSE
filter_mouse_data = FALSE
compute_pretraining_data_mouse = FALSE

load_mouse_data = FALSE
apply_mouse_exclusion_criteria = FALSE

compute_mouse_group_acf = FALSE
compute_mouse_power_spectra = FALSE
compute_mouse_logreg = FALSE
compute_slider_History_Accuracy_lmer = FALSE
compute_mouse_Tw_LogReg = FALSE
mouse_compute_RT_Accuracy_History = FALSE
compute_mouse_training_history = FALSE
extract_mouse_additional_Slider_Data = FALSE

##
## Optimization
##
run_optim_human = FALSE
run_optim_mouse = TRUE

## generate output from fitted data
generate_output_human = FALSE
run_logreg_Confidence_mu_minus_1 = FALSE 

generate_output_mouse = FALSE
run_mouse_logreg_mu_minus_1 = FALSE

##
## Simulation
##

# visualize bias 
run_visualize_bias_sim = FALSE
run_visualize_alt_sim = FALSE

# Adaptive simulation
run_adaptive_simulation = FALSE
 
# Posterior Parameters
run_simulation = FALSE
compute_power_spectra_simulation = FALSE
run_control_simulation = FALSE
compute_simulation_control_group_acf = FALSE

visualize_circular_inference = FALSE 

#### General Markdown Settings
library(pander)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)

library(knitcitations)
cleanbib()
cite_options(citation_format = "pandoc", check.entries = FALSE)
library(bibtex)
```

```{r libraries}
library(lme4)
library(afex)
library(ppcor)
library(optimx)
library(sjPlot)
library(sjlabelled)
library(sjmisc)
library(effects)
library(quickpsy)
library(pracma)

library(ggplot2)
library(ggridges)
library(ggExtra)
library(gridExtra)
library(ggpubr)
#library(kableExtra)

library(tidyr)
library(plyr)

library(e1071)
library(readxl)

# red: "#E41A1C"
# blue: "#377EB8"
# green: "#66C2A5"
# orange:  "#FC8D62"
  
# library("RColorBrewer")
#   display.brewer.pal(n = 8, name = 'Set2')
#   brewer.pal(n = 8, name = 'Set2')
```

```{r functions}
source("./Functions/helper_functions.R", local = knitr::knit_global())
```

```{r load_and_prepare_data}
##
## loads data from the Confidence database and computes autocorrelations
##
Studies <-
  data.frame(read_excel("Database_Information.xlsx", skip = 0))
Experiments <- Studies

if (collect_data) {
  source("./Functions/f_collect_data.R", local = knitr::knit_global())
  PwData <- f_collect_data(Experiments)
}
```

```{r additional_autocorrelations, cache = TRUE}
##
## add additional autocorrelations (External Stimuli and binarized Difficulty)
##

if (additional_autocorrelations) {
  source("./Functions/f_add_autocorrelations.R", local = knitr::knit_global())
  PwData <- f_add_autocorrelations(PwData)
  
  source("./Functions/f_correct_autocorrelations.R", local = knitr::knit_global())
  PwData <- f_correct_autocorrelations(PwData)
}
```

```{r compute_slider}
source("./Functions/f_compute_slider.R", local = knitr::knit_global())

##
## Compute Dynamic probability of stimulus- and history-congruence
##

if (compute_slider) {

for (id in unique(PwData$subject_id)) {
index = PwData$subject_id == id
print(id)

PwData$History_slider[index] <- f_compute_slider(PwData$History[index], sliding_window)
PwData$Accuracy_slider[index] <- f_compute_slider(PwData$Accuracy[index], sliding_window)
PwData$RT_slider[index] <- f_compute_slider(PwData$RT[index], sliding_window)
PwData$Confidence_slider[index] <- f_compute_slider(PwData$Confidence[index], sliding_window)
}
}
```

```{r load_preprocessed_data, cache = TRUE}
##
## load preprocessed trial-wise human data
##
if (load_data) {
  PwData <-
    read.csv(
      paste(
        root,
        "PwData_full_new_preproc_plus_slider_normalized_hardeasy_external_stimulus_history.csv",
        #"PwData_corr.csv",
        sep = ""
      )
    )
  
  ## only evaluate perceptual studies with binary perceptual responses
  PwData <-
    PwData[PwData$study_type == "Perception" &
             PwData$response_type == "bin",]
  PwData[PwData == "NaN"] = NA
}

##
## compute summary measures on human data + apply exlcusion based on performance
##
if (!load_summary_data) {
  Behav <-
    ddply(
      PwData,
      .(subject_id, study_id),
      summarise,
      History = sum(History, na.rm = TRUE) / length(History) * 100,
      Min_History = min(History_slider, na.rm = TRUE) * 100,
      Max_History = max(History_slider, na.rm = TRUE) * 100,
      Accuracy = sum(Accuracy, na.rm = TRUE) / length(Accuracy) * 100,
      Min_Accuracy = min(Accuracy_slider, na.rm = TRUE) * 100,
      Max_Accuracy = max(Accuracy_slider, na.rm = TRUE) * 100,
      Stimulus_History = mean(Stimulus_History, na.rm = TRUE) * 100,
      Stimulus_Bias = abs(0.5 - sum(Stimulus == max(Stimulus), na.rm = TRUE) /
                            length(Stimulus)) * 100,
      Response_Bias = abs(0.5 - sum(Response == max(Response), na.rm = TRUE) /
                            length(Response)) * 100
    )
  
  Behav$Bias = Behav$Response_Bias - Behav$Stimulus_Bias + 50
  
  Behav$null_History = Behav$History - 50
  
  Behav_diff <-
    ddply(
      PwData[!is.na(PwData$Accuracy),],
      .(subject_id, study_id, Accuracy),
      summarise,
      History = sum(History, na.rm = TRUE) / length(History) * 100
    )
  
  ##
  ## exclude studies with stimulus-congruence below chance level (unclear response labels?)
  ##
  
  Study_Behav <-
    ddply(
      Behav,
      .(study_id),
      summarise,
      mean_Accuracy = mean(Accuracy, na.rm = TRUE),
      mean_History = mean(History, na.rm = TRUE),
      error_Accuracy = sd(Accuracy, na.rm = TRUE) / sqrt(length(Accuracy)),
      error_History = sd(History, na.rm = TRUE) / sqrt(length(History)),
      mean_id = mean(subject_id)
    )
  
  exclusion_list = Study_Behav[Study_Behav$mean_Accuracy < 50,]$study_id
  
  Behav[Behav$study_id %in% exclusion_list,] = NA
  Study_Behav[Study_Behav$study_id %in% exclusion_list,] = NA
  
  
  ##
  ## study-wise effect of perceptual history
  ##
  
  Study_Behav$estimate = NA
  Study_Behav$p = NA
  for (study_idx in unique(Study_Behav$study_id[!is.na(Study_Behav$study_id)])) {
    T_test_save = t.test(Behav[Behav$study_id == study_idx,]$History - 50)
    Study_Behav$estimate[Study_Behav$study_id == study_idx] <-
      T_test_save$estimate
    Study_Behav$p[Study_Behav$study_id == study_idx] <-
      T_test_save$p.value
    
  }
  
  Behav$study_id <- as.factor(Behav$study_id)
  
  ##
  ## exclude_studies with low accuracy
  ##
  
  '%ni%' <- Negate('%in%')
  PwData <- PwData[PwData$study_id %ni% exclusion_list,]
  '%ni%' <- Negate('%in%')
  PwData <- PwData[PwData$study_id %ni% exclusion_list,]
  PwData[PwData$Trial == 1,]$History = NA
  
  n_trials_human = nrow(PwData)
  n_studies_human = length(unique(PwData$study_id))
  n_participants_human = length(unique(PwData$subject_id))
  
  if (save_summary_data) {
    save(
      Behav,
      Behav_diff,
      Study_Behav,
      exclusion_list,
      n_trials_human,
      n_studies_human,
      n_participants_human,
      file = "./Summary_Data/preprocessed_data_human.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/preprocessed_data_human.Rdata")
}

if (load_data){  
  
  PwData <- PwData[PwData$study_id %in% Behav$study_id,]
  
  
  n_trials_human = nrow(PwData)
  n_studies_human = length(unique(PwData$study_id))
  n_participants_human = length(unique(PwData$subject_id))
  
  PwData[PwData$study_id %in% c(72, 77, 78, 105, 106, 117, 125, 135, 137, 138, 144),]$Difficulty = -PwData[PwData$study_id %in% c(72, 77, 78, 105, 106, 117, 125, 135, 137, 138, 144),]$Difficulty
  
  PwData[PwData$study_id == 14,]$Difficulty <- scale(abs(PwData[PwData$study_id == 14,]$Difficulty))
}
```

# Title Page

**Bimodal Inference in Mice and Men** \
\

<br/><br/>
**Authors**: 

Veith Weilnhammer$^{1,2}$, Heiner Stuke$^{1,2}$, Kai Standvoss$^{1,3,5}$,  Philipp Sterzer$^{1,3,4,5}$\
\
<br/><br/>
**Affiliations**: 

$^{1}$ Department of Psychiatry, Charité-Universitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu Berlin, 10117 Berlin, Germany \
$^{2}$ Berlin Institute of Health, Charité-Universitätsmedizin Berlin and Max Delbrück Center, 10178 Berlin, Germany \
$^{3}$ Bernstein Center for Computational Neuroscience, Charité-Universitätsmedizin Berlin, 10117 Berlin, Germany\
$^{4}$ Berlin School of Mind and Brain, Humboldt-Universität zu Berlin, 10099 Berlin, Germany\
$^{5}$ Einstein Center for Neurosciences Berlin, 10117 Berlin, Germany\
\
<br/><br/>

**Corresponding Author**:

Veith Weilnhammer, Department of Psychiatry, Charité Campus Mitte, Charitéplatz 1, 10117 Berlin, phone: 0049 (0)30 450 517 317, email: veith.weilnhammer@gmail.com \

\newpage

\linenumbers

# Abstract

Perception is known to cycle through periods of enhanced and reduced sensitivity to external information. Here, we asked whether such slow fluctuations arise as a noise-related epiphenomenon of limited processing capacity or, alternatively, represent a structured mechanism of perceptual inference. Using two large-scale datasets, we found that humans and mice waver between alternating intervals of externally- and internally-oriented modes of sensory analysis. During external mode, perception aligned more closely with the external sensory information, whereas internal mode was characterized by enhanced biases toward perceptual history. Computational modeling indicated that dynamic changes in mode are enabled by two interlinked factors: (i), the integration of subsequent inputs over time and, (ii), slow anti-phase oscillations in the perceptual impact of external sensory information versus internal predictions that are provided by perceptual history. **We propose that between-mode fluctuations may benefit perception by generating unambiguous error signals that enable optimal inference and learning in volatile environments.**

# One sentence summary

Humans and mice fluctuate between external and internal modes of sensory processing.

\
<br/><br/>

\newpage
# Introduction

The capacity to respond to changes in the environment is a defining feature of life[@Schrodinger1944; @Ashby1947; @Friston2013]. Intriguingly, the ability of living things to process their surroundings fluctuates considerably over time[@Palva2011; @VanRullen2016]. In humans and mice, perception[@Verplanck1952; @Atkinson1963;  @Dehaene1993; @Gilden1995; @Gilden1995a; @Monto2008; @Ashwood2022], cognition[@Gilden2001] and memory[@Duncan2012] cycle through prolonged periods of enhanced and reduced sensitivity to external information, suggesting that the brain detaches from the world in recurring intervals that last from milliseconds to seconds and even minutes[@Palva2011; @VanRullen2016]. Yet breaking from external information is risky, as swift responses to the environment are often crucial to survival.

What could be the reason for these fluctuations in perceptual performance[@Monto2008]? First, periodic fluctuations in the ability to parse external information[@ClareKelly2008; @Monto2008; @Hesselmann2008] may arise simply due to bandwidth limitations and noise. Second, it may be advantageous to actively reduce the costs of neural processing by seeking sensory information only in recurring intervals[@Schroeder2010a; @VanRullen2016], otherwise relying on random or stereotypical responses to the external world. Third, spending time away from the ongoing stream of sensory inputs may also reflect a functional strategy that facilitates flexible behavior and learning[@Honey2017]: Intermittently relying more strongly on information acquired from past experiences may enable agents to build up stable internal predictions about the environment despite an ongoing stream of external sensory signals[@Weilnhammer2021a]. By the same token, recurring intervals of enhanced sensitivity to external information may help to detect changes in both the state of the environment and the amount of noise that is inherent in sensory encoding[@Weilnhammer2021a]. 

In this work, we sought to elucidate whether periodicities in the sensitivity to external information represent an epiphenomenon of limited processing capacity or, alternatively, result from a structured and adaptive mechanism of perceptual inference. To this end, we analyzed two large-scale datasets on perceptual decision-making in humans[@Rahnev2020] and mice[@Aguillon-Rodriguez2020]. When less sensitive to external stimulus information, humans and mice showed stronger serial dependencies[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Akrami2018; @Braun2018; @Bergen2019; @Urai2019; @Hsu2020], which have been conceptualized as internal predictions that reflect the auto-correlation of natural environments[@Dong1995] and bias perceptual decisions toward preceding choices[@Burr2014; @Braun2018; @Bergen2019]. Computational modeling indicated that ongoing changes in perceptual performance may be driven by systematic fluctuations between externally- and internally-oriented modes of sensory analysis. **We propose that that such bimodal inference may improve, (i), the ability to robustly determine the statistical properties of volatile environments and, (ii), the ability to calibrate internal beliefs about the precision of sensory information relative to prior (Bayesian) beliefs.**

# Results

## Human perception fluctuates between epochs of enhanced and reduced sensitivity to external information

```{r serial_dependence, cache = TRUE}

if (!load_summary_data) {
  ##
  ## STATS on History-congruent perception
  ##
  Global_History_Accuracy <-
    lmer(null_History ~ Accuracy + (1 |
                                      study_id), data = Behav)
  true_Global_History_Accuracy <-
    lmer(History ~ Accuracy + (1 | study_id), data = Behav)
  STAT.Global_History_Accuracy = summary(Global_History_Accuracy)
  
  diff_History_Accuracy <-
    lmer(History ~ Accuracy + (1 |
                                 study_id), data = Behav_diff)
  STAT.diff_History_Accuracy = summary(diff_History_Accuracy)
  
  if (save_summary_data) {
    save(
      Global_History_Accuracy,
      true_Global_History_Accuracy,
      STAT.Global_History_Accuracy,
      diff_History_Accuracy,
      STAT.diff_History_Accuracy,
      file = "./Summary_Data/serial_dependence.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/serial_dependence.Rdata")
}
```

```{r stats_autocorrelations, cache = TRUE}

##
## STATS based on autocorrelations
##

if (!load_summary_data) {
  # compute difference to random autocorrelation and remove outliers
  PwData$diff_acf_Stimulus = exclude_3SD(PwData$acf_Stimulus - PwData$random_acf_Stimulus)
  PwData$diff_acf_History = exclude_3SD(PwData$acf_History - PwData$random_acf_History)
  PwData$diff_acf_Difficulty = exclude_3SD(PwData$acf_Difficulty - PwData$random_acf_Difficulty)
  PwData$diff_acf_Condition = exclude_3SD(PwData$acf_Condition - PwData$random_acf_Condition)
  PwData$diff_acf_RT = exclude_3SD(PwData$acf_RT - PwData$random_acf_RT)
  PwData$diff_acf_Confidence = exclude_3SD(PwData$acf_Confidence - PwData$random_acf_Confidence)
  PwData$diff_acf_HardEasy = exclude_3SD(PwData$acf_HardEasy - PwData$random_acf_HardEasy)
  PwData$diff_acf_External = exclude_3SD(PwData$acf_External - PwData$random_acf_External)
  
  # compute group-level acf
  if (compute_group_acf) {
    source("./Functions/f_compute_group_acf.R",
           local = knitr::knit_global())
    
    acf_to_test = c('diff_acf_Stimulus',  'diff_acf_History')
    max_trial = 99
    
    Summary_acf <- f_compute_group_acf(PwData, acf_to_test, max_trial)
    
  } else {
    Summary_acf <-
      read.csv(
        "./Results/Summary_acf_study_corrected_normalized_3SD_HardEasy_External2.csv" ## extended_version: hardeasy + external
        #"./Results/Summary_acf_corr.csv"
      )
  }
  Summary_acf$Trial <- Summary_acf$Trial - 1
  
  ##
  ## Summary over exceed statistic
  ##
  ID_Summary_acf <- ddply(
    PwData,
    .(study_id, subject_id),
    summarise,
    lag_significant_Stimulus = min(which(diff(
      which(c(0, exceed_acf_Stimulus, 0) < n_permutations / 2)
    ) != 1)) - 1,
    lag_significant_History = min(which(diff(
      which(c(0, exceed_acf_History, 0) < n_permutations / 2)
    ) != 1)) - 1,
    lag_significant_Difficulty = min(which(diff(
      which(c(0, exceed_acf_Difficulty, 0) < n_permutations / 2)
    ) != 1)) - 1,
    lag_significant_External = min(which(diff(
      which(c(0, exceed_acf_External, 0) < n_permutations / 2)
    ) != 1)) - 1,
    Confidence_Stimulus_con = mean(Confidence[Accuracy == 1], na.rm = TRUE) - mean(Confidence[Accuracy == 0], na.rm = TRUE),
    RT_Stimulus_con = mean(RT[Accuracy == 1], na.rm = TRUE) -  mean(RT[Accuracy == 0], na.rm = TRUE),
    Confidence_History_con = mean(Confidence[History == 1], na.rm = TRUE) - mean(Confidence[History == 0], na.rm = TRUE),
    RT_History_con = mean(RT[History == 1], na.rm = TRUE) - mean(RT[History == 0], na.rm = TRUE)
  )
  
  ID_Summary_acf$lag_significant_Stimulus[ID_Summary_acf$lag_significant_Stimulus == Inf] = NA
  ID_Summary_acf$lag_significant_History[ID_Summary_acf$lag_significant_History == Inf] = NA
  ID_Summary_acf$lag_significant_Difficulty[ID_Summary_acf$lag_significant_Difficulty == Inf] = NA
  ID_Summary_acf$lag_significant_External[ID_Summary_acf$lag_significant_External == Inf] = NA
  
  gathercol = colnames(PwData[, c(29, 30)])
  Exceed_long  <-
    gather(PwData[, c(1, 4, 5, 29, 30)],
           "Variable",
           "Probability",
           gathercol,
           factor_key = TRUE)
  
  Summary_exceed <-  ddply(
    Exceed_long[Exceed_long$Trial != 1, ],
    .(Trial, Variable),
    summarise,
    
    mean_probability = mean(exclude_3SD((100 - Probability)), na.rm = TRUE),
    ci_probability = qnorm(0.975) * sd(exclude_3SD(100 - Probability), na.rm = TRUE) /
      sqrt(length(exclude_3SD(100 - Probability)))
  )
  Summary_exceed$Trial <- Summary_exceed$Trial - 1
  
  ##
  ## STATS: Autocorrelations decay over time
  ##
  Stimulus_linear <-
    summary(lm(Mean ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus" &
                                                   Summary_acf$Trial < 21, ]))
  Stimulus_exponential <-
    summary(lm(log(Mean) ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus" &
                                                        Summary_acf$Trial < 21, ]))
  
  History_linear <-
    summary(lm(Mean ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_History" &
                                                   Summary_acf$Trial < 21, ]))
  History_exponential <-
    summary(lm(log(Mean) ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_History" &
                                                        Summary_acf$Trial < 21, ]))
  
  PwData$min_diff_acf_Stimulus = PwData$diff_acf_Stimulus - min(PwData$diff_acf_Stimulus, na.rm = TRUE)
  PwData$min_diff_acf_History = PwData$diff_acf_History - min(PwData$diff_acf_History, na.rm = TRUE)
  
  lmer_acf_Stimulus <-
    lmer(log(min_diff_acf_Stimulus) ~ Trial + (1|
                                                 study_id / subject_id),
         data = PwData[PwData$Trial > 1 & PwData$Trial < 21, ])
  
  STAT.lmer_acf_Stimulus <- summary(lmer_acf_Stimulus)
  
  lmer_acf_History <-
    lmer(log(min_diff_acf_History) ~ Trial + (1  |
                                                study_id / subject_id),
         data = PwData[PwData$Trial > 1 &
                         PwData$Trial < 21 & PwData$min_diff_acf_History > 0, ])
  STAT.lmer_acf_History <- summary(lmer_acf_History)
  
  
  ##
  ## STATS: RT as a function of stimulus- and history-congruence
  ##
  lmer_RT_Accuracy_History <-
    lmer(RT ~ Accuracy + History + (1  |
                                      study_id / subject_id),
         data = PwData)
  
  STAT.lmer_RT_Accuracy_History <- summary(lmer_RT_Accuracy_History)
  
  lmer_Confidence_Accuracy_History <-
    lmer(Confidence ~ Accuracy + History + (1 |
                                              study_id / subject_id),
         data = PwData)
  
  STAT.lmer_Confidence_Accuracy_History <- summary(lmer_Confidence_Accuracy_History)
  
  
  if (save_summary_data) {
    save(
      Summary_acf,
      ID_Summary_acf,
      Summary_exceed,
      Stimulus_linear,
      Stimulus_exponential,
      History_linear,
      History_exponential,
      lmer_acf_Stimulus,
      STAT.lmer_acf_Stimulus,
      lmer_acf_History,
      STAT.lmer_acf_History,
      #lmer_RT_Accuracy_History,
      STAT.lmer_RT_Accuracy_History,
      #lmer_Confidence_Accuracy_History,
      STAT.lmer_Confidence_Accuracy_History,
      file = "./Summary_Data/stats_autocorrelations.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/stats_autocorrelations.Rdata")
}
```

We began by selecting `r n_studies_human` studies from the Confidence Database[@Rahnev2020] that investigated how human participants (N = `r n_participants_human`) perform binary perceptual decisions (Figure 1A; see Methods section for details on inclusion criteria). As a metric for perceptual performance (i.e., the sensitivity to external sensory information), we asked whether the participant's response and the presented stimulus matched (*stimulus-congruent* choices) or differed from each other (*stimulus-incongruent* choices; Figure 1B and C) in a total of $`r n_trials_human/100000`$ million trials.

In a first step, we asked whether the ability to accurately perceive sensory stimuli is constant over time or, alternatively, fluctuates in periods of enhanced and reduced sensitivity to external information. We found perception to be stimulus-congruent in `r mean(Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Accuracy, na.rm = TRUE)/sqrt(length(Behav$Accuracy))`% of trials (mean ± standard error of the mean; Figure 2A), which was highly consistent across the selected studies (Supplemental Figure S1A). In line with previous work[@Dehaene1993], we found that the probability of stimulus-congruence was not independent across successive trials: At the group level, stimulus-congruent perceptual choices were significantly autocorrelated for up to `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` trials. Autocorrelation coefficients decayed exponentially over time (rate $\gamma$ = $`r STAT.lmer_acf_Stimulus$coefficients[2,1]`$ ± $`r STAT.lmer_acf_Stimulus$coefficients[2,2]`$, T($`r STAT.lmer_acf_Stimulus$coefficients[2,3]`$) = $`r STAT.lmer_acf_Stimulus$coefficients[2,4]`$, p = $`r STAT.lmer_acf_Stimulus$coefficients[2,5]`$; Figure 2B). Importantly, the autocorrelation of stimulus-congruent perception was not a trivial consequence of the experimental design, but remained significant when controlling for the trial-wise autocorrelation of task difficulty (Supplemental Figure S2A) or the sequence of presented stimuli (Supplemental Figure S2B).

In addition, stimulus-congruence was significantly autocorrelated not only at the group-level, but also in individual participants, where the autocorrelation of stimulus-congruent perception exceeded the respective autocorrelation of randomly permuted data within an interval of `r mean(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_Summary_acf)` trials (Figure 2C). In other words, if a participant's experience was congruent (or incongruent) with the external stimulus information at a given trial, her perception was more likely to be stimulus-congruent (or incongruent) for approximately `r round(mean(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE))` trials into the future. 

```{r logistic_regression_autocorrelation, cache = TRUE}
##
## Reproduce autocorrelation with logistic regression (correcting for difficulty and stimulus repetition)
##
if (!load_summary_data) {
   n_back = 25
  if (compute_Tw_LogReg == TRUE) {
   
    source("./Functions/f_compute_trial_wise_logreg.R",
           local = knitr::knit_global())
    Tw_LogReg <- f_compute_trial_wise_logreg(PwData, n_back)
  } else {
    #Tw_LogReg <- read.csv("./Results/Tw_LogReg.csv")
    Tw_LogReg <- read.csv("./Results/Tw_LogReg_corr.csv")
  }
  
  gathercol = colnames(Tw_LogReg[, c(5, 7)])
  Tw_LogReg_long <-
    gather(Tw_LogReg[, c(2:5, 7)],
           "Variable",
           "beta",
           gathercol,
           factor_key = TRUE)
  
  Tw_LogReg_long$Variable <-
    gsub("History_weight", "History", Tw_LogReg_long$Variable)
  Tw_LogReg_long$Variable <-
    gsub("Stimulus_weight", "Stimulus", Tw_LogReg_long$Variable)
  
  Group_Tw_LogReg_long <-  ddply(
    Tw_LogReg_long,
    .(lag, Variable),
    summarise,
    Mean = mean(exclude_3SD(beta), na.rm = TRUE),
    Error = sd(exclude_3SD(beta), na.rm = TRUE) / sqrt(length(exclude_3SD(beta)))
  )
  
  
  ## trial-wise stats
  Group_Tw_LogReg_long$p_value = NA
  for (lag_idx in c(1:n_back)) {
    Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "History" &
                           Group_Tw_LogReg_long$lag == lag_idx,]$p_value <-
      summary(lmer(exclude_3SD(History_weight) ~ 1 + (1 |
                                                        study_id) , data = Tw_LogReg[Tw_LogReg$lag == lag_idx,]))$coefficients[5]
    
    Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "Stimulus" &
                           Group_Tw_LogReg_long$lag == lag_idx,]$p_value <-
      summary(lmer(exclude_3SD(Stimulus_weight) ~ 1 + (1 |
                                                         study_id) , data = Tw_LogReg[Tw_LogReg$lag == lag_idx,]))$coefficients[5]
  }
  

  if (save_summary_data) {
    save(Group_Tw_LogReg_long,
         Tw_LogReg_long,
         Tw_LogReg,
         n_back,
         Group_Tw_LogReg_long,
         file = "./Summary_Data/log_reg_autocorrelation.Rdata")
  }
  
} else {
  load("./Summary_Data/log_reg_autocorrelation.Rdata")
}

```

To further corroborate the autocorrelation of stimulus-congruence, we used logistic regression models that predicted the stimulus-congruence of perception at the index trial $t = 0$ from the stimulus-congruence at the preceding trials within a lag of `r n_back` trials. We found that regression weights were significantly greater than zero for up to `r min(which(Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "Stimulus",]$p_value > 0.05))` trials (Supplemental Figure S3). 
<!-- . While controlling for the effect of task difficulty and stimulus repetition at the index trial $t = 0$ as well as for general response biases (i.e., the propensity to chose one of the two outcomes more frequently than the other) -->

```{r power_spectra, cache = TRUE}
##
## Spectra analysis of behavior
##
if (!load_summary_data) {
  
  if (compute_power_spectra) {
    source("./Functions/f_compute_power_spectra.R",
           local = knitr::knit_global())
    
    sliders = c("Accuracy_slider", "History_slider")
    Power_Spectra <- f_compute_power_spectra(PwData, sliders)
  } else {
    Power_Spectra <-
      read.csv(paste(root, "power_spectra_smoothed.csv", sep = ""))
  }
  
  Power_Spectra$r_freq = round(Power_Spectra$freq, digits = 4)
  
  step_size = 0.01
  bins <-
    seq(
      from = min(Power_Spectra$freq),
      to = max(Power_Spectra$freq),
      by = step_size
    )
  
  names <- round(bins, digits = 2)
  names = names[1:length(names) - 1]
  
  Power_Spectra$bin_freq <-
    cut(Power_Spectra$freq, breaks = bins, labels = names)
  Power_Spectra$bin_freq <-
    as.numeric(Power_Spectra$bin_freq) * step_size
  Power_Spectra$bin_freq[is.na(Power_Spectra$bin_freq)] = max(Power_Spectra$freq)
  
  Power_Spectra$Coherence <- Power_Spectra$Coherence * 100
  
  gathercol = colnames(Power_Spectra[, c(2, 3)])
  Power_Spectra_long  <-
    gather(Power_Spectra[, c(1, 2, 3, 6, 7, 8, 9)],
           "Variable",
           "Power",
           gathercol,
           factor_key = TRUE)
  
  Power_Frequency <-  ddply(
    Power_Spectra_long,
    .(r_freq, Variable),
    summarise,
    
    mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
    ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE) / sqrt(length(exclude_3SD(Power)))
  )
  
  Coherence_Phase_Frequency <- ddply(
    Power_Spectra,
    .(bin_freq),
    summarise,
    
    mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
    ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE) /
      sqrt(length(exclude_3SD(Coherence))),
    
    mode_phase = getmode(exclude_3SD((abs(
      Phase
    )))),
    mean_phase = getmode(exclude_3SD((abs(
      Phase
    )))),
    ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE) /
      sqrt(length(exclude_3SD(Phase)))
  )
  
  
  ##
  ## mode of coherence and phase
  ##
  Summary_Power_Spectra <-
    ddply(
      Power_Spectra[Power_Spectra$freq > 0.01 &
                      Power_Spectra$freq < 0.1, ],
      .(study_id, subject_id),
      summarise,
      mean_coherence = mean(exclude_3SD((Coherence))),
      mode_phase = getmode(exclude_3SD((abs(
        Phase
      ))))
    )
  
  
  ##
  ## STATS Power vs Frequency (1/f noise)
  ##
  lmer_power_freq_Stimulus <-
    lmer(log(Power_Stimulus) ~ log(freq) + (1 |
                                              study_id / subject_id),
         data = Power_Spectra[Power_Spectra$freq > 0.01 &
                                Power_Spectra$freq < 0.1 &
                                is.finite(log(Power_Spectra$Power_Stimulus)), ])
  
  STAT.lmer_power_freq_Stimulus <- summary(lmer_power_freq_Stimulus)
  
  lmer_power_freq_History <-
    lmer(log(Power_History) ~ log(freq) + (1 |
                                             study_id / subject_id),
         data = Power_Spectra[Power_Spectra$freq > 0.01 &
                                Power_Spectra$freq < 0.1 &
                                is.finite(log(Power_Spectra$Power_History)), ])
  
  STAT.lmer_power_freq_History <- summary(lmer_power_freq_History)
  
  if (save_summary_data) {
    save(
      Power_Spectra,
      Power_Spectra_long,
      Power_Frequency,
      Coherence_Phase_Frequency,
      Summary_Power_Spectra,
      lmer_power_freq_Stimulus,
      lmer_power_freq_History,
      STAT.lmer_power_freq_Stimulus,
      STAT.lmer_power_freq_History,
      file = "./Summary_Data/power_spectra.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/power_spectra.Rdata")
}
```

These results confirm that the ability to process sensory signals is not constant over time, but unfolds in multi-trial epochs of enhanced and reduced sensitivity to external information[@Dehaene1993]. As a consequence of this autocorrelation, the dynamic probability of stimulus-congruent perception (i.e., computed in sliding windows of ± 5 trials; Figure 1C) fluctuated considerably within participants (average minimum: `r mean(Behav$Min_Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Min_Accuracy, na.rm = TRUE)/sqrt(length(Behav$Min_Accuracy))`%, maximum: `r mean(Behav$Max_Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Max_Accuracy, na.rm = TRUE)/sqrt(length(Behav$Max_Accuracy))`%). 
In line with previous findings[@Gilden1995], such fluctuations in the sensitivity to external information had a power density that was inversely proportional to the frequency in the slow spectrum[@Monto2008] (power ~ 1/$f^\beta$, $\beta$ = $`r STAT.lmer_power_freq_Stimulus$coefficients[2,1]`$ ± $`r STAT.lmer_power_freq_Stimulus$coefficients[2,2]`$, T($`r STAT.lmer_power_freq_Stimulus$coefficients[2,3]`$) = $`r STAT.lmer_power_freq_Stimulus$coefficients[2,4]`$, p = $`r STAT.lmer_power_freq_Stimulus$coefficients[2,5]`$; Figure 2D). This feature, which is also known as *1/f noise*[@Montroll1982; @Bak1987], represents a characteristic of ongoing fluctuations in complex dynamic systems such as the brain[@Chialvo2010] and the cognitive processes it entertains[@Gilden1995; @Gilden1995a; @Gilden2001; @Wagenmakers2004; @VanOrden2005]. 

## Human perception fluctuates between external and internal modes of sensory processing

In a second step, we sought to explain why perception cycles through periods of enhanced and reduced sensitivity to external information[@Palva2011; @VanRullen2016]. We reasoned that observers may intermittently rely more strongly on internal information, i.e., on predictions about the environment that are constructed from previous experiences[@Bergen2019; @Weilnhammer2021a].  

In perception, *serial dependencies* represent one of the most basic internal predictions that cause perceptual decisions to be systematically biased toward preceding choices[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Akrami2018; @Braun2018; @Bergen2019; @Urai2019; @Hsu2020]. Such effects of perceptual history mirror the continuity of the external world, in which the recent past often predicts the near future[@Dong1995; @Chopin2012; @Burr2014; @Braun2018; @Bergen2019]. Therefore, as a metric for the perceptual impact of internal information, we computed whether the participant's response at a given trial matched or differed from her response at the preceding trial (*history-congruent* and *history-incongruent perception*, respectively; Figure 1B and C).  

```{r logistic_regression_outcomes, cache = TRUE}

if (!load_summary_data) {
  
 if (compute_logreg) {
source("./Functions/f_compute_logreg.R", local = knitr::knit_global())
LogAggr <- f_compute_logreg(PwData)
} else {
  LogAggr <- read.csv("./Results/LogAggr.csv")
}

LogAggr$diff_AIC = exclude_3SD(LogAggr$reduced_AIC - LogAggr$full_AIC)
LogAggr$diff_AIC = (LogAggr$reduced_AIC - LogAggr$full_AIC)

LogAggr$study_id <- as.factor(LogAggr$study_id)

Study_LogAggr <- 
  ddply(LogAggr, 
    .(study_id),
    summarise,
    mean_diff_AIC = mean(diff_AIC, na.rm = TRUE),
    error_diff_AIC = sd(diff_AIC, na.rm = TRUE)/sqrt(length(diff_AIC))
    )
Study_LogAggr[Study_LogAggr == "NaN"] = NA

STAT.LogReg = summary(lmer(diff_AIC ~ 1 + (1|study_id), data = LogAggr))

if (compute_logreg) {
full_logit_Sum <-
    glmer(Response ~ Stimulus + Response_minus_1 + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1,],
        family = binomial)
reduced_logit_Sum <-
    glmer(Response ~ Stimulus + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1,],
        family = binomial)
} else {
  full_logit_Sum <- readRDS(file = "./Results/full_logit_Sum.rds")
  reduced_logit_Sum <- readRDS(file = "./Results/reduced_logit_Sum.rds")
}
STAT.full_logit_Sum <- summary(full_logit_Sum) 
STAT.reduced_logit_Sum <- summary(reduced_logit_Sum)  
  if (save_summary_data) {
    save(STAT.full_logit_Sum, 
         STAT.reduced_logit_Sum,
         STAT.LogReg, 
         Study_LogAggr,
         LogAggr,
         file = "./Summary_Data/log_reg_outcomes.Rdata")
  }
  
} else {
  load("./Summary_Data/log_reg_outcomes.Rdata")
}
```

First, we ensured that perceptual history played a significant role in perception despite the ongoing stream of external information. With a global average of `r mean(Behav$History, na.rm = TRUE)`% ± `r sd(Behav$History, na.rm = TRUE)/sqrt(length(Behav$History))`% history-congruent trials, we found a small but highly significant perceptual bias towards preceding experiences ($\beta$ = $`r STAT.Global_History_Accuracy$coefficients[1,1]`$ ± $`r STAT.Global_History_Accuracy$coefficients[1,2]`$, T($`r STAT.Global_History_Accuracy$coefficients[1,3]`$) = $`r STAT.Global_History_Accuracy$coefficients[1,4]`$, p = $`r STAT.Global_History_Accuracy$coefficients[1,5]`$; Figure 2A) that was largely consistent across studies (Supplemental Figure 1B) and more pronounced in participants who were less sensitive to external sensory information (Supplemental Figure 1C). 
Logistic regression confirmed the internal information provided by perceptual history made a significant contribution to perception ($\beta$ = $`r STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum$coefficients[3,4]`$) over and above the ongoing stream of external sensory information ($\beta$ = $`r STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward one of the two potential outcomes ($\beta$ = $`r STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r STAT.full_logit_Sum$coefficients[1,4]`$; see Supplemental Figure S4A for model comparisons within individual participants). 

In addition, we confirmed that history-congruence was not a corollary of the sequence of presented stimuli: History-congruent perceptual choices were more frequent at trials when perception was stimulus-incongruent (`r mean(Behav_diff[Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)`% ± `r sd(Behav_diff[Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)/sqrt(length(Behav_diff[Behav_diff$Accuracy == 0,]$History))`%) as opposed to stimulus-congruent (`r mean(Behav_diff[Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)`% ± `r sd(Behav_diff[Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)/sqrt(length(Behav_diff[Behav_diff$Accuracy == 1,]$History))`%, $\beta$ = $`r STAT.diff_History_Accuracy$coefficients[2,1]`$ ± $`r STAT.diff_History_Accuracy$coefficients[2,2]`$, T($`r STAT.diff_History_Accuracy$coefficients[2,3]`$) = $`r STAT.diff_History_Accuracy$coefficients[2,4]`$, p = $`r STAT.diff_History_Accuracy$coefficients[2,5]`$; Figure 2A, lower panel). Despite being adaptive in auto-correlated real-world environments[@Dong1995; @Burr2014; @Cicchini2018; @Weilnhammer2021a], perceptual history thus represented a source of bias in the randomized experimental designs studied here[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019]. 

Second, we asked whether perception cycles through multi-trial epochs during which perception is characterized by stronger or weaker biases toward preceding experiences. Indeed, in close analogy to stimulus-congruence, history-congruence was significantly autocorrelated for up to `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` trials (Figure 2B). Following a peak at the first trial, the respective autocorrelation coefficients decreased exponentially over time (rate $\gamma$ = $`r STAT.lmer_acf_History$coefficients[2,1]`$ ± $`r STAT.lmer_acf_History$coefficients[2,2]`$, T($`r STAT.lmer_acf_History$coefficients[2,3]`$) = $`r STAT.lmer_acf_History$coefficients[2,4]`$, p = $`r STAT.lmer_acf_History$coefficients[2,5]`$). 
History-congruence remained significantly autocorrelated when controlling for task difficulty (Supplemental Figure S2A) and the sequence of presented stimuli (Supplemental Figure S2B). In individual participants, the autocorrelation of history-congruence was elevated above randomly permuted data for a lag of `r mean(ID_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_Summary_acf)` trials (Figure 2C), confirming that the autocorrelation of history-congruence was not only a group-level phenomenon. The autocorrelation of history-congruence was confirmed by logistic regression models that successfully predicted the history-congruence of perception at an index trial $t = 0$ from the history-congruence at the preceding trials within a lag of `r min(which(Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "History",]$p_value > 0.05))` trials (Supplemental Figure S3).  

Third, we asked whether the impact of internal information fluctuates as **a scale-invariant process with a 1/f power law (i.e., a feature typically associated with fluctuations in the sensitivity to external information**[@Gilden1995; @Gilden1995a; @Gilden2001; @Wagenmakers2004; @VanOrden2005]). The dynamic probability of history-congruent perception (i.e., computed in sliding windows of ± 5 trials; Figure 1C) varied considerably over time, ranging between a minimum of `r mean(Behav$Min_History, na.rm = TRUE)`% ± `r sd(Behav$Min_History, na.rm = TRUE)/sqrt(length(Behav$Min_History))`% and a maximum `r mean(Behav$Max_History, na.rm = TRUE)`% ± `r sd(Behav$Max_History, na.rm = TRUE)/sqrt(length(Behav$Max_History))`%. In analogy to stimulus-congruence, we found that history-congruence fluctuated as **a scale-invariant process with a 1/f power law**, with power densities that were inversely proportional to the frequency in the slow spectrum[@Monto2008] (power ~ 1/$f^\beta$, $\beta$ = $`r STAT.lmer_power_freq_History$coefficients[2,1]`$ ± $`r STAT.lmer_power_freq_History$coefficients[2,2]`$, T($`r STAT.lmer_power_freq_History$coefficients[2,3]`$) = $`r STAT.lmer_power_freq_History$coefficients[2,4]`$, p = $`r STAT.lmer_power_freq_History$coefficients[2,5]`$; Figure 2D). 

```{r slider_History_Accuracy, cache = TRUE}
##
## Compute Association between History- and Accuracy_slider
##

if (!load_summary_data) {
  
  slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|study_id/subject_id), data = PwData)
STAT.slider_History_vs_Accuracy <- summary(slider_History_vs_Accuracy)
  
  if (save_summary_data) {
    save(slider_History_vs_Accuracy, 
         STAT.slider_History_vs_Accuracy,  
         file = "./Summary_Data/slider_History_Accuracy.Rdata")
  }
  
} else {
  load("./Summary_Data/slider_History_Accuracy.Rdata")
}
```

Finally, we ensured that fluctuations in stimulus- and history-congruence are linked to each other. When perceptual choices were less biased toward external information, participants relied more strongly on internal information acquired from perceptual history (and vice versa, $\beta$ = $`r STAT.slider_History_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_vs_Accuracy$coefficients[2,5]`$). 
Thus, while sharing the characteristic of **a scale-invariant process with a 1/f power law**, fluctuations in stimulus- and history-congruence were shifted against each other by approximately half a cycle and showed a squared coherence of `r mean(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)` ± `r sd(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(Summary_Power_Spectra$mean_coherence)`% (Figure 2E and F; we report the average phase and coherence for frequencies below 0.1 $1/N_{trials}$; see Methods for details).

```{r summary_internal_external_mode, cache = TRUE}
##
## Create summary_variable for Modes
##

if (!load_summary_data) {
  
PwData <-  ddply(
      PwData,
      .(subject_id),
      mutate,
      
      directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
      scaled_directed_mode = scale(directed_mode),
      strength_mode = abs(scaled_directed_mode)
      )

ID_mode <-  ddply(
      PwData,
      .(subject_id),
      summarise,
      strength_mode = mean(strength_mode, na.rm = TRUE),
      directed_mode = mean(directed_mode, na.rm = TRUE)
)  
  
  if (save_summary_data) {
    save(ID_mode,  file = "./Summary_Data/Summary_mode.Rdata")
  }
  
} else {
  load("./Summary_Data/Summary_mode.Rdata")
}
```

In sum, our analyses indicate that perceptual decisions may result from a competition between external sensory signals with internal predictions provided by perceptual history. Crucially, we show that the impact of these external and internal sources of information is not stable over time, but fluctuates systematically, emitting overlapping autocorrelation curves and antiphase 1/f profiles.

These links between stimulus- and history-congruence suggest that the fluctuations in the impact of external and internal information may be generated by a unifying mechanism that causes perception to alternate between two opposing *modes*[@Honey2017] (Figure 1D): During *external mode*, perception is more strongly driven by the available external stimulus information. Conversely, during *internal mode*, participants rely more heavily on internal predictions that are implicitly provided by preceding perceptual experiences. Fluctuations in mode (i.e., the degree of bias toward external versus internal information) may thus provide a novel explanation for ongoing fluctuations in the sensitivity to external information[@Palva2011; @VanRullen2016; @Honey2017].

## Internal and external modes of processing facilitate response behavior and enhance confidence in human perceptual decision-making

Alternatively, however, fluctuating biases toward externally- and internally-oriented modes may not represent a perceptual phenomenon, but result from cognitive processes that are situated up- or downstream of perception. For instance, it may be argued that participants may be prone to stereotypically repeat the preceding choice when not attending to the experimental task. Thus, fluctuations in mode may arise due to systematic changes in the level of tonic arousal[@McGinley2015b] or on-task attention[@Rosenberg2013; @Zalta2020]. Since arousal and attention typically link closely with response times[@Rosenberg2013; @Prado2011] (RTs), this alternative explanation entails that RTs increase monotonically as one moves away from externally-biased and toward internally-biases modes of sensory processing. 

```{r EI_RT_Confidence, cache = TRUE}

##
## Confidence and RT vs mode
##
if (!load_summary_data) {
  
PwData$clear_RT = PwData$RT
PwData$clear_RT = exclude_3SD(PwData$clear_RT)

PwData$clear_Confidence = PwData$Confidence
PwData$clear_Confidence = exclude_3SD(PwData$clear_Confidence)

Post_Perceptual_Modes = ddply(
      PwData,
      .(directed_mode),
      summarise,
      average_RT = mean(clear_RT, na.rm = TRUE),
      se_RT = sd(exclude_3SD(clear_RT), na.rm = TRUE)/sqrt(length(clear_RT)),
      average_Confidence = mean(clear_Confidence, na.rm = TRUE),
      se_Confidence = sd(exclude_3SD(clear_Confidence), na.rm = TRUE)/sqrt(length(clear_Confidence)),
      n = length(clear_Confidence),
      n_percent = (length(clear_Confidence)/nrow(PwData))*100
      )  

##
## Confidence/RT vs mode: STATS and plot preparation
##

RT_vs_mode <- lmer(clear_RT ~ poly(directed_mode, 2) + (1|study_id/subject_id), data = PwData[!is.na(PwData$directed_mode),])
STAT.RT_vs_mode <- summary(RT_vs_mode)

Confidence_vs_mode <- lmer(clear_Confidence ~ poly(directed_mode, 2) + (1|study_id/subject_id), data = PwData[!is.na(PwData$directed_mode),])
STAT.Confidence_vs_mode <- summary(Confidence_vs_mode)


RT_Confidence_Behav <- 
  ddply(
    PwData[!is.na(PwData$History) & !is.na(PwData$Accuracy),], 
    .(subject_id, study_id),
    summarise,
    diff_Confidence_History = mean(clear_Confidence[History == 1], na.rm = TRUE) -  mean(clear_Confidence[History == 0], na.rm = TRUE),
    diff_Confidence_Stimulus = mean(clear_Confidence[Accuracy == 1], na.rm = TRUE) -  mean(clear_Confidence[Accuracy == 0], na.rm = TRUE),
    diff_RT_History = mean(clear_RT[History == 1], na.rm = TRUE) -  mean(clear_RT[History == 0], na.rm = TRUE),
    diff_RT_Stimulus = mean(clear_RT[Accuracy == 1], na.rm = TRUE) -  mean(clear_RT[Accuracy == 0], na.rm = TRUE),
    confidence_bias = mean(clear_Confidence, na.rm = TRUE),
    mode_strength_bias = mean(strength_mode, na.rm =TRUE))

gathercol = colnames(RT_Confidence_Behav[, c(3,4)])
RT_Confidence_Behav_long_C  <-
  gather(RT_Confidence_Behav[,c(1,3,4)],
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)
RT_Confidence_Behav_long_C$Type = "Confidence"

gathercol = colnames(RT_Confidence_Behav[, c(5,6)])
RT_Confidence_Behav_long_R  <-
  gather(RT_Confidence_Behav[,c(1,5,6)],
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)
RT_Confidence_Behav_long_R$Type = "RT"

RT_Confidence_Behav_long <- rbind(RT_Confidence_Behav_long_R, RT_Confidence_Behav_long_C)

RT_Confidence_Behav_long$Variable <-
  gsub("diff_RT_History", "History", RT_Confidence_Behav_long$Variable)
RT_Confidence_Behav_long$Variable <-
  gsub("diff_RT_Stimulus", "Stimulus", RT_Confidence_Behav_long$Variable)
RT_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_History", "History", RT_Confidence_Behav_long$Variable)
RT_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_Stimulus", "Stimulus", RT_Confidence_Behav_long$Variable)

diff_RT <- lmer(diff ~ Variable + (1 |subject_id), data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "RT",])
STAT.diff_RT <- summary(diff_RT)

diff_Confidence <- lmer(diff ~ Variable + (1|subject_id), data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "Confidence",])
STAT.diff_Confidence <- summary(diff_Confidence)

Summary_RT_Confidence_Behav <- 
  ddply(
    RT_Confidence_Behav_long, 
    .(Variable, Type),
    summarise,
    Mean = mean(diff, na.rm = TRUE),
    Error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))
  
  if (save_summary_data) {
    save(Summary_RT_Confidence_Behav, 
         STAT.diff_RT,
         STAT.diff_Confidence, 
         RT_Confidence_Behav_long,
         RT_Confidence_Behav, 
         STAT.Confidence_vs_mode,
         STAT.RT_vs_mode,
         Post_Perceptual_Modes,
         file = "./Summary_Data/EI_Confidence_RT.Rdata")
  }
  
} else {
  load("./Summary_Data/EI_Confidence_RT.Rdata")
}

```

As expected, stimulus-congruent (as opposed to stimulus-incongruent) choices were associated with faster responses ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$; Figure 2G). 
Intriguingly, whilst controlling for the effect of stimulus-congruence, we found that history-congruent (as opposed to history-incongruent) choices were also characterized by shorter RTs ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$; Figure 2G). 

When analyzing the speed of response against the mode of sensory processing (Figure 2H), we found that RTs were shorter during externally-oriented perception ($\beta_1$ = $`r STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r STAT.RT_vs_mode$coefficients[2,2]`$, T($`r STAT.RT_vs_mode$coefficients[2,3]`$) = $`r STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r STAT.RT_vs_mode$coefficients[2,5]`$). 
Crucially, as indicated by a quadratic relationship between the mode of sensory processing and RTs ($\beta_2$ = $`r STAT.RT_vs_mode$coefficients[3,1]`$ ± $`r STAT.RT_vs_mode$coefficients[3,2]`$, T($`r STAT.RT_vs_mode$coefficients[3,3]`$) = $`r STAT.RT_vs_mode$coefficients[3,4]`$, p = $`r STAT.RT_vs_mode$coefficients[3,5]`$), participants became faster at indicating their perceptual decision when biases toward both internal and external mode grew stronger. This argued against the view that the dynamics of pre-perceptual variables such as arousal or attention provide a plausible alternative explanation for the fluctuating perceptual impact of internal and external information. 

```{r human_training, cache = TRUE}
if (!load_summary_data) {
  
training_RT <- lmer(clear_RT ~ Trial + (1 |study_id/subject_id), data = PwData)
STAT.training_RT <- summary(training_RT)

if (compute_training_history){
training_History <- glmer(History ~ Trial + (1|study_id/subject_id),
        data = PwData,
        family = binomial)
} else {
 training_History <- readRDS(paste(root, "training_history.rds", sep = "")) 
}
STAT.training_History <- summary(training_History)  
  
  if (save_summary_data) {
    save(STAT.training_RT, STAT.training_History,  file = "./Summary_Data/training_human.Rdata")
  }
  
} else {
  load("./Summary_Data/training_human.Rdata")
}

```

Second, it may be assumed that participants tend to repeat preceding choices when they are not yet familiar with the experimental task, leading to history-congruent choices that are caused by insufficient training. In the Confidence database[@Rahnev2020], training effects were visible from RTs that were shortened by increasing exposure to the task ($\beta$ = $`r STAT.training_RT$coefficients[2,1]`$ ± $`r STAT.training_RT$coefficients[2,2]`$, T($`r STAT.training_RT$coefficients[2,3]`$) = $`r STAT.training_RT$coefficients[2,4]`$, p = $`r STAT.training_RT$coefficients[2,5]`$). 
Intriguingly, however, history-congruent choices became more frequent with increased exposure to the task ($\beta$ = $`r STAT.training_History$coefficients[2,1]`$ ± $`r STAT.training_History$coefficients[2,2]`$, z = $`r STAT.training_History$coefficients[2,3]`$, p = $`r STAT.training_History$coefficients[2,4]`$), speaking against the proposition that insufficient training induces seriality in response behavior.

As a third caveat, it could be argued that biases toward internal information reflect a post-perceptual strategy that repeats preceding choices when the subjective confidence in the perceptual decision is low. According to this view, subjective confidence should increase monotonically as biases toward external mode become stronger. 

Stimulus-congruent (as opposed to stimulus-incongruent) choices were associated with enhanced confidence ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$; Figure 2I). 
Yet whilst controlling for the effect of stimulus-congruence, we found that history-congruence also increased confidence ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$; Figure 2I).

When depicted against the mode of sensory processing (Figure 2J), subjective confidence was indeed enhanced when perception was more externally-oriented ($\beta_1$ = $`r STAT.Confidence_vs_mode$coefficients[2,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[2,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[2,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[2,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[2,5]`$). Importantly, however, 
participants were more confident in their perceptual decision for stronger biases toward both internal and external mode ($\beta_2$ = $`r STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[3,5]`$). In analogy to RTs, subjective confidence thus showed a quadratic relationship to the mode of sensory processing (Figure 2J), contradicting the notion that biases toward internal mode may reflect a post-perceptual strategy employed in situations of low subjective confidence.

```{r metacognitive_sensitivity, cache = TRUE}

if (!load_summary_data) {
  
  if (compute_metacognitive_sensitivity) {
  
  source("./Functions/f_compute_metacognition.R", local = knitr::knit_global())
  Metacognitive <- f_compute_metacognition(PwData)
  
} else {
  Metacognitive <- read.csv("./Results/Metacognitive_2.csv")
}

Metacognitive$ratio <-
Metacognitive$average_external / Metacognitive$average_internal

Mode_Meta = data.frame(
  meta_dprime = Metacognitive$meta_dprime,
  average_ei = Metacognitive$average_diff_ei,
  average_internal = Metacognitive$average_internal,
  lag_History = ID_Summary_acf$lag_significant_History,
  lag_Stimulus = ID_Summary_acf$lag_significant_Stimulus,
  dprime =  Metacognitive$dprime,
  strength_mode = ID_mode$strength_mode,
  directed_mode = ID_mode$directed_mode,
  study_id = Metacognitive$study_id,
  subject_id = Metacognitive$subject_id,
  Stimulus = Behav[!is.na(Behav$subject_id), ]$Accuracy,
  History = Behav[!is.na(Behav$subject_id), ]$History
)

Mode_Meta$clear_dprime <- Mode_Meta$dprime
Mode_Meta$clear_dprime[Mode_Meta$clear_dprime > median(Mode_Meta$clear_dprime, na.rm = TRUE) + 3* median(Mode_Meta$clear_dprime, na.rm = TRUE)] = NA
Mode_Meta$clear_dprime[Mode_Meta$clear_dprime < median(Mode_Meta$clear_dprime, na.rm = TRUE) - 3* median(Mode_Meta$clear_dprime, na.rm = TRUE)] = NA


Mode_Meta$clear_meta_dprime <- Mode_Meta$meta_dprime
Mode_Meta$clear_meta_dprime[Mode_Meta$clear_meta_dprime > median(Mode_Meta$clear_meta_dprime, na.rm = TRUE) + 3* median(Mode_Meta$clear_meta_dprime, na.rm = TRUE)] = NA
Mode_Meta$clear_meta_dprime[Mode_Meta$clear_meta_dprime < median(Mode_Meta$clear_meta_dprime, na.rm = TRUE) - 3* median(Mode_Meta$clear_meta_dprime, na.rm = TRUE)] = NA

Mode_Meta$meta_dprime_ratio = Mode_Meta$clear_meta_dprime/Mode_Meta$clear_dprime

Mode_Meta$clear_meta_dprime_ratio <- Mode_Meta$meta_dprime_ratio
Mode_Meta$clear_meta_dprime_ratio[Mode_Meta$clear_meta_dprime_ratio > median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE) + 3* median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE)] = NA
Mode_Meta$clear_meta_dprime_ratio[Mode_Meta$clear_meta_dprime_ratio < median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE) - 3* median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE)] = NA

Meta_dprime_ratio_vs_internal <- lmer(clear_meta_dprime_ratio ~ History + (1 |study_id), data = Mode_Meta)
STAT.Meta_dprime_ratio_vs_internal <- summary(Meta_dprime_ratio_vs_internal)

##
## metacognitive  bias
##

Confidence_bias_vs_strength_mode <- lmer(confidence_bias ~ mode_strength_bias + (1|study_id), data = RT_Confidence_Behav)
STAT.Confidence_bias_vs_strength_mode <- summary(Confidence_bias_vs_strength_mode)
  
  if (save_summary_data) {
    save(STAT.Meta_dprime_ratio_vs_internal,
         STAT.Confidence_bias_vs_strength_mode,
         Mode_Meta,
         Metacognitive,
         file = "./Summary_Data/metacognitive.Rdata")
  }
  
} else {
  load("./Summary_Data/metacognitive.Rdata")
}
```

The above results indicate that reporting behavior and metacognition do not map linearly onto the mode of sensory processing, suggesting that slow fluctuations in the respective impact of external and internal information are most likely to affect perception at an early level of sensory analysis[@St.John-Saaltink2016; @Cicchini2021]. Such low-level processing may integrate perceptual history with external inputs into a decision variable[@Kepecs2008] that influences not only perceptual choices, but also downstream functions such as speed of response and subjective confidence. Consequently, our findings predict that human participants lack full metacognitive insight into how strongly external signals and internal predictions contribute to perceptual decision-making. Stronger biases toward perceptual history thus lead to two seemingly contradictory effects: more frequent errors (Supplemental Figure 1C) and increasing subjective confidence (Figure 2I-J). 

This observation generates an intriguing prediction regarding the association of between-mode fluctuations and perceptual metacognition: Metacognitive efficiency should be lower in individuals who spend more time in internal mode, since their confidence reports are less predictive of whether the corresponding perceptual decision is correct. We computed each participant's M-ratio[@Fleming2014] (meta-d'/d' = `r mean(Mode_Meta$meta_dprime_ratio, na.rm = TRUE)` ± `r sd(Mode_Meta$meta_dprime_ratio, na.rm = TRUE)/sqrt(length(Mode_Meta$meta_dprime_ratio))`) to probe this hypothesis independently of inter-individual differences in perceptual performance. Indeed, we found that biases toward internal information (i.e., as defined by the average probability of history-congruence) were stronger in participants with lower metacognitive efficiency ($\beta$ = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,1]`$ ± $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,2]`$, T($`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,3]`$) = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,4]`$, p = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,5]`$).

## Fluctuations between internal and external mode modulate perceptual performance beyond the effect of general response biases

```{r bias_slider, cache = TRUE, warning = FALSE}

##
## relation of history-dependent biases and general reponse biases to fluctuations in stimulus-congruency
##

if (!load_summary_data) {

if (extract_additional_Slider_Data == TRUE) {
Slider_Data = PwData[, c(1, 4, 5, 7, 36, 37)]

Slider_Data$Preferred = NA
Slider_Data$Preferred_slider = NA
for (subj_idx in unique(Slider_Data$subject_id)) {
print(subj_idx)
responses = Slider_Data[Slider_Data$subject_id == subj_idx, ]$Response

outcomes = unique(responses)

n = rep(NA, length(outcomes))
for (outcome_idx in c(1,length(outcomes))) {
n[outcome_idx] = sum(responses == outcomes[outcome_idx])
}

Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred = rep(0, length(responses))
Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred[responses == outcomes[which(n == max(n))]] = 1
Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred_slider = f_compute_slider(Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred, sliding_window)
}
} else {Slider_Data <- read.csv(paste(root, "Slider_Data.csv", sep = ""))}

slider_History_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ History_slider + Preferred_slider + (1|study_id/subject_id), data = Slider_Data)
STAT.slider_History_Preferred_vs_Accuracy <- summary(slider_History_Preferred_vs_Accuracy)

slider_History_vs_Preferred <- lmer(History_slider ~ Preferred_slider + (1|study_id/subject_id), data = Slider_Data)
STAT.slider_History_vs_Preferred <- summary(slider_History_vs_Preferred)

Global_History_Preferred_vs_Bias <- lmer(Accuracy ~ History + Bias + (1|study_id), data = Behav)
STAT.Global_History_Preferred_vs_Bias <- summary(Global_History_Preferred_vs_Bias)

    Variance_History_General <- 
    ddply(
    Slider_Data, 
    .(subject_id, study_id),
    summarise,
    var_History = sd(History_slider, na.rm = TRUE) - sd(Preferred_slider, na.rm = TRUE)
  )

Variance_History_vs_Preferred <- lmer(var_History ~ 1 + (1|study_id), data = Variance_History_General)
STAT.Variance_History_vs_Preferred <- summary(Variance_History_vs_Preferred)
  
  
  if (save_summary_data) {
    save(Variance_History_General,
         STAT.Variance_History_vs_Preferred,
         STAT.Global_History_Preferred_vs_Bias,
         STAT.slider_History_vs_Preferred,
         STAT.slider_History_Preferred_vs_Accuracy,
         file = "./Summary_Data/bias_slider.Rdata")
  }
  
} else {
  load("./Summary_Data/bias_slider.Rdata")
}
```


```{r mouse_bias_slider, cache = TRUE}

if (!load_summary_data) {
  
  if (extract_mouse_additional_Slider_Data == TRUE) {
    source("./Functions/f_compute_slider.R", local = knitr::knit_global())
    M_Slider_Data = MwData[, c(1,2, 4, 6, 37, 38)]
    
    M_Slider_Data$Preferred = NA
    M_Slider_Data$Preferred_slider = NA
    for (subj_idx in unique(M_Slider_Data$subject_id)) {
      print(subj_idx)
      responses = M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Response
      
      outcomes = unique(responses)
      
      n = rep(NA, length(outcomes))
      for (outcome_idx in c(1,length(outcomes))) {
        n[outcome_idx] = sum(responses == outcomes[outcome_idx])
      }
      
      M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred = rep(0, length(responses))
      M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred[responses == outcomes[which(n == max(n))]] = 1
      M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred_slider = f_compute_slider(M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred, sliding_window)
    }
  } else {M_Slider_Data <- read.csv(paste(root, "M_Slider_Data.csv", sep = ""))}
  
  M_slider_History_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ History_slider + Preferred_slider + (1|subject_id/session_id), data = M_Slider_Data)
  M_STAT.slider_History_Preferred_vs_Accuracy <- summary(M_slider_History_Preferred_vs_Accuracy)
  
  M_slider_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ Preferred_slider + (1|subject_id/session_id), data = M_Slider_Data)
  M_STAT.slider_Preferred_vs_Accuracy <- summary(M_slider_Preferred_vs_Accuracy)
  
  M_slider_History_vs_Preferred <- lmer(History_slider ~ Preferred_slider + (1|subject_id/session_id), data = M_Slider_Data)
  M_STAT.slider_History_vs_Preferred <- summary(M_slider_History_vs_Preferred)
  
  M_Global_History_Preferred_vs_Bias <- lm(Accuracy ~ History + Bias, data = M_Behav)
  M_STAT.Global_History_Preferred_vs_Bias <- summary(M_Global_History_Preferred_vs_Bias)
  
  M_Variance_History_General <- 
    ddply(
      M_Slider_Data, 
      .(subject_id, session_id),
      summarise,
      var_History = sd(History_slider, na.rm = TRUE) - sd(Preferred_slider, na.rm = TRUE)
    )
  
  M_Variance_History_vs_Preferred <- lmer(var_History ~ 1 + (1|session_id), data = M_Variance_History_General)
  M_STAT.Variance_History_vs_Preferred <- summary(M_Variance_History_vs_Preferred)
  
  if (save_summary_data) {
    save(M_STAT.slider_History_Preferred_vs_Accuracy,
         M_STAT.slider_Preferred_vs_Accuracy,
         M_STAT.slider_History_vs_Preferred,
         M_STAT.Global_History_Preferred_vs_Bias,
         M_Variance_History_General,
         M_STAT.Variance_History_vs_Preferred,
         file = "./Summary_Data/mouse_bias_slider.Rdata")
  }
  
} else {
  load("./Summary_Data/mouse_bias_slider.Rdata")
}
```

The above sections provide correlative evidence that recurring intervals of stronger perceptual history temporally reduce the participants' sensitivity to external information. Importantly, the history-dependent biases that characterize internal mode processing must be differentiated from general response biases. In binary perceptual decision-making, general response biases are defined by a propensity to choose one of the two outcomes more often than the alternative. Indeed, in the experiments considered here, participants selected the more frequent of the two possible outcomes in `r mean(Behav$Bias, na.rm = TRUE)`% ± `r sd(Behav$Bias, na.rm = TRUE)/sqrt(length(Behav$Bias))`% of trials. 

Two caveats have to be considered to make sure that the effect of history-congruence is distinct from the effect of general response biases. First, history-congruent states become more likely for larger response biases that cause a increasing imbalance in the likelihood of the two outcomes ($\beta$ = $`r STAT.slider_History_vs_Preferred$coefficients[2,1]`$ ± $`r STAT.slider_History_vs_Preferred$coefficients[2,2]`$, T($`r STAT.slider_History_vs_Preferred$coefficients[2,3]`$) = $`r STAT.slider_History_vs_Preferred$coefficients[2,4]`$, p = $`r STAT.slider_History_vs_Preferred$coefficients[2,5]`$). One may thus ask whether the autocorrelation of history-congruence could be entirely driven by general response biases. Yet the above analyses account for general response biases by computing group-level autocorrelations (see Figure 2C) relative to randomly permuted data (i.e., by subtracting the autocorrelation of randomly permuted data from the raw autocorrelation curve). This precludes that general response biases contribute to the observed autocorrelation of history-congruence (see Supplemental Figure S5 for a visualization of the correction procedure for simulated data with general response biases ranging from 60 to 90%). 

Second, it may be argued that fluctuations in perceptual performance may be solely driven by ongoing changes in the strength of general response biases.
To assess the links between dynamic fluctuations in stimulus-congruence on the one hand and history-congruence as well as general response bias on the other hand, we computed all variables as dynamic probabilities in sliding windows of ± 5 trials (see Figure 1C). Linear mixed effects modeling indicated that fluctuations in history-congruent biases were larger in amplitude than the corresponding fluctuations in general response biases ($\beta_0$ = $`r STAT.Variance_History_vs_Preferred$coefficients[1,1]`$ ± $`r STAT.Variance_History_vs_Preferred$coefficients[1,2]`$, T($`r STAT.Variance_History_vs_Preferred$coefficients[1,3]`$) = $`r STAT.Variance_History_vs_Preferred$coefficients[1,4]`$, p = $`r STAT.Variance_History_vs_Preferred$coefficients[1,5]`$). 
Crucially, ongoing fluctuations in history-congruence had a significant effect on stimulus-congruence ($\beta_1$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) 
beyond the effect of ongoing changes in general response biases ($\beta_2$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$). In sum, the above control analyses confirm that the observed influence of preceding choices on perceptual decision-making cannot be reduced to general response biases.

## Internal mode is characterized by lower thresholds as well as by history-dependent changes in biases and lapses

```{r psychometric_function, cache = TRUE}
if (!load_summary_data) {
  Behav_PM <-
    ddply(
      PwData,
      .(subject_id, study_id, round(Difficulty, digits = 0)),
      summarise,
      p_Accuracy = sum(Accuracy == 1, na.rm = TRUE) / length(Accuracy),
      n_Accuracy = length(Accuracy)
    )
  
  colnames(Behav_PM) <-
    c("subject_id",
      "study_id",
      "Difficulty",
      "p_Accuracy",
      "n_Accuracy")
  
  Behav_PM <-
    Behav_PM[Behav_PM$Difficulty >= -3 &
               Behav_PM$Difficulty <= 3 &
               !is.na(Behav_PM$Difficulty), ]
  
  PwData$rDifficulty <- round(PwData$Difficulty, digits = 0)
  
  P_Behav = Behav
  P_Behav = P_Behav[P_Behav$subject_id %in% unique(PwData[!is.na(PwData$Difficulty),]$subject_id),]
  
  N_PM <- ddply(PwData,
                .(rDifficulty),
                summarise,
                n_trials = length(rDifficulty))
  
  difficulty_levels = seq(-3, 3, 1)
  
  type_list = c(
    "full_b",
    "full_0",
    "full_1",
    "external_b",
    "external_0",
    "external_1",
    "internal_b",
    "internal_0",
    "internal_1"
  )
  
  P_Behav = cbind(P_Behav, matrix(NA, ncol = length(type_list) * 5, nrow = nrow(P_Behav)))
  colnames(P_Behav) <-
    c(
      "subject_id",
      "study_id",
      "History",
      "Min_History",
      "Max_History",
      "Accuracy",
      "Min_Accuracy",
      "Max_Accuracy",
      "Stimulus_History",
      "Stimulus_Bias",
      "Response_Bias",
      "Bias",
      "null_History",
      "full_b_lower_lapse",
      "full_b_higher_lapse",
      "full_b_av_lapse",
      "full_b_bias",
      "full_b_threshold",
      "full_0_lower_lapse",
      "full_0_higher_lapse",
      "full_0_av_lapse",
      "full_0_bias",
      "full_0_threshold",
      "full_1_lower_lapse",
      "full_1_higher_lapse",
      "full_1_av_lapse",
      "full_1_bias",
      "full_1_threshold",
      "external_b_lower_lapse",
      "external_b_higher_lapse",
      "external_b_av_lapse",
      "external_b_bias",
      "external_b_threshold",
      "external_0_lower_lapse",
      "external_0_higher_lapse",
      "external_0_av_lapse",
      "external_0_bias",
      "external_0_threshold",
      "external_1_lower_lapse",
      "external_1_higher_lapse",
      "external_1_av_lapse",
      "external_1_bias",
      "external_1_threshold",
      "internal_b_lower_lapse",
      "internal_b_higher_lapse",
      "internal_b_av_lapse",
      "internal_b_bias",
      "internal_b_threshold",
      "internal_0_lower_lapse",
      "internal_0_higher_lapse",
      "internal_0_av_lapse",
      "internal_0_bias",
      "internal_0_threshold",
      "internal_1_lower_lapse",
      "internal_1_higher_lapse",
      "internal_1_av_lapse",
      "internal_1_bias",
      "internal_1_threshold"
    )
  
  
  ##
  ## FIT
  ##
  
  for (subj_idx in unique(P_Behav$subject_id)) {
    print(subj_idx)
    
    Input_Data = PwData[PwData$subject_id == subj_idx &
                          PwData$rDifficulty %in% difficulty_levels, c(
                            "Stimulus",
                            "Response",
                            "rDifficulty",
                            "Response_minus_1",
                            "scaled_directed_mode"
                          )]
    
    max_env = which(Input_Data$Stimulus == max(Input_Data$Stimulus, na.rm = TRUE))
    min_env = which(Input_Data$Stimulus == min(Input_Data$Stimulus, na.rm = TRUE))
    Input_Data$Stimulus[max_env] = 1
    Input_Data$Stimulus[min_env] = 0
    
    max_response = which(Input_Data$Response == max(Input_Data$Response, na.rm = TRUE))
    min_response = which(Input_Data$Response == min(Input_Data$Response, na.rm = TRUE))
    Input_Data$Response[max_response] = 1
    Input_Data$Response[min_response] = 0
    
    max_response_minus_1 = which(Input_Data$Response_minus_1 == max(Input_Data$Response_minus_1, na.rm = TRUE))
    min_response_minus_1 = which(Input_Data$Response_minus_1 == min(Input_Data$Response_minus_1, na.rm = TRUE))
    Input_Data$Response_minus_1[max_response_minus_1] = 1
    Input_Data$Response_minus_1[min_response_minus_1] = 0
    
    for (type_idx in type_list) {
      if (type_idx == "full_b") {
        trial_selector = rep(TRUE, nrow(Input_Data))
      } else if (type_idx == "full_0") {
        trial_selector =
          Input_Data$Response_minus_1 == 0
      } else if (type_idx == "full_1") {
        trial_selector =
          Input_Data$Response_minus_1 == 1
      } else if (type_idx == "internal_b") {
        trial_selector =
          Input_Data$scaled_directed_mode < 0
      } else if (type_idx == "internal_0") {
        trial_selector =
          Input_Data$Response_minus_1 == 0 &
          Input_Data$scaled_directed_mode < 0
      } else if (type_idx == "internal_1") {
        trial_selector =
          Input_Data$Response_minus_1 == 1 &
          Input_Data$scaled_directed_mode < 0
      } else if (type_idx == "external_b") {
        trial_selector =
          Input_Data$scaled_directed_mode > 0
      } else if (type_idx == "external_0") {
        trial_selector =
          Input_Data$Response_minus_1 == 0 &
          Input_Data$scaled_directed_mode > 0
      } else if (type_idx == "external_1") {
        trial_selector =
          Input_Data$Response_minus_1 == 1 &
          Input_Data$scaled_directed_mode > 0
      }
      
      source("./Functions/fit_psychometric_function.R")
      par = c(0.0, 0.0, 0, 5)
      lower = c(0.0, 0.0,-5, 0.5)
      upper = c(0.5, 0.5, 5, 25)
      
      
      add_PM =  optimx(
        par,
        fit_psychometric_function,
        Input_Data = Input_Data[trial_selector[!is.na(trial_selector)], ],
        lower = lower,
        upper = upper,
        method = "L-BFGS-B"
      )
      
      P_Behav[P_Behav$subject_id == subj_idx, grepl(type_idx, colnames(P_Behav))] <-
        c(add_PM$p1, add_PM$p2, mean(c(add_PM$p1, add_PM$p2)), add_PM$p3, add_PM$p4)
    }
  }
  
  STAT.PM_History_lapse_bias_threshold <-
    summary(lmer(
      History ~ 1 + full_b_av_lapse + abs(full_b_bias) + full_b_threshold + (1 |
                                                                               study_id),
      data = P_Behav
    ))
  
  STAT.PM_Accuracy_History_lapse <-
    summary(lmer(Accuracy ~ 1 + History + full_b_av_lapse + (1 |
                                                               study_id),
                 data = P_Behav))
  
  P_Behav$threshold_diff = P_Behav$external_b_threshold - P_Behav$internal_b_threshold
  P_Behav$bias_diff = abs(P_Behav$external_b_bias) - abs(P_Behav$internal_b_bias)
  P_Behav$lapse_diff = P_Behav$external_b_av_lapse - P_Behav$internal_b_av_lapse
  
  STAT.PM_diff_bias <-
    summary(lmer(bias_diff ~ 1 + lapse_diff + threshold_diff + (1 |
                                                                  study_id),
                 data = P_Behav))
  
  STAT.PM_diff_lapse <-
    summary(lmer(lapse_diff ~ 1 + bias_diff + threshold_diff + (1 |
                                                                  study_id),
                 data = P_Behav))
  
  STAT.PM_diff_threshold <-
    summary(lmer(threshold_diff ~ 1 + bias_diff + lapse_diff + (1 |
                                                                  study_id),
                 data = P_Behav))
  
    STAT.PM_zero_bias_b <-
    summary(lmer(full_b_bias ~ 1 + (1 | study_id), data = P_Behav))
    
    STAT.PM_zero_bias_0 <-
    summary(lmer(full_0_bias ~ 1 + (1 | study_id), data = P_Behav))
    
    STAT.PM_zero_bias_1 <-
    summary(lmer(full_0_bias ~ 1 + (1 | study_id), data = P_Behav))
    
    
  STAT.PM_diff_lapse_1_lower <- summary(lmer(external_1_lower_lapse - internal_1_lower_lapse ~ 1 + (1|study_id), data = P_Behav))
  STAT.PM_diff_lapse_0_lower <- summary(lmer(external_0_lower_lapse - internal_0_lower_lapse ~ 1 + (1|study_id), data = P_Behav))
  STAT.PM_diff_lapse_1_higher <- summary(lmer(external_1_higher_lapse - internal_1_higher_lapse ~ 1 + (1|study_id), data = P_Behav))
  STAT.PM_diff_lapse_0_higher <- summary(lmer(external_0_higher_lapse - internal_0_higher_lapse ~ 1 + (1|study_id), data = P_Behav))
  
  STAT.PM_diff_threshold_0_1 <- summary(lmer(full_0_threshold - full_1_threshold ~ 1 + (1|study_id), data = P_Behav))
  
##
## Generate parameter summary
##
gathercol = colnames(P_Behav[, seq(14, ncol(P_Behav) - 3)])
P_Behav_long  <-
  gather(P_Behav,
         "Variable",
         "Estimate",
         gathercol,
         factor_key = TRUE)

P_Behav_long$Conditioned = NA
P_Behav_long$Type = NA

P_Behav_long[grepl("full", P_Behav_long$Variable), ]$Type = "full"
P_Behav_long[grepl("internal", P_Behav_long$Variable), ]$Type = "internal"
P_Behav_long[grepl("external", P_Behav_long$Variable), ]$Type = "external"
P_Behav_long$Type <-
  factor(P_Behav_long$Type, levels = c("full", "internal", "external"))

P_Behav_long[grepl("_b", P_Behav_long$Variable), ]$Conditioned = "all"
P_Behav_long[grepl("_0", P_Behav_long$Variable), ]$Conditioned = "y(t-1) = 0"
P_Behav_long[grepl("_1", P_Behav_long$Variable), ]$Conditioned = "y(t-1) = 1"

P_Behav_long$Variable_full = P_Behav_long$Variable
P_Behav_long$Variable <- gsub(c("full_"), "", P_Behav_long$Variable)
P_Behav_long$Variable <-
  gsub("internal_", "", P_Behav_long$Variable)
P_Behav_long$Variable <-
  gsub("external_", "", P_Behav_long$Variable)
P_Behav_long$Variable <- gsub("b_", "", P_Behav_long$Variable)
P_Behav_long$Variable <- gsub("1_", "", P_Behav_long$Variable)
P_Behav_long$Variable <- gsub("0_", "", P_Behav_long$Variable)

##
## simulate
##
Sim_PM = data.frame()
stim_range = seq(-5,+5, 0.25)
for (subj_idx in unique(P_Behav$subject_id)) {
  print(subj_idx)
  for (type_idx in type_list) {
  par = P_Behav_long[P_Behav_long$subject_id == subj_idx & grepl(type_idx, P_Behav_long$Variable_full), ]$Estimate[c(1, 2, 4, 5)]
  lower_lapse = par[1]
  higher_lapse = par[2]
  bias = par[3]
  contrast_threshold = par[4]
  
  Sim_PM = rbind(
    Sim_PM,
    add_sim_PM = data.frame(
      subject_id = subj_idx,
      study_id = unique(P_Behav[P_Behav$subject_id == subj_idx,]$study_id),
      Stimulus = stim_range,
      y_prob = lower_lapse + (1 - lower_lapse - higher_lapse) *  (erf((stim_range - bias) / contrast_threshold) + 1) / 2,
      Variable = type_idx
    )
)
}
}

Sim_PM$Conditioned = NA
Sim_PM$Type = NA

Sim_PM[grepl("full", Sim_PM$Variable), ]$Type = "full"
Sim_PM[grepl("internal", Sim_PM$Variable), ]$Type = "internal"
Sim_PM[grepl("external", Sim_PM$Variable), ]$Type = "external"
Sim_PM$Type <-
  factor(Sim_PM$Type, levels = c("full", "internal", "external"))


Sim_PM[grepl("_b", Sim_PM$Variable), ]$Conditioned = "all"
Sim_PM[grepl("_0", Sim_PM$Variable), ]$Conditioned = "y(t-1) = 0"
Sim_PM[grepl("_1", Sim_PM$Variable), ]$Conditioned = "y(t-1) = 1"

Sim_PM$Conditioned <- as.factor(Sim_PM$Conditioned)
Sim_PM$subject_id <- as.factor(Sim_PM$subject_id)

Sim_PM_group <-
    ddply(
    Sim_PM,
    .(Stimulus, Conditioned, Type),
    summarise,
    mean_y_prob = mean(y_prob, na.rm = TRUE),
    error_y_prob = sd(y_prob, na.rm = TRUE)/sqrt(length(y_prob))
  )

Sim_PM_group$Type <-
  factor(Sim_PM_group$Type, levels = c("full", "internal", "external"))
  

  if (save_summary_data) {
    save(
      P_Behav,
      STAT.PM_History_lapse_bias_threshold,
      STAT.PM_Accuracy_History_lapse,
      STAT.PM_diff_threshold,
      STAT.PM_diff_lapse,
      STAT.PM_diff_bias,
      STAT.PM_zero_bias_b,
      STAT.PM_zero_bias_0,
      STAT.PM_zero_bias_1,
      STAT.PM_diff_lapse_1_lower,
      STAT.PM_diff_lapse_0_lower,
      STAT.PM_diff_lapse_1_higher,
      STAT.PM_diff_lapse_0_higher,
      STAT.PM_diff_threshold_0_1,
      P_Behav_long,
      Sim_PM,
      Sim_PM_group,
      file = "./Summary_Data/psychometric_data.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/psychometric_data.Rdata")
}

```

In a final control analysis, we asked whether history-independent changes in biases and lapses may provide an alternative explanation of internal mode processing. To this end, we estimated full and history-conditioned psychometric curves to investigate how internal and external mode relate to biases (i.e., the horizontal position of the psychometric curve), lapses (i.e, the asymptotes of the psychometric curve) and thresholds (i.e., 1/sensitivity, estimated from the slope of the psychometric curve). We used a maximum likelihood procedure to predict trial-wise choices $y$ ($y = 0$ and $y = 1$ for outcomes A and B respectively) from the choice probabilities $y_p$. $y_p$ was computed from difficulty-weighted inputs $s_w$ via a parametric error function defined by the parameters $\gamma$ (lower lapse), $\delta$ (upper lapse), $\mu$ (bias) and $t$ (threshold; see Methods for details):

\begin{equation}
y_p = \gamma + (1 - \gamma - \delta) *  (erf(\frac{s_w + \mu}{t}) + 1) / 2
\end{equation}

Across the full dataset (i.e., irrespective of the preceding perceptual choice $y_{t-1}$), biases $\mu$ were distributed around zero (`r mean(P_Behav$full_b_bias, na.rm = TRUE)` ± `r sd(P_Behav$full_b_bias, na.rm = TRUE)/sqrt(length(P_Behav$full_b_bias))`; $\beta_0$ = $`r STAT.PM_zero_bias_b$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_b$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_b$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_b$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_b$coefficients[1,5]`$; see Figure 3A and B, upper panel). 
When conditioned on perceptual history, biases $\mu$ varied according to the preceding perceptual choice, with negative biases for $y_{t-1} = 0$ (`r -mean(P_Behav$full_0_bias, na.rm = TRUE)` ± `r sd(P_Behav$full_0_bias, na.rm = TRUE)/sqrt(length(P_Behav$full_0_bias))`; $\beta_0$ = $`r STAT.PM_zero_bias_0$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_0$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_0$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_0$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_0$coefficients[1,5]`$) 
and positive biases for $y_{t-1} = 1$ (`r -mean(P_Behav$full_1_bias, na.rm = TRUE)` ± `r sd(P_Behav$full_1_bias, na.rm = TRUE)/sqrt(length(P_Behav$full_1_bias))`; $\beta_0$ = $`r STAT.PM_zero_bias_1$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_1$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_1$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_1$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_1$coefficients[1,5]`$).
Absolute biases $|\mu|$ were larger in internal mode (`r mean(abs(P_Behav$internal_b_bias), na.rm = TRUE)` ± `r sd(abs(P_Behav$internal_b_bias), na.rm = TRUE)/sqrt(length(abs(P_Behav$internal_b_bias)))`) as compared to external mode (`r mean(abs(P_Behav$external_b_bias), na.rm = TRUE)` ± `r sd(abs(P_Behav$external_b_bias), na.rm = TRUE)/sqrt(length(abs(P_Behav$external_b_bias)))`; $\beta_0$ = $`r STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r STAT.PM_diff_bias$coefficients[1,2]`$, T($`r STAT.PM_diff_bias$coefficients[1,3]`$) = $`r STAT.PM_diff_bias$coefficients[1,4]`$, p = $`r STAT.PM_diff_bias$coefficients[1,5]`$; controlling for differences in lapses and thresholds). 

Lower and upper lapses amounted to $\gamma$ = `r mean(P_Behav$full_b_lower_lapse, na.rm = TRUE)` ± `r sd(P_Behav$full_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_Behav$full_b_higher_lapse))` and $\delta$ = `r mean(P_Behav$full_b_higher_lapse, na.rm = TRUE)` ± `r sd(P_Behav$full_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_Behav$full_b_higher_lapse))` (see Figure 3A, C and D). 
Lapses were larger in internal mode ($\gamma$ = `r mean(P_Behav$internal_b_lower_lapse, na.rm = TRUE)` ± `r sd(P_Behav$internal_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_Behav$internal_b_higher_lapse))`, $\delta$ = `r mean(P_Behav$internal_b_higher_lapse, na.rm = TRUE)` ± `r sd(P_Behav$internal_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_Behav$internal_b_higher_lapse))`) as compared to external mode ($\gamma$ = `r mean(P_Behav$external_b_lower_lapse, na.rm = TRUE)` ± `r sd(P_Behav$external_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_Behav$external_b_higher_lapse))`, $\delta$ = `r mean(P_Behav$external_b_higher_lapse, na.rm = TRUE)` ± `r sd(P_Behav$external_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_Behav$external_b_higher_lapse))`; $\beta_0$ = $`r STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse$coefficients[1,5]`$; controlling for differences in biases and thresholds). 

Conditioning on the previous perceptual choice revealed that the between-mode difference in lapse was not general, but depended on perceptual history: 
For $y_{t-1} = 0$, 
only higher lapses $\delta$ differed between internal and external mode ($\beta_0$ = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,5]`$), 
whereas lower lapses $\gamma$ did not ($\beta_0$ = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,5]`$). 
Vice versa, 
for $y_{t-1} = 1$, 
lower lapses $\gamma$ differed between internal and external mode ($\beta_0$ = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,5]`$), 
whereas higher lapses $\delta$ did not ($\beta_0$ = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,5]`$).

Thresholds $t$ were estimated at `r mean(P_Behav$full_b_threshold, na.rm = TRUE)` ± `r sd(P_Behav$full_b_threshold, na.rm = TRUE)/sqrt(length(P_Behav$full_b_threshold))` (see Figure 3A and E). Thresholds $t$ were larger in internal mode (`r mean(P_Behav$internal_b_threshold, na.rm = TRUE)` ± `r sd(P_Behav$internal_b_threshold, na.rm = TRUE)/sqrt(length(P_Behav$internal_b_threshold))`) as compared to external mode (`r mean(P_Behav$external_b_threshold, na.rm = TRUE)` ± `r sd(P_Behav$external_b_threshold, na.rm = TRUE)/sqrt(length(P_Behav$external_b_bias))`; $\beta_0$ = $`r STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold$coefficients[1,2]`$, T($`r STAT.PM_diff_threshold$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold$coefficients[1,5]`$; controlling for differences in biases and lapses). 
In contrast to the bias $\mu$ and the lapse rates $\gamma$ and $\delta$, thresholds $t$ were not modulated by perceptual history ($\beta_0$ = $`r STAT.PM_diff_threshold_0_1$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold_0_1$coefficients[1,2]`$, T($`r   STAT.PM_diff_threshold_0_1$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold_0_1$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold_0_1$coefficients[1,5]`$).

In sum, the above analyses showed that internal and external mode differ with respect to biases, lapses and thresholds. Internally-biased processing was characterized by higher thresholds, indicating a reduced sensitivity to sensory information, as well as by larger biases and lapses. Importantly, between-mode differences in biases and lapses strongly depended on perceptual history. This confirmed that internal mode processing cannot be explained solely on the ground of a general (i.e., history-independent) increase in lapses or bias.  

## Mice waver between external and internal modes of perceptual decision-making

```{r load_preprocess_mouse_data_new, cache = TRUE}

if (!load_summary_data) {

if (preprocess_mouse_data) {
  loc = paste(root, "/Mouse/export_dataframe.csv", sep = "")
  n_permutations = 100
  
  source("./Functions/f_preprocess_mouse_data.R",
  local = knitr::knit_global())
  MwData <- f_preprocess_mouse_data(loc, n_permutations)
}

## 
## Dynamic probability of stimulus- and history-congruence
## 
if (compute_slider_mouse_data) {
source("./Functions/f_compute_slider.R", local = knitr::knit_global())

for (subj_idx in unique(MwData$subject_id)) {
print(subj_idx)
for (session_idx in unique(MwData$subject_id)) {
index = (MwData$subject_id == subj_idx & MwData$session_id == session_idx)

MwData$History_slider[index] <- f_compute_slider(MwData$History[index], sliding_window)
MwData$Accuracy_slider[index] <- f_compute_slider(MwData$Accuracy[index], sliding_window)
MwData$RT_slider[index] <- f_compute_slider(MwData$RT[index], sliding_window)
}
  
}
}

##
## Prepare diagnostics for exclusion criteria
##

if (filter_mouse_data){
MwData$keep = NA
for (subject_idx in unique(MwData$subject_id)) {
    print(subject_idx)
    for (session_idx in unique(MwData[MwData$subject_id == subject_idx,]$session_id)) { 
    print(session_idx)
      
      M_Behav_excl <- data.frame()
    M_Behav_excl <- 
    ddply(
    MwData[MwData$subject_id == subject_idx & MwData$session_id == session_idx,], 
    .(Difficulty),
    summarise,
    History = mean(History, na.rm = TRUE)*100, 
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    median_RT = median(RT, na.rm = TRUE)
  )

       if (max(M_Behav_excl[M_Behav_excl$Difficulty >= 0.5,]$Accuracy, na.rm = TRUE) > 80){
         MwData[MwData$subject_id == subject_idx & MwData$session_id == session_idx,]$keep = 1
       } else {
         MwData[MwData$subject_id == subject_idx & MwData$session_id == session_idx,]$keep = 0
       }
    
      }
}
}

## load fully preprocessed data
if (load_mouse_data){
  #MwData <- read.csv(paste(root, "/Mouse/balanced_MwData_slider.csv", sep = ""))
  MwData <- read.csv(paste(root, "/Mouse/MwData_new_preproc_filter_slider_excl.csv", sep = ""))
}


##
## Apply exlcusion criteria
##

if (apply_mouse_exclusion_criteria){
MwData <- MwData[MwData$keep == 1,]


M_Behav_excl <-
  ddply(
    MwData,
    .(subject_id, session_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100,
    History = mean(History, na.rm = TRUE)*100,
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    Imbalance = mean(abs(0.5-prob_left), na.rm = TRUE)*100 + 50,
    RT = mean(RT, na.rm = TRUE)
  )


data_select = M_Behav_excl[M_Behav_excl$Imbalance<60,c(1,2)]

sMwData = data.frame()
for (subj_idx in unique(data_select$subject_id)){
print(subj_idx)
row_select = double()
row_select = (MwData$subject_id == subj_idx & MwData$session_id %in% data_select[data_select$subject_id == subj_idx,]$session_id)
sMwData <- rbind(sMwData, MwData[which(row_select),])
}
MwData <- sMwData
}


MwData$clear_RT = MwData$RT
MwData$scaled_RT = scale(MwData$clear_RT)
MwData$clear_RT[MwData$clear_RT > median(MwData$clear_RT, na.rm = TRUE) + 3*median(MwData$clear_RT, na.rm =TRUE)] = NA

MwData$clear_RT2 = MwData$RT
MwData$clear_RT2[MwData$clear_RT2 > median(MwData$clear_RT2, na.rm = TRUE) + 1.5*median(MwData$clear_RT2, na.rm =TRUE)] = NA

##
## compute weighted stimulus
##
MwData$w_Stimulus = 0.5 + (MwData$Stimulus-0.5)*MwData$Difficulty

##
## go to milliseconds
##
MwData$clear_RT <- MwData$clear_RT*1000
MwData$clear_RT2 <- MwData$clear_RT2*1000

  n_trials_mice = nrow(MwData)
  n_participants_mice = length(unique(MwData$subject_id))
  
  if (save_summary_data) {
    save(n_trials_mice, n_participants_mice, 
         file = "./Summary_Data/size_IBL_dataset.Rdata")
  }
  
} else {
  load("./Summary_Data/size_IBL_dataset.Rdata")
}

```

In a prominent functional explanation for serial dependencies[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @St.John-Saaltink2016; @Fritsche2020; @Urai2017; @Urai2019; @Hsu2020], perceptual history is cast as an internal prediction that leverages the temporal autocorrelation of natural environments for efficient decision-making[@Dong1995; @Chopin2012; @Burr2014; @Braun2018; @Bergen2019]. We reasoned that, since this autocorrelation is one of the most basic features of our sensory world, fluctuating biases toward preceding perceptual choices should not be a uniquely human phenomenon. 

To test whether externally and internally oriented modes of processing exist beyond the human mind, we analyzed data on perceptual decision-making in mice that were extracted from the International Brain Laboratory (IBL) dataset[@Aguillon-Rodriguez2020]. Here, we restricted our analyses to the *basic* task[@Aguillon-Rodriguez2020], in which mice responded to gratings of varying contrast that appeared either in the left or right hemifield of with equal probability. We excluded sessions in which mice did not respond correctly to stimuli presented at a contrast above 50% in more than 80% of trials (see Methods), which yielded a final sample of N = `r n_participants_mice` adequately trained mice that went through $`r n_trials_mice/1000000`$ million trials.  

```{r overview_mouse_data, cache = TRUE}
if (!load_summary_data) {
  
M_Behav <- 
  ddply(
    MwData, 
    .(subject_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100, 
    History = mean(History, na.rm = TRUE)*100, 
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    Bias = abs(0.5 - sum(Response == max(Response), na.rm = TRUE)/length(Response))*100 + 50
  )


M_Behav$null_History = M_Behav$History - 50
M_STAT.Global_History_Accuracy = t.test(M_Behav$null_History)


MwData[(is.na(MwData$Stimulus)), c("Accuracy", "History", "History_slider", "Accuracy_slider")] = NA

##
## History effects global
##

gathercol = colnames(M_Behav[, c(3,4)])
M_Behav_long  <-
gather(M_Behav,
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

M_Behav_diff <- 
   ddply(
    MwData[!is.na(MwData$Accuracy),], 
    .(subject_id, Accuracy),
    summarise,
    History = mean(History, na.rm = TRUE)*100
  )

gathercol <- colnames(M_Behav_diff[,c(3)])

M_Behav_diff_long  <-
gather(M_Behav_diff[, c(1,2,3)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

M_Behav_diff_long$Accuracy <- as.character(M_Behav_diff_long$Accuracy) 
M_Behav_diff_long$Accuracy <-
gsub("0", "error", M_Behav_diff_long$Accuracy)
M_Behav_diff_long$Accuracy <-
gsub("1", "correct", M_Behav_diff_long$Accuracy)

M_STAT.diff_History_Accuracy = t.test(M_Behav_diff[M_Behav_diff$Accuracy == 0,]$History, M_Behav_diff[M_Behav_diff$Accuracy == 1,]$History, paired = TRUE) 
  
  if (save_summary_data) {
    save(M_Behav, 
         M_STAT.Global_History_Accuracy, 
         M_Behav_long,
         M_Behav_diff,
         M_Behav_diff_long, 
         M_STAT.diff_History_Accuracy,
         file = "./Summary_Data/Summary_Behavior_IBL.Rdata")
  }
  
} else {
  load("./Summary_Data/Summary_Behavior_IBL.Rdata")
}
```

```{r compute_mouse_logreg, cache = TRUE}

if (!load_summary_data) {
  ##
  ## Logistic regression
  ##
  if (compute_mouse_logreg) {
    source("./Functions/f_compute_mouse_logreg.R",
           local = knitr::knit_global())
    M_LogAggr <- f_compute_mouse_logreg(MwData)
  } else {
    M_LogAggr <- read.csv("./Results/M_LogAggr_new_preproc.csv")
  }
  
  M_LogAggr$diff_AIC = (M_LogAggr$full_AIC - M_LogAggr$reduced_AIC)
  
  M_STAT.LogReg = t.test(M_LogAggr$diff_AIC)
  
  if (compute_mouse_logreg) {
    M_full_logit_Sum <-
      glmer(
        Response ~ Stimulus + Response_minus_1 + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1, ],
        family = binomial
      )
    
     M_reduced_logit_Sum <-
      glmer(
        Response ~ Stimulus + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1, ],
        family = binomial
      )
  } else {
    M_full_logit_Sum <-
      readRDS(file = "./Results/M_full_logit_Sum_new_preproc.rds")
    M_reduced_logit_Sum <-
      readRDS(file = "./Results/M_reduced_logit_Sum_new_preproc.rds")
  }
  M_STAT.full_logit_Sum <- summary(M_full_logit_Sum)
  M_STAT.reduced_logit_Sum <- summary(M_reduced_logit_Sum)
  
  if (save_summary_data) {
    save(M_LogAggr, 
         M_STAT.LogReg, 
         M_STAT.full_logit_Sum,
         M_STAT.reduced_logit_Sum,
         file = "./Summary_Data/LogReg_mice.Rdata")
  }
  
} else {
  load("./Summary_Data/LogReg_mice.Rdata")
}
```

In line with humans, mice were biased toward perceptual history in `r mean(M_Behav$History, na.rm = TRUE)`% ± `r sd(M_Behav$History, na.rm = TRUE)/sqrt(length(M_Behav$History))`% of trials (T(`r M_STAT.Global_History_Accuracy$parameter`) = `r M_STAT.Global_History_Accuracy$statistic`, p = $`r M_STAT.Global_History_Accuracy$p.value`$; Figure 4A and Supplemental Figure S1D). 
Perceptual history effects remained significant ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[3,4]`$) when controlling for external sensory information ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward one of the two potential outcomes ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[1,4]`$; see Supplemental Figure S4C-D for model comparisons and $\beta$ values computed within individual mice). 

In the *basic* task of the IBL dataset[@Aguillon-Rodriguez2020], stimuli were presented at random in either the left or right hemifield. Stronger biases toward perceptual history should therefore decrease perceptual performance. Indeed, history-congruent choices were more frequent when perception was stimulus-incongruent (`r mean(M_Behav_diff[M_Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)`% ± `r sd(M_Behav_diff[M_Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)/sqrt(length(Behav_diff[M_Behav_diff$Accuracy == 0,]$History))`%) as opposed to stimulus-congruent (`r mean(M_Behav_diff[M_Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)`% ± `r sd(M_Behav_diff[M_Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)/sqrt(length(M_Behav_diff[Behav_diff$Accuracy == 1,]$History))`%, T(`r M_STAT.diff_History_Accuracy$parameter`) = `r M_STAT.diff_History_Accuracy$statistic`, p = $`r M_STAT.diff_History_Accuracy$p.value`$; T(`r M_STAT.diff_History_Accuracy$parameter`) = `r M_STAT.diff_History_Accuracy$statistic`, p = $`r M_STAT.diff_History_Accuracy$p.value`$; Figure 4A, lower panel), confirming that perceptual history was a source of error[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] as opposed to a feature of the experimental paradigm. 
Overall, perception was stimulus-congruent in `r mean(M_Behav$Accuracy, na.rm = TRUE)`% ± `r sd(M_Behav$Accuracy, na.rm = TRUE)/sqrt(length(M_Behav$Accuracy))`% of trials (Figure 4A).

```{r mouse_stats_autocorrelations, cache = TRUE}

if (!load_summary_data) {
  
##
## Summary stats autocorrelation
##

## compute_Summary_acf

source("./Functions/f_compute_mouse_group_acf.R", local = knitr::knit_global())

# compute difference to random autocorrelation and remove outliers  
MwData$diff_acf_Stimulus = exclude_3SD(MwData$acf_Stimulus- MwData$random_acf_Stimulus) 
MwData$diff_acf_History = exclude_3SD(MwData$acf_History - MwData$random_acf_History) 
MwData$diff_acf_Difficulty = exclude_3SD(MwData$acf_Difficulty - MwData$random_acf_Difficulty) 
MwData$diff_acf_RT = exclude_3SD(MwData$acf_RT - MwData$random_acf_RT)
MwData$diff_acf_HardEasy = exclude_3SD(MwData$acf_HardEasy - MwData$random_acf_HardEasy)
MwData$diff_acf_External = exclude_3SD(MwData$acf_External - MwData$random_acf_External)

acf_to_test = c('diff_acf_Stimulus', 'diff_acf_History')
max_trial = 99

if (compute_mouse_group_acf){
M_Summary_acf <- f_compute_mouse_group_acf(MwData, acf_to_test, max_trial)

} else {
  M_Summary_acf <- read.csv("./Results/M_Summary_acf_session_corrected_normalized_3SD_new_preproc.csv")
}
M_Summary_acf$Trial <- M_Summary_acf$Trial - 1


ID_M_Summary_acf <- ddply(
    MwData, 
    .(subject_id),
    summarise,
    lag_significant_Stimulus = min(which(diff(which(c(0, exceed_acf_Stimulus,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_History = min(which(diff(which(c(0, exceed_acf_History,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_Difficulty = min(which(diff(which(c(0, exceed_acf_Difficulty,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_External = min(which(diff(which(c(0, exceed_acf_External,0) < n_permutations/2)) != 1)) - 1
  )
ID_M_Summary_acf$lag_significant_Stimulus[ID_M_Summary_acf$lag_significant_Stimulus == Inf] = NA
ID_M_Summary_acf$lag_significant_History[ID_M_Summary_acf$lag_significant_History == Inf] = NA
ID_M_Summary_acf$lag_significant_Difficulty[ID_M_Summary_acf$lag_significant_Difficulty == Inf] = NA
ID_M_Summary_acf$lag_significant_External[ID_M_Summary_acf$lag_significant_External == Inf] = NA


##
## Exceed Plot
##

gathercol <- colnames(ID_M_Summary_acf[,c(2,3)])

ID_M_Summary_acf_long  <-
gather(ID_M_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

M_Stimulus_linear <- summary(lm(Mean ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" & M_Summary_acf$Trial < 21,]))
M_Stimulus_exponential <- summary(lm(log(Mean) ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" & M_Summary_acf$Trial < 21,]))

M_History_linear <- summary(lm(Mean ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History" & M_Summary_acf$Trial < 21,]))
M_History_exponential <- summary(lm(log(Mean) ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History" & M_Summary_acf$Trial < 21,]))

MwData$min_diff_acf_Stimulus = MwData$diff_acf_Stimulus - min(MwData$diff_acf_Stimulus, na.rm = TRUE)
MwData$min_diff_acf_History = MwData$diff_acf_History - min(MwData$diff_acf_History, na.rm = TRUE)

M_lmer_acf_Stimulus <- lmer(log(min_diff_acf_Stimulus) ~ Trial + (1|subject_id), data = MwData[MwData$Trial > 1 & MwData$Trial < 21,])
M_STAT.lmer_acf_Stimulus <- summary(M_lmer_acf_Stimulus)
M_lmer_acf_History <- lmer(log(min_diff_acf_History) ~ Trial + (1|subject_id), data = MwData[MwData$Trial > 1 & MwData$Trial < 21,])
M_STAT.lmer_acf_History <- summary(M_lmer_acf_History)
  
  if (save_summary_data) {
    save(M_Summary_acf,
         ID_M_Summary_acf,
         ID_M_Summary_acf_long,
         M_Stimulus_linear,
         M_Stimulus_exponential,
         M_History_linear,
         M_History_exponential,
         M_STAT.lmer_acf_Stimulus,
         M_STAT.lmer_acf_History,
         file = "./Summary_Data/Autocorrelation_mice.Rdata")
  }
  
} else {
  load("./Summary_Data/Autocorrelation_mice.Rdata")
}
```

At the group level, we found significant autocorrelations in both stimulus-congruence (`r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials) and history-congruence (`r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials), which remained significant when taking into account the respective autocorrelation of task difficulty and external stimulation (Supplemental Figure 2C-D). 
In contrast to humans, mice showed a negative autocorrelation coefficient of stimulus-congruence at trial 2. This was due to a feature of the experimental design: Errors at a contrast above 50% were followed by a high-contrast stimulus at the same location. Thus, stimulus-incongruent choices on easy trials were more likely to be followed by stimulus-congruent perceptual choices that were facilitated by high-contrast visual stimuli[@Aguillon-Rodriguez2020]. 

The autocorrelation of history-congruence closely overlapped with the human data and decayed exponentially after a peak at the first trial (rate $\gamma$ = $`r M_STAT.lmer_acf_History$coefficients[2,1]`$ ± $`r M_STAT.lmer_acf_History$coefficients[2,2]`$, T($`r M_STAT.lmer_acf_History$coefficients[2,3]`$) = $`r M_STAT.lmer_acf_History$coefficients[2,4]`$, p = $`r M_STAT.lmer_acf_History$coefficients[2,5]`$; Figure 4B). 
On the level of individual mice, autocorrelation coefficients were elevated above randomly permuted data within a lag of `r mean(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_M_Summary_acf)` trials for stimulus-congruence and `r mean(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_M_Summary_acf)` trials for history-congruence (Figure 4C). 

```{r logistic_regression_autocorrelation_mouse, cache = TRUE}
##
## Reproduce autocorrelation with logistic regression (correcting for difficulty and stimulus repetition)
##
if (!load_summary_data) {
  ##
  ## Mice: Trialwise logistic regression
  ##
  n_back = 25
  if (compute_mouse_Tw_LogReg == TRUE) {
    source("./Functions/f_compute_mouse_trial_wise_logreg.R",
           local = knitr::knit_global())
    
    M_Tw_LogReg <- f_compute_mouse_trial_wise_logreg(MwData, n_back)
    
  } else {
    M_Tw_LogReg <- read.csv("./Results/M_Tw_LogReg_new_preproc_25back.csv")
  }
  
  gathercol = colnames(M_Tw_LogReg[, c(4, 6)])
  M_Tw_LogReg_long <-
    gather(M_Tw_LogReg[, c(1:4, 6)],
           "Variable",
           "beta",
           gathercol,
           factor_key = TRUE)
  
  M_Tw_LogReg_long$Variable <-
    gsub("History_weight",
         "Weight: History-congruence",
         M_Tw_LogReg_long$Variable)
  M_Tw_LogReg_long$Variable <-
    gsub("Stimulus_weight",
         "Weight: Stimulus-congruence",
         M_Tw_LogReg_long$Variable)
  
  
  M_Group_Tw_LogReg_long <-  ddply(
    M_Tw_LogReg_long,
    .(lag, Variable),
    summarise,
    Mean = mean(exclude_3SD(beta), na.rm = TRUE),
    Error = sd(exclude_3SD(beta), na.rm = TRUE) / sqrt(length(exclude_3SD(beta)))
  )
  
  ## trial-wise stats
  M_Group_Tw_LogReg_long$p_value = NA
  for (lag_idx in c(1:n_back)) {
    M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: History-congruence" &
                             M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
      summary(lmer(exclude_3SD(History_weight) ~ 1 + (1 |
                                                        session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
    
    M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence" &
                             M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
      summary(lmer(exclude_3SD(Stimulus_weight) ~ 1 + (1 |
                                                         session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
  }
  
  
  
  if (save_summary_data) {
    save(M_Group_Tw_LogReg_long,
         M_Tw_LogReg_long,
         M_Tw_LogReg,
         M_Group_Tw_LogReg_long,
         n_back,
         file = "./Summary_Data/log_reg_autocorrelation_mouse.Rdata")
  }
  
} else {
  load("./Summary_Data/log_reg_autocorrelation_mouse.Rdata")
}

```

To further corroborate a significant autocorrelation of stimulus- and history-congruence in mice, we used logistic regression models that predicted the stimulus-/history-congruence of perception at the index trial $t = 0$ from the stimulus/history-congruence at the preceding trials within a lag of `r n_back` trials. We found that regression weights were significantly greater than zero for more than 25 trials for stimulus-congruence. For history-congruence, regression weights significantly greater than zero for `r min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: History-congruence",]$p_value > 0.05))` trials prior to the index trial (Supplemental Figure S3). In analogy to humans, mice showed anti-phase 1/f fluctuations in the sensitivity to internal and external information (Figure 4D-F).
<!-- , while controlling for the effect of both difficulty and stimulus-repetition at trial $t = 0$. Over and above the effects of difficulty, stimulus repetition and general biases toward one of the two outcomes, -->

```{r mouse_spectral_analysis, cache = TRUE}
##
## Power
##
if (!load_summary_data) {
  
  if (compute_mouse_power_spectra) {
source("./Functions/f_compute_power_spectra.R", local = knitr::knit_global())

sliders = c("Accuracy_slider", "History_slider")  
M_Power_Spectra <- f_compute_power_spectra(MwData, sliders)  
} else {
  M_Power_Spectra <- read.csv(paste(root, "M_power_spectra_smoothed_new_preproc.csv", sep = ""))

}

M_Power_Spectra$r_freq = round(M_Power_Spectra$freq, digits = 4)

step_size = 0.01
bins <- seq(from = min(M_Power_Spectra$freq), to = max(M_Power_Spectra$freq), by = step_size)
names <- round(bins, digits = 2)
names = names[1:length(names) - 1]

M_Power_Spectra$bin_freq <- cut(M_Power_Spectra$freq, breaks = bins, labels = names)
M_Power_Spectra$bin_freq <- as.numeric(M_Power_Spectra$bin_freq) * step_size
M_Power_Spectra$bin_freq[is.na(M_Power_Spectra$bin_freq)] = max(M_Power_Spectra$freq)

M_Power_Spectra$Coherence <- M_Power_Spectra$Coherence * 100



gathercol = colnames(M_Power_Spectra[, c(2,3)])
M_Power_Spectra_long  <-
gather(M_Power_Spectra[, c(1,2,3, 6,7,8)],
"Variable",
"Power",
gathercol,
factor_key = TRUE)

M_Power_Frequency <-  ddply(
      M_Power_Spectra_long,
      .(r_freq, Variable),
      summarise,
      
      mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
      ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE)/sqrt(length(exclude_3SD(Power)))
 )

M_Coherence_Phase_Frequency <- ddply(
      M_Power_Spectra,
      .(bin_freq),
      summarise,
    
      mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
      ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE)/sqrt(length(exclude_3SD(Coherence))),
      
      mode_phase = getmode(exclude_3SD((abs(Phase)))),
      mean_phase = getmode(exclude_3SD((abs(Phase)))),
      ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE)/sqrt(length(exclude_3SD(Phase)))
 )
 

##
## mode of coherence and phase
##
M_Summary_Power_Spectra <- 
   ddply(
 M_Power_Spectra[M_Power_Spectra$freq > 0.01 & M_Power_Spectra$freq < 0.1,],
       .(subject_id),
       summarise,
       mean_coherence = mean(exclude_3SD((
 Coherence))),
       mode_phase = getmode(exclude_3SD((
 abs(Phase))))
   )

##
## STATS Power vs Frequency (1/f noise)
##

M_lmer_power_freq_Stimulus <- lmer(log(Power_Stimulus) ~ log(freq) + (1|subject_id), data = M_Power_Spectra[M_Power_Spectra$freq > 0.01 & M_Power_Spectra$freq < 0.1 & is.finite(log(M_Power_Spectra$Power_Stimulus)),])
STAT.lmer_power_freq_Stimulus <- summary(M_lmer_power_freq_Stimulus)

M_lmer_power_freq_History <- lmer(log(Power_History) ~ log(freq) + (1|subject_id), data = M_Power_Spectra[M_Power_Spectra$freq > 0.01 & M_Power_Spectra$freq < 0.1 & is.finite(log(M_Power_Spectra$Power_History)),])
STAT.lmer_power_freq_History <- summary(M_lmer_power_freq_History)
  
  if (save_summary_data) {
    save(M_Power_Frequency,
         M_Coherence_Phase_Frequency,
         M_Summary_Power_Spectra,
         STAT.lmer_power_freq_Stimulus,
         STAT.lmer_power_freq_History,
         M_Power_Spectra,
         file = "./Summary_Data/spectral_analysis_mouse.Rdata")
  }
  
} else {
  load("./Summary_Data/spectral_analysis_mouse.Rdata")
}

```

```{r mouse_slider_history_accuracy}
if (!load_summary_data) {
  
if (compute_slider_History_Accuracy_lmer) {
M_slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|subject_id), data = MwData)
} else {
   M_slider_History_vs_Accuracy <- readRDS(file = "./Results/M_slider_History_vs_Accuracy_new_preproc.rds")
}
M_STAT.slider_History_vs_Accuracy <- summary(M_slider_History_vs_Accuracy)
  
  if (save_summary_data) {
    save(M_STAT.slider_History_vs_Accuracy,  file = "./Summary_Data/mouse_slider_history_accuracy.Rdata")
  }
} else {
  load("./Summary_Data/mouse_slider_history_accuracy.Rdata")
}

```

```{r mouse_mode_RT_new, cache = TRUE}
##
## Mode vs. RT
##

if (!load_summary_data) {
  
  M_Mode_gather <-  ddply(
      MwData,
      .(subject_id),
      summarise,
      
      directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
      fine_directed_mode = (Accuracy_slider -History_slider)*100,

      scaled_directed_mode = scale(directed_mode),
      strength_mode = abs(scaled_directed_mode)
      )

MwData$directed_mode = M_Mode_gather$directed_mode
MwData$fine_directed_mode = M_Mode_gather$fine_directed_mode
MwData$scaled_directed_mode = M_Mode_gather$scaled_directed_mode
MwData$strength_mode = M_Mode_gather$strength_mode

M_ID_mode <-  ddply(
      MwData,
      .(subject_id),
      summarise,
      strength_mode = mean(strength_mode, na.rm = TRUE),
      directed_mode = mean(directed_mode, na.rm = TRUE)
)


scaled_data = ddply(
      MwData,
      .(subject_id),
      summarise,
      scaled_clear_RT = scale(clear_RT)
      )

MwData$scaled_clear_RT <- scaled_data$scaled_clear_RT



M_Post_Perceptual_Modes = ddply(
      MwData[!is.na(MwData$directed_mode),],
      .(directed_mode),
      summarise,
      average_RT = mean(clear_RT, na.rm = TRUE),
      se_RT = sd(clear_RT, na.rm = TRUE)/sqrt(length(clear_RT)),
      average_RT2 = mean(clear_RT2, na.rm = TRUE),
      se_RT2 = sd(clear_RT2, na.rm = TRUE)/sqrt(length(clear_RT2)),
      n = length(clear_RT),
      n_percent = (sum(!is.na(clear_RT))/nrow(MwData))*100
      )

M_RT_vs_mode <- lmer(clear_RT ~ poly(directed_mode, 2) + (1|subject_id), data = MwData[!is.na(MwData$directed_mode),])
M_STAT.RT_vs_mode <- summary(M_RT_vs_mode)

M_RT_vs_mode2 <- lmer(clear_RT2 ~ poly(directed_mode, 2) + (1|subject_id), data = MwData[!is.na(MwData$directed_mode) & MwData$directed_mode <= 70 & MwData$directed_mode >= -20,])
M_STAT.RT_vs_mode2 <- summary(M_RT_vs_mode2)


M_RT_Behav <- 
  ddply(
    MwData[!is.na(MwData$History) & !is.na(MwData$Accuracy),], 
    .(subject_id),
    summarise,
    diff_RT_History = mean(clear_RT[History == 1], na.rm = TRUE) -  mean(clear_RT[History == 0], na.rm = TRUE),
    diff_RT_Stimulus = mean(clear_RT[Accuracy == 1], na.rm = TRUE) -  mean(clear_RT[Accuracy == 0], na.rm = TRUE))


gathercol = colnames(M_RT_Behav[, c(2,3)])
M_RT_Behav_long  <-
gather(M_RT_Behav,
"Variable",
"diff_RT",
gathercol,
factor_key = TRUE)

M_RT_Behav_long$Variable <-
gsub("diff_RT_History", "History", M_RT_Behav_long$Variable)
M_RT_Behav_long$Variable <-
gsub("diff_RT_Stimulus", "Stimulus", M_RT_Behav_long$Variable)

M_diff_RT <- lmer(diff_RT ~ Variable + (1|subject_id), data = M_RT_Behav_long)
M_STAT.diff_RT <- summary(M_diff_RT)

M_Summary_RT_Behav <- 
  ddply(
    M_RT_Behav_long, 
    .(Variable),
    summarise,
    Mean = mean(diff_RT, na.rm = TRUE),
    Error = sd(diff_RT, na.rm = TRUE)/sqrt(length(diff_RT)))


M_STAT.RT_diff_History <- t.test(M_RT_Behav$diff_RT_History)
M_STAT.RT_diff_Stimulus <- t.test(M_RT_Behav$diff_RT_Stimulus)

if (mouse_compute_RT_Accuracy_History) {
M_lmer_RT_Accuracy_History <- lmer(RT ~ Accuracy + History + (1|subject_id), data = MwData)
saveRDS(M_lmer_RT_Accuracy_History, "./Results/M_lmer_RT_Accuracy_History.rds")

} else {
M_lmer_RT_Accuracy_History <- readRDS(file = "./Results/M_lmer_RT_Accuracy_History.rds")

}
M_STAT.lmer_RT_Accuracy_History <- summary(M_lmer_RT_Accuracy_History)

  
  if (save_summary_data) {
    save(M_Mode_gather, 
         M_ID_mode, 
         M_Post_Perceptual_Modes,
         M_STAT.RT_vs_mode,
         M_STAT.RT_vs_mode2,
         M_RT_Behav,
         M_RT_Behav_long,
         M_STAT.diff_RT,
         M_Summary_RT_Behav,
         M_STAT.RT_diff_History,
         M_STAT.RT_diff_Stimulus,
         M_STAT.lmer_RT_Accuracy_History,
         file = "./Summary_Data/mouse_mode_RT.Rdata")
  }
} else {
  load("./Summary_Data/mouse_mode_RT.Rdata")
}
```

Next, we asked how external and internal modes relate to the trial duration (TD, a coarse measure of RT in mice that spans the interval from stimulus onset to feedback[@Aguillon-Rodriguez2020]). Stimulus-congruent (as opposed to stimulus-incongruent) choices were associated with shorter TDs ($\delta$ =  $`r mean(M_RT_Behav$diff_RT_Stimulus, na.rm = TRUE)`$ ± $`r sd(M_RT_Behav$diff_RT_Stimulus, na.rm = TRUE)/sqrt(length(M_RT_Behav$diff_RT_Stimulus))`$, T(`r M_STAT.RT_diff_Stimulus$parameter`) = `r M_STAT.RT_diff_Stimulus$statistic`, p = $`r M_STAT.RT_diff_Stimulus$p.value`$), while history-congruent choices were characterized by longer TDs ($\delta$ =  $`r mean(M_RT_Behav$diff_RT_History, na.rm = TRUE)`$ ± $`r sd(M_RT_Behav$diff_RT_History, na.rm = TRUE)/sqrt(length(M_RT_Behav$diff_RT_History))`$, T(`r M_STAT.RT_diff_History$parameter`) = `r M_STAT.RT_diff_History$statistic`, p = $`r M_STAT.RT_diff_History$p.value`$; Figure 4G). 

Across the full spectrum of the available data, TDs showed a linear relationship with the mode of sensory processing, with shorter TDs during external mode ($\beta_1$ = $`r M_STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r M_STAT.RT_vs_mode$coefficients[2,2]`$, T($`r M_STAT.RT_vs_mode$coefficients[2,3]`$) = $`r M_STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r M_STAT.RT_vs_mode$coefficients[2,5]`$, Figure 4H). 
However, an explorative post-hoc analysis limited to TDs that differed from the median TD by no more than 1.5 x MAD (median absolute distance[@Leys2013]) indicated that, when mice engaged with the task more swiftly, TDs did indeed show a quadratic relationship with the mode of sensory processing ($\beta_2$ = $`r M_STAT.RT_vs_mode2$coefficients[3,1]`$ ± $`r M_STAT.RT_vs_mode2$coefficients[3,2]`$, T($`r M_STAT.RT_vs_mode2$coefficients[3,3]`$) = $`r M_STAT.RT_vs_mode2$coefficients[3,4]`$, p = $`r M_STAT.RT_vs_mode2$coefficients[3,5]`$, Figure 4I).

```{r mouse_behavior_during_training, cache = TRUE}

if (!load_summary_data) {
  
  if (compute_pretraining_data_mouse){
fully_trained <- unique(MwData$subject_id)
Global_MwData <- read.csv(paste(root, "/Mouse/MwData_new_preproc_filter_slider.csv", sep = ""))

Global_MwData$clear_RT = Global_MwData$RT
Global_MwData$clear_RT[Global_MwData$clear_RT > median(Global_MwData$clear_RT, na.rm = TRUE) + 3*median(Global_MwData$clear_RT, na.rm =TRUE)] = NA
Global_MwData$clear_RT <- Global_MwData$clear_RT*1000

Global_MwData <- Global_MwData[which(Global_MwData$subject_id %in% fully_trained),]

M_Behav_training <-
  ddply(
    Global_MwData,
    .(subject_id, session_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100,
    History = mean(History, na.rm = TRUE)*100,
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    Imbalance = mean(abs(0.5-prob_left), na.rm = TRUE)*100 + 50,
    RT = mean(clear_RT, na.rm = TRUE)
  )
} else {
  M_Behav_training <- read.csv("./Results/Mouse_Behav_training.csv")
}

gathercol = colnames(M_Behav_training[, c(4,5,7)])
M_Behav_training_long   <-
gather(M_Behav_training[M_Behav_training$Imbalance < 60,c(1,2,4,5,7)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

M_Behav_training_long$Variable <-
gsub("Accuracy", "Stimulus", M_Behav_training_long$Variable)

Summary_M_Behav_training_long <-   ddply(
    M_Behav_training_long ,
    .(session_id, Variable),
    summarise,
    Mean = mean(Frequency, na.rm = TRUE),
    Error = sd(Frequency, na.rm = TRUE)/sqrt(length(Frequency))
  )

M_lmer_History_training <- lmer(History ~ session_id + (1|subject_id), data = M_Behav_training)
M_STAT.lmer_History_training <- summary(M_lmer_History_training)

M_lmer_Stimulus_training <- lmer(Accuracy ~ session_id + (1|subject_id), data = M_Behav_training)
M_STAT.lmer_Stimulus_training <- summary(M_lmer_Stimulus_training)

M_lmer_RT_training <- lmer(RT ~ session_id + (1|subject_id), data = M_Behav_training)
M_STAT.lmer_RT_training <- summary(M_lmer_RT_training)

M_training_RT <- lmer(clear_RT ~ Trial + (1|subject_id/session_id), data = MwData)
M_STAT.training_RT <- summary(M_training_RT)

if (compute_training_history){
M_training_History <- glmer(History ~ Trial + (1|subject_id/session_id),
        data = MwData,
        family = binomial)
} else {
 M_training_History <- readRDS(paste(root, "training_history.rds", sep = "")) 
}
M_STAT.training_History <- summary(M_training_History)
  
  if (save_summary_data) {
    save(M_Behav_training, 
         M_Behav_training_long,
         Summary_M_Behav_training_long,
         M_STAT.lmer_History_training,
         M_STAT.lmer_Stimulus_training,
         M_STAT.lmer_RT_training,
         M_STAT.training_RT,
         M_STAT.training_History,
         file = "./Summary_Data/mouse_behavior_training.Rdata")
  }
  
} else {
  load("./Summary_Data/mouse_behavior_training.Rdata")
}
```

As in humans, it is an important caveat to consider whether the observed serial dependencies in murine perception reflect a phenomenon of perceptual inference, or, alternatively, an unspecific strategy that occurs at the level of reporting behavior. We reasoned that, if mice indeed tended to repeat previous choices as a general response pattern, history effects should decrease during training of the perceptual task. We therefore analyzed how stimulus- and history-congruent perceptual choices evolved across sessions in mice that, by the end of training, achieved proficiency (i.e., stimulus-congruence $\geq$ 80%) in the *basic* task of the IBL dataset[@Aguillon-Rodriguez2020].

As expected, we found that stimulus-congruent perceptual choices became more frequent ($\beta$ = $`r M_STAT.lmer_Stimulus_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_Stimulus_training$coefficients[2,2]`$, T($`r M_STAT.lmer_Stimulus_training$coefficients[2,3]`$) = $`r M_STAT.lmer_Stimulus_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$; Supplemental Figure S6) and TDs were progressively shortened ($\beta$ = $`r M_STAT.lmer_RT_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_RT_training$coefficients[2,2]`$, T($`r M_STAT.lmer_RT_training$coefficients[2,3]`$) = $`r M_STAT.lmer_RT_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$) across sessions.
Crucially, the frequency of history-congruent perceptual choices also increased during training ($\beta$ = $`r M_STAT.lmer_History_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_History_training$coefficients[2,2]`$, T($`r M_STAT.lmer_History_training$coefficients[2,3]`$) = $`r M_STAT.lmer_History_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_History_training$coefficients[2,5]`$; Supplemental Figure S6). 

As in humans, longer within-session task exposure was associated with an increase in history-congruence ($\beta$ = $`r M_STAT.training_History$coefficients[2,1]`$ ± $`r M_STAT.training_History$coefficients[2,2]`$, z = $`r M_STAT.training_History$coefficients[2,3]`$, p = $`r M_STAT.training_History$coefficients[2,4]`$) and a decrease in TDs ($\beta$ = $`r M_STAT.training_RT$coefficients[2,1]`$ ± $`r M_STAT.training_RT$coefficients[2,2]`$, T($`r M_STAT.training_RT$coefficients[2,3]`$) = $`r M_STAT.training_RT$coefficients[2,4]`$, p = $`r M_STAT.training_RT$coefficients[2,5]`$). In sum, these findings strongly argue against the proposition that mice show biases toward perceptual history due to an unspecific response strategy.

As in humans, fluctuations in the strength of history-congruent biases were, (i), larger in amplitude than the corresponding fluctuations in general response biases ($\beta_0$ = $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,1]`$ ± $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,2]`$, T($`r M_STAT.Variance_History_vs_Preferred$coefficients[1,3]`$) = $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,4]`$, p = $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,5]`$)
and, (ii), had a significant effect on stimulus-congruence ($\beta_1$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) 
beyond the effect of ongoing changes in general response biases ($\beta_2$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$). 
This confirmed that, in both humans and mice, perceptual performance is modulated by systematic fluctuations between externally- and internally-oriented modes of sensory processing.

```{r psychometric_function_mouse, cache = TRUE}
if (!load_summary_data) {
  
   N_PM <- ddply(
      MwData,
      .(Difficulty),
      summarise,
      n_trials = length(Difficulty))
  
   difficulty_levels = N_PM$Difficulty[N_PM$n_trials > 100]
   
  P_M_Behav = M_Behav
  
  type_list = c(
    "full_b",
    "full_0",
    "full_1",
    "external_b",
    "external_0",
    "external_1",
    "internal_b",
    "internal_0",
    "internal_1"
  )
  
  P_M_Behav = cbind(P_M_Behav, matrix(NA, ncol = length(type_list) * 5, nrow = nrow(P_M_Behav)))
  colnames(P_M_Behav) <-
    c(
      "subject_id",
      "Stimulus_History",
      "History",
      "Accuracy",
      "Bias",
      "null_History",
      "full_b_lower_lapse",
      "full_b_higher_lapse",
      "full_b_av_lapse",
      "full_b_bias",
      "full_b_threshold",
      "full_0_lower_lapse",
      "full_0_higher_lapse",
      "full_0_av_lapse",
      "full_0_bias",
      "full_0_threshold",
      "full_1_lower_lapse",
      "full_1_higher_lapse",
      "full_1_av_lapse",
      "full_1_bias",
      "full_1_threshold",
      "external_b_lower_lapse",
      "external_b_higher_lapse",
      "external_b_av_lapse",
      "external_b_bias",
      "external_b_threshold",
      "external_0_lower_lapse",
      "external_0_higher_lapse",
      "external_0_av_lapse",
      "external_0_bias",
      "external_0_threshold",
      "external_1_lower_lapse",
      "external_1_higher_lapse",
      "external_1_av_lapse",
      "external_1_bias",
      "external_1_threshold",
      "internal_b_lower_lapse",
      "internal_b_higher_lapse",
      "internal_b_av_lapse",
      "internal_b_bias",
      "internal_b_threshold",
      "internal_0_lower_lapse",
      "internal_0_higher_lapse",
      "internal_0_av_lapse",
      "internal_0_bias",
      "internal_0_threshold",
      "internal_1_lower_lapse",
      "internal_1_higher_lapse",
      "internal_1_av_lapse",
      "internal_1_bias",
      "internal_1_threshold"
    )
  
  for (subj_idx in unique(P_M_Behav$subject_id)) {
  print(subj_idx)
    
  for (type_idx in type_list){
   
   if (type_idx == "full_b") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels
   } else if (type_idx == "full_0") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 0
   } else if (type_idx == "full_1") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 1
   } else if (type_idx == "internal_b") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$scaled_directed_mode < 0
   } else if (type_idx == "internal_0") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 0 &
       MwData$scaled_directed_mode < 0
   } else if (type_idx == "internal_1") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 1 &
       MwData$scaled_directed_mode < 0
   } else if (type_idx == "external_b") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$scaled_directed_mode > 0
   } else if (type_idx == "external_0") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 0 &
       MwData$scaled_directed_mode > 0
   } else if (type_idx == "external_1") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 1 &
       MwData$scaled_directed_mode > 0 
   }
   
    Input_Data <- data.frame (
      Stimulus = MwData[trial_selector, ]$Stimulus,
      Difficulty = MwData[trial_selector, ]$Difficulty,
      Response = MwData[trial_selector, ]$Response,
      Response_minus_1 = MwData[trial_selector, ]$Response_minus_1
    )
   
    source("./Functions/fit_psychometric_function_mouse.R")
    par = c(0.0, 0.0, 0, 0.05)
    lower = c(0.0, 0.0,-0.5, 0.01)
    upper = c(0.25, 0.25, 0.5, 1.5)
    
    add_PM =  optimx(
      par,
      fit_psychometric_function_mouse,
      Input_Data = Input_Data,
      lower = lower,
      upper = upper,
      method ="L-BFGS-B"
    )
    
    P_M_Behav[P_M_Behav$subject_id == subj_idx,grepl(type_idx, colnames(P_M_Behav))] <- 
      c(add_PM$p1, add_PM$p2, mean(c(add_PM$p1, add_PM$p2)), add_PM$p3, add_PM$p4)
 }   
}
  
  ##
  ## STATS
  ##
  M_STAT.PM_History_lapse_bias_threshold <-
    summary(lm(
      History ~ 1 + full_b_av_lapse + abs(full_b_bias) + full_b_threshold,
      data = P_M_Behav
    ))
  
  M_STAT.PM_Accuracy_History_lapse <-
    summary(lm(
      Accuracy ~ 1 + History + full_b_av_lapse,
      data = P_M_Behav
    ))
  
    P_M_Behav$threshold_diff = P_M_Behav$external_b_threshold - P_M_Behav$internal_b_threshold
    P_M_Behav$bias_diff = abs(P_M_Behav$external_b_bias) - abs(P_M_Behav$internal_b_bias)
    P_M_Behav$lapse_diff = P_M_Behav$external_b_av_lapse - P_M_Behav$internal_b_av_lapse
    
  M_STAT.PM_diff_bias <-
    summary(lm(
      bias_diff ~ 1 + lapse_diff + threshold_diff,
      data = P_M_Behav
    ))
  
  M_STAT.PM_diff_lapse <-
    summary(lm(
      lapse_diff ~ 1 + bias_diff + threshold_diff,
      data = P_M_Behav
    ))
  
  M_STAT.PM_diff_threshold <-
    summary(lm(
      threshold_diff ~ 1 + bias_diff + lapse_diff,
      data = P_M_Behav
    ))
  
  M_STAT.PM_zero_bias_b = t.test(P_M_Behav$full_b_bias)
M_STAT.PM_zero_bias_1 = t.test(P_M_Behav$full_1_bias)
M_STAT.PM_zero_bias_0 = t.test(P_M_Behav$full_0_bias)


M_STAT.PM_diff_lapse_1_higher = t.test(P_M_Behav$external_1_higher_lapse - P_M_Behav$internal_1_higher_lapse)
M_STAT.PM_diff_lapse_0_higher = t.test(P_M_Behav$external_0_higher_lapse - P_M_Behav$internal_0_higher_lapse)
M_STAT.PM_diff_lapse_1_lower = t.test(P_M_Behav$external_1_lower_lapse - P_M_Behav$internal_1_lower_lapse)
M_STAT.PM_diff_lapse_0_lower = t.test(P_M_Behav$external_0_lower_lapse - P_M_Behav$internal_0_lower_lapse)

M_STAT.PM_diff_lapse_1_higher_vs_lower = t.test((P_M_Behav$external_1_higher_lapse - P_M_Behav$internal_1_higher_lapse) - (P_M_Behav$external_1_lower_lapse - P_M_Behav$internal_1_lower_lapse))

M_STAT.PM_diff_lapse_0_higher_vs_lower = t.test((P_M_Behav$external_0_higher_lapse - P_M_Behav$internal_0_higher_lapse) - (P_M_Behav$external_0_lower_lapse - P_M_Behav$internal_0_lower_lapse))

M_STAT.PM_diff_threshold_0_1 <- t.test(P_M_Behav$full_0_threshold - P_M_Behav$full_1_threshold)
  
##
## Generate parameter summary
##
gathercol = colnames(P_M_Behav[, seq(7, ncol(P_M_Behav) - 3)])
P_M_Behav_long  <-
  gather(P_M_Behav,
         "Variable",
         "Estimate",
         gathercol,
         factor_key = TRUE)

P_M_Behav_long$Conditioned = NA
P_M_Behav_long$Type = NA

P_M_Behav_long[grepl("full", P_M_Behav_long$Variable), ]$Type = "full"
P_M_Behav_long[grepl("internal", P_M_Behav_long$Variable), ]$Type = "internal"
P_M_Behav_long[grepl("external", P_M_Behav_long$Variable), ]$Type = "external"
P_M_Behav_long$Type <-
  factor(P_M_Behav_long$Type, levels = c("full", "internal", "external"))


P_M_Behav_long[grepl("_b", P_M_Behav_long$Variable), ]$Conditioned = "all"
P_M_Behav_long[grepl("_0", P_M_Behav_long$Variable), ]$Conditioned = "y(t-1) = 0"
P_M_Behav_long[grepl("_1", P_M_Behav_long$Variable), ]$Conditioned = "y(t-1) = 1"

P_M_Behav_long$Variable_full = P_M_Behav_long$Variable
P_M_Behav_long$Variable <- gsub(c("full_"), "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("internal_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("external_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("b_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("1_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("0_", "", P_M_Behav_long$Variable)

##
## simulate
##
Sim_PM_M = data.frame()
stim_range = seq(-1,+1, 0.01)
for (subj_idx in unique(P_M_Behav$subject_id)) {
  print(subj_idx)
  for (type_idx in type_list) {
  par = P_M_Behav_long[P_M_Behav_long$subject_id == subj_idx & grepl(type_idx, P_M_Behav_long$Variable_full), ]$Estimate[c(1, 2, 4, 5)]
  lower_lapse = par[1]
  higher_lapse = par[2]
  bias = par[3]
  contrast_threshold = par[4]
  
  Sim_PM_M = rbind(
    Sim_PM_M,
    add_sim_PM = data.frame(
      subject_id = subj_idx,
      Stimulus = stim_range,
      y_prob = lower_lapse + (1 - lower_lapse - higher_lapse) *  (erf((stim_range - bias) / contrast_threshold) + 1) / 2,
      Variable = type_idx
    )
)
}
}

Sim_PM_M$Conditioned = NA
Sim_PM_M$Type = NA

Sim_PM_M[grepl("full", Sim_PM_M$Variable), ]$Type = "full"
Sim_PM_M[grepl("internal", Sim_PM_M$Variable), ]$Type = "internal"
Sim_PM_M[grepl("external", Sim_PM_M$Variable), ]$Type = "external"
Sim_PM_M$Type <-
  factor(Sim_PM_M$Type, levels = c("full", "internal", "external"))


Sim_PM_M[grepl("_b", Sim_PM_M$Variable), ]$Conditioned = "all"
Sim_PM_M[grepl("_0", Sim_PM_M$Variable), ]$Conditioned = "y(t-1) = 0"
Sim_PM_M[grepl("_1", Sim_PM_M$Variable), ]$Conditioned = "y(t-1) = 1"

Sim_PM_M$Conditioned <- as.factor(Sim_PM_M$Conditioned)
Sim_PM_M$subject_id <- as.factor(Sim_PM_M$subject_id)

Sim_PM_M_group <-
    ddply(
    Sim_PM_M,
    .(Stimulus, Conditioned, Type),
    summarise,
    mean_y_prob = mean(y_prob, na.rm = TRUE),
    error_y_prob = sd(y_prob, na.rm = TRUE)/sqrt(length(y_prob))
  )

Sim_PM_M_group$Type <-
  factor(Sim_PM_M_group$Type, levels = c("full", "internal", "external"))
  
  if (save_summary_data) {
    save(
      P_M_Behav,
      M_STAT.PM_History_lapse_bias_threshold,
      M_STAT.PM_Accuracy_History_lapse,
      M_STAT.PM_diff_threshold,
      M_STAT.PM_diff_lapse,
      M_STAT.PM_diff_bias,
      M_STAT.PM_zero_bias_b,
      M_STAT.PM_zero_bias_1,
      M_STAT.PM_zero_bias_0,
      M_STAT.PM_diff_lapse_1_higher,
      M_STAT.PM_diff_lapse_0_higher,
      M_STAT.PM_diff_lapse_1_lower,
      M_STAT.PM_diff_lapse_0_lower,
      M_STAT.PM_diff_lapse_1_higher_vs_lower,
      M_STAT.PM_diff_lapse_0_higher_vs_lower,
      M_STAT.PM_diff_threshold_0_1,
      P_M_Behav_long,
      Sim_PM_M,
      Sim_PM_M_group,
      file = "./Summary_Data/psychometric_data_mouse.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/psychometric_data_mouse.Rdata")
}
```

Finally, we fitted full and history-conditioned psychometric curves to the data from the IBL database. When estimated based on the full dataset (i.e., irrespective of the preceding perceptual choice $y_{t-1}$), biases $\mu$ were distributed around zero (`r mean(P_M_Behav$full_b_bias, na.rm = TRUE)` ± `r sd(P_M_Behav$full_b_bias, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_bias))`; T(`r M_STAT.PM_zero_bias_b$parameter`) = `r M_STAT.PM_zero_bias_b$statistic`, p = $`r M_STAT.PM_zero_bias_b$p.value`$; Figure 5A and B, upper panel). 
When conditioned on the preceding perceptual choice, biases were negative for $y_{t-1} = 0$ (`r -mean(P_M_Behav$full_0_bias, na.rm = TRUE)` ± `r sd(P_M_Behav$full_0_bias, na.rm = TRUE)/sqrt(length(P_M_Behav$full_0_bias))`; T(`r M_STAT.PM_zero_bias_0$parameter`) = `r -M_STAT.PM_zero_bias_0$statistic`, p = $`r M_STAT.PM_zero_bias_0$p.value`$; Figure 5A and B, middle panel) 
and positive for $y_{t-1} = 1$ (`r -mean(P_M_Behav$full_1_bias, na.rm = TRUE)` ± `r sd(P_M_Behav$full_1_bias, na.rm = TRUE)/sqrt(length(P_M_Behav$full_1_bias))`; T(`r M_STAT.PM_zero_bias_1$parameter`) = `r -M_STAT.PM_zero_bias_1$statistic`, p = $`r M_STAT.PM_zero_bias_1$p.value`$; Figure 5A and B, lower panel). 
As in humans, mice showed larger biases during internal mode (`r mean(abs(P_M_Behav$internal_b_bias), na.rm = TRUE)` ± `r sd(abs(P_M_Behav$internal_b_bias), na.rm = TRUE)/sqrt(length(abs(P_M_Behav$internal_b_bias)))`) as compared to external mode (`r mean(abs(P_M_Behav$external_b_bias), na.rm = TRUE)` ± `r sd(abs(P_M_Behav$external_b_bias), na.rm = TRUE)/sqrt(length(abs(P_M_Behav$external_b_bias)))`; $\beta_0$ = $`r M_STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_bias$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_bias$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_bias$coefficients[1,4]`$; controlling for differences in lapses and thresholds). 

Lower and upper lapses amounted to $\gamma$ = `r mean(P_M_Behav$full_b_lower_lapse, na.rm = TRUE)` ± `r sd(P_M_Behav$full_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_higher_lapse))` and $\delta$ = `r mean(P_M_Behav$full_b_higher_lapse, na.rm = TRUE)` ± `r sd(P_M_Behav$full_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_higher_lapse))` (see Figure 5A, C and D). 
Lapse rates were higher in internal mode ($\gamma$ = `r mean(P_M_Behav$internal_b_lower_lapse, na.rm = TRUE)` ± `r sd(P_M_Behav$internal_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$internal_b_higher_lapse))`, $\delta$ = `r mean(P_M_Behav$internal_b_higher_lapse, na.rm = TRUE)` ± `r sd(P_M_Behav$internal_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$internal_b_higher_lapse))`) as compared to external mode ($\gamma$ = `r mean(P_M_Behav$external_b_lower_lapse, na.rm = TRUE)` ± `r sd(P_M_Behav$external_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$external_b_higher_lapse))`, $\delta$ = `r mean(P_M_Behav$external_b_higher_lapse, na.rm = TRUE)` ± `r sd(P_M_Behav$external_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$external_b_higher_lapse))`; $\beta_0$ = $`r M_STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_lapse$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_lapse$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_lapse$coefficients[1,4]`$; controlling for differences in biases and thresholds). 

For $y_{t-1} = 0$, the difference between internal and external mode was more pronounced for higher lapses $\delta$ (T(`r M_STAT.PM_diff_lapse_1_higher_vs_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_higher_vs_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher_vs_lower$p.value`$). 
Conversely, for $y_{t-1} = 1$, the difference between internal and external mode was more pronounced for lower lapses $\gamma$ (T(`r M_STAT.PM_diff_lapse_0_higher_vs_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_higher_vs_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher_vs_lower$p.value`$). 
In contrast to the human data, higher lapses $\delta$ and lower lapses $\gamma$ were significantly elevated during internal mode irrespective of the preceding perceptual choice 
(higher lapses $\delta$ for $y_{t-1} = 1$: T(`r M_STAT.PM_diff_lapse_1_higher$parameter`) = `r M_STAT.PM_diff_lapse_1_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher$p.value`$;
higher lapses $\delta$ for $y_{t-1} = 0$: T(`r M_STAT.PM_diff_lapse_0_higher$parameter`) = `r M_STAT.PM_diff_lapse_0_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher$p.value`$;
lower lapses $\gamma$ for $y_{t-1} = 1$: T(`r M_STAT.PM_diff_lapse_1_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_lower$p.value`$;
lower lapses $\gamma$ for $y_{t-1} = 0$: T(`r M_STAT.PM_diff_lapse_0_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_lower$p.value`$).

In mice, thresholds $t$ amounted to `r mean(P_M_Behav$full_b_threshold, na.rm = TRUE)` ± `r sd(P_M_Behav$full_b_threshold, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_threshold))` (see Figure 5A and E) and were higher in internal mode (`r mean(P_M_Behav$internal_b_threshold, na.rm = TRUE)` ± `r sd(P_M_Behav$internal_b_threshold, na.rm = TRUE)/sqrt(length(P_M_Behav$internal_b_threshold))`) as compared to external mode (`r mean(P_M_Behav$external_b_threshold, na.rm = TRUE)` ± `r sd(P_M_Behav$external_b_threshold, na.rm = TRUE)/sqrt(length(P_M_Behav$external_b_threshold))`; $\beta_0$ = $`r M_STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_threshold$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_threshold$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_threshold$coefficients[1,4]`$; controlling for differences in biases and lapses).
Thresholds $t$ were not modulated by perceptual history (T(`r M_STAT.PM_diff_threshold_0_1$parameter`) = `r M_STAT.PM_diff_threshold_0_1$statistic`, p = $`r M_STAT.PM_diff_threshold_0_1$p.value`$).

In sum, the above analyses of the psychometric function in mice corroborated our findings in humans. Higher thresholds indicated a reduced sensitivity to external information during internal mode. Additionally, internally-biased processing was characterized by a history-dependent modulation of biases and lapses. 

## Fluctuations in mode result from coordinated changes in the impact of external and internal information on perception

The empirical data presented above indicate that, for both humans and mice, perception fluctuates between internal an external modes, i.e., multi-trial epochs that are characterized by enhanced sensitivity toward either internal or external information. Since natural environments typically show high temporal redundancy[@Dong1995], previous experiences are often good predictors of new stimuli[@Chopin2012; @Burr2014; @Braun2018; @Bergen2019]. Serial dependencies may therefore induce autocorrelations in perception by serving as an internal prediction (or *memory* processes[@Gilden1995; @Gilden2001]) about the environment that actively integrates noisy sensory information over time[@Maloney2005]. 

Previous work has shown that such internal predictions are built by dynamically updating the estimated probability of being in a particular perceptual state from the sequence of preceding experiences[@Burr2014; @Glaze2015; @St.John-Saaltink2016]. The integration of sequential inputs may lead to accumulating effects of perceptual history that progressively override incoming sensory information, enabling internal mode processing[@Weilnhammer2021a]. However, since such a process would lead to internal biases that may eventually become impossible to overcome[@Wexler2015], we assumed that changes in mode may additionally be driven by ongoing wave-like fluctuations[@Gilden1995; @Gilden2001] in the perceptual impact of external and internal information that occur *irrespective* of the sequence of previous experiences and temporarily de-couple the decision variable from implicit internal representations of the environment[@Weilnhammer2021a]. 

Following Bayes' theorem, we reasoned that binary perceptual decisions depend on the posterior log ratio $L$ of the two alternative states of the environment that participants learn about via noisy sensory information[@Glaze2015]. We computed the posterior by combining the sensory evidence available at time-point $t$ (i.e., the log likelihood ratio $LLR$) with the prior probability $\psi$**, weighted by the respective precision terms $\omega_{LLR}$ and $\omega_{\psi}$**:

\begin{equation}
L_t = LLR_t * \omega_{LLR} + \psi_t(L_{t-1}, H) * \omega_{\psi}
\end{equation}

We derived the prior probability $\psi$ at timepoint $t$ from the posterior probability of perceptual outcomes at timepoint $L_{t-1}$. Since a switch between the two states can occur at any time, the effect of perceptual history varies according to both the sequence of preceding experiences and the estimated stability of the external environment (i.e., the *hazard rate* $H$[@Glaze2015]): 

\begin{equation}
\psi_t(L_{t-1}, H)  = L_{t-1} + log(\frac{1-H}{H} + exp(-L_{t-1})) - log(\frac{1-H}{H} + exp(L_{t-1}))
\end{equation}

The $LLR$ was computed from inputs $s_t$ by applying a sigmoid function defined by parameter $\alpha$ that controls the sensitivity of perception to the available sensory information (see Methods for detailed equations on humans and mice):

\begin{equation}
u_t = \frac{1}{1 + exp(-\alpha * s_t)}
\end{equation}

\begin{equation}
LLR_t = log(\frac{u_t}{1-u_t})
\end{equation}

To allow for *bimodal inference*, i..e, alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that the relative influences of likelihood and prior show coherent anti-phase fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$ that are determined by amplitude $a$, frequency $f$ and phase $p$. **This implements a hyperprior in which the sensory and prior precisions fluctuate at a particular time scale**:

\begin{equation}
\omega_{LLR} = a_{LLR} * sin(f * t + p) + 1
\end{equation}

\begin{equation}
\omega_{\psi} = a_{\psi} * sin(f * t + p + \pi) + 1
\end{equation}

<!-- \begin{equation} -->
<!-- \omega_{LLR} = \frac{\frac{1}{1+exp(-a * sin(f * t + p) + 1}}{max(\frac{1}{1+exp(-a * sin(f * t + p) + 1})} -->
<!-- end{equation} -->

<!-- \begin{equation} -->
<!-- \omega_{\psi} = \frac{\frac{1}{1+exp(-a * sin(f * t + p + \pi) + 1}}{max(\frac{1}{1+exp(-a * sin(f * t + p + \pi) + 1})} -->
<!-- end{equation} -->

Finally, a sigmoid transform of the posterior $L_t$ yields the probability of observing the perceptual decision $y_t$ at a temperature determined by $\zeta^{-1}$:

\begin{equation}
P(y_t = 1) = 1 - P(y_t = 0) = \frac{1}{1 + exp(-\zeta * L_t)}
\end{equation}

```{r optimization, cache = TRUE}

if (!load_summary_data) {
  
  # type_list = c(
  #             "fit_glaze_osc_zeta_v4_LLR_amp",
  #             "fit_glaze_osc_zeta_v4_Prior_amp",
  #             "fit_glaze_osc_zeta_v4_one_amp",
  #             "fit_glaze_osc_zeta_v4",
  #             "fit_glaze_osc_zeta_v4_no_amp",
  #             "fit_glaze_osc_zeta_v4_no_integration"
  #             ) 
  
    type_list = c(
              "fit_glaze_osc_zeta_v1_LLR_amp",
              "fit_glaze_osc_zeta_v1_Prior_amp",
              "fit_glaze_osc_zeta_v1",
              "fit_glaze_osc_zeta_v1_no_amp",
              "fit_glaze_osc_zeta_v1_no_integration"
              ) 

if (run_optim_human) {
  
  # available models
  source("./Functions/fit_glaze_osc_zeta_v1.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_one_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_integration.R",
  local = knitr::knit_global())

  type_idx = 1
  
  Optim = data.frame()
  for (id in unique(PwData$subject_id)) {
  Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_RT", "clear_Confidence")]
  
  if (type_list[type_idx] == "export_data") {
  ## Starting parameters
  write.csv(Input_Data, paste("./Modeling_Data/human_subject_", as.character(id), sep = ""),
  row.names = FALSE)
    add_O = data.frame()
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {
  ## Starting parameters
  par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_one_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_LLR_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp") {
  
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_Prior_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp") {
  ## Starting parameters
  par = c(0.01, 0.01, 5)
  lower = c(0.01, 0.01, 1)
  upper = c(0.99, 20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration") {
  ## Starting parameters
  par = c(0.01, 5)
  lower = c(0.01, 1)
  upper = c(20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_integration,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  add_O$subject_id = id
  add_O$trial_n = nrow(Input_Data)
  add_O$free_param = length(par)
  add_O$type = type_list[type_idx]
  add_O$AIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 0
  )

  add_O$cAIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 1
  )

  Optim = rbind(Optim, add_O)

  print(paste((mean(Optim$p1, na.rm = TRUE)),
  (id / max(PwData$subject_id)) * 100,
  sep = ","
  ))
  }
  
  write.csv(Optim,
  paste("./Results/", type_list[type_idx], ".csv", sep = ""),
  row.names = FALSE)
} else {
 
## Load Modelspace 
Compare_Optim <- data.frame()
for (load_idx in c(1:length(type_list))){
load_Optim <- read.csv(paste("./Results/", type_list[load_idx], ".csv", sep = ""))
Compare_Optim = rbind(Compare_Optim, load_Optim[,c(1,seq(ncol(load_Optim)-13,ncol(load_Optim)))])
}
Compare_Optim = Compare_Optim[Compare_Optim$convcode == 0,]
Compare_Optim$AIC = exclude_3SD(Compare_Optim$AIC)
colMeans(Wide_Compare_Optim, na.rm = TRUE)
Wide_Compare_Optim = reshape(Compare_Optim[,c("AIC", "type","subject_id")], idvar = "subject_id", timevar = "type", direction = "wide")
Wide_Compare_Optim$min_AIC <- apply(Wide_Compare_Optim[,2:ncol(Wide_Compare_Optim)], 1, FUN = max, na.rm = TRUE)
Wide_Compare_Optim[,2:(ncol(Wide_Compare_Optim)-1)] = Wide_Compare_Optim[,2:(ncol(Wide_Compare_Optim)-1)] - Wide_Compare_Optim$min_AIC

colMeans(Wide_Compare_Optim[,2:(ncol(Wide_Compare_Optim)-1)], na.rm = TRUE)

Lbits = data.frame()
for (load_idx in c(1:length(type_list))) {
add_Lbits = data.frame(Lbits = ((-Compare_Optim[Compare_Optim$type == type_list[load_idx], ]$value)  - (-Compare_Optim[Compare_Optim$type == type_list[length(type_list)], ]$value)) /
((Compare_Optim[Compare_Optim$type == type_list[load_idx], ]$trial_n) *
log(2)),
model = type_list[load_idx],
subject_id = Compare_Optim[Compare_Optim$type == type_list[load_idx], ]$subject_id)
Lbits = rbind(Lbits, add_Lbits)
}

model_AIC = data.frame()
for (load_idx in c(1:length(type_list))) {
add_model_AIC = data.frame(model_AIC = ((Compare_Optim[Compare_Optim$type == type_list[load_idx], ]$AIC)  - (Compare_Optim[Compare_Optim$type == type_list[length(type_list)], ]$AIC)),
model = type_list[load_idx],
subject_id = Compare_Optim[Compare_Optim$type == type_list[load_idx], ]$subject_id)
model_AIC = rbind(model_AIC, add_model_AIC)
}

Summary_Compare_Optim_Lbits <-
  ddply(
    Lbits,
    .(model),
    summarise,
    mean_Lbits = mean(Lbits, na.rm = TRUE),
    error_Lbits = sd(Lbits, na.rm = TRUE) / sqrt(length(Lbits)))
Summary_Compare_Optim_Lbits

Optim <- read.csv("./Results/Optim_human_free_phase_zeta.csv")
}

Optim <- Optim[Optim$subject_id %in% Behav$subject_id,]

Optim_Behav <-
  ddply(
    Optim,
    .(subject_id),
    summarise,
    Hazard = mean(p1, na.rm = TRUE),
    Precision =  mean(p2, na.rm = TRUE),
    amp_prior = mean(p3, na.rm = TRUE),
    amp_LLR = mean(p4, na.rm = TRUE),
    frequency = mean(p5, na.rm = TRUE),
    phase = mean(p6, na.rm = TRUE),
    zeta = mean(p7, na.rm = TRUE)
  )
Optim_Behav$study_id = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$study_id


##
## STATS Summary Modelling (+ sanity check)
##

Optim_Behav$Stimulus_History = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$Stimulus_History
Optim_Behav$Accuracy = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$Accuracy
Optim_Behav$History = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$History
Optim_Behav$Under_Estimation = Optim_Behav$Stimulus_History - (100-Optim_Behav$Hazard*100)


Optim_Underestimation <- lmer(Under_Estimation ~ 1 + (1|study_id), data = Optim_Behav)
STAT.Optim_Underestimation <- summary(Optim_Underestimation)

Optim_Accuracy_Precision <- lmer(Accuracy ~ Precision + (1|study_id), data = Optim_Behav)
STAT.Optim_Accuracy_Precision <- summary(Optim_Accuracy_Precision)

Optim_History_Hazard <- lmer(History ~ Hazard + (1|study_id), data = Optim_Behav)
STAT.Optim_History_Hazard <- summary(Optim_History_Hazard)

##
## get trial-wise posterior log ratio
##

if (generate_output_human) {

  Output = data.frame()
 for (id in unique(Optim$subject_id)) {
    print(id)
 
    
    Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_Confidence", "clear_RT","Accuracy","History", "study_id")]
    

   source("./Functions/output_glaze_osc_zeta_v4_one_amp.R", local = knitr::knit_global())
   add_Output <- output_glaze_osc_zeta_v4_one_amp(as.double(Optim[Optim$subject_id == id,c(1:5)]), Input_Data)
  
   Input_Data$clear_Confidence_minus_1 = c(NA, Input_Data$clear_Confidence[c(1:length(Input_Data$clear_Confidence)-1)])
   Input_Data$clear_RT_minus_1 = c(NA, Input_Data$clear_RT[c(1:length(Input_Data$clear_RT)-1)])
   add_Output$mu_minus_1 = c(NA,   add_Output$mu[c(1:length(add_Output$mu)-1)])
   
   Output = rbind(Output, cbind(abs(add_Output$mu),  Input_Data$clear_RT, Input_Data$clear_Confidence,
                                abs(add_Output$mu_minus_1),  Input_Data$clear_RT_minus_1, Input_Data$clear_Confidence_minus_1,
                                Input_Data$Accuracy, Input_Data$History,
                                id, Input_Data$study_id))
    
  
 }
} else {
   Output <- read.csv(paste(root, "Output_ext_zeta.csv", sep = ""))
}

colnames(Output) <- c("Certainty", "RT", "Confidence", "Certainty_minus_1", "RT_minus_1", "Confidence_minus_1","Accuracy","History","subject_id", "study_id")

Optim_RT_Certainty <- lmer(RT ~ Certainty + (1|study_id/subject_id), data = Output)
STAT.Optim_RT_Certainty <- summary(Optim_RT_Certainty)

Optim_Confidence_Certainty <- lmer(Confidence ~ Certainty + (1|study_id/subject_id), data = Output)
STAT.Optim_Confidence_Certainty <- summary(Optim_Confidence_Certainty)

if (run_logreg_Confidence_mu_minus_1){
Optim_History_mu_minus_1 <- glmer(History ~ Certainty_minus_1 + (1|study_id/subject_id),
        data = Output,
        family = binomial)
Optim_History_Confidence_minus_1 <- glmer(History ~ Confidence_minus_1 + (1|study_id/subject_id),
        data = Output,
        family = binomial)
} else {
Optim_History_mu_minus_1 <- readRDS(file = paste(root, "Optim_History_mu_minus_1_zeta.rds", sep = ""))
Optim_History_Confidence_minus_1 <- readRDS(file = paste(root, "Optim_History_Confidence_minus_1_zeta.rds", sep = ""))
}
STAT.Optim_History_mu_minus_1 <- summary(Optim_History_mu_minus_1)
STAT.Optim_History_Confidence_minus_1 <- summary(Optim_History_Confidence_minus_1)
  
  if (save_summary_data) {
    save(Optim, Optim_Behav,
         STAT.Optim_Underestimation,
         STAT.Optim_Accuracy_Precision,
         STAT.Optim_History_Hazard,
         STAT.Optim_RT_Certainty,
         STAT.Optim_Confidence_Certainty,
         STAT.Optim_History_mu_minus_1,
         STAT.Optim_History_Confidence_minus_1,
         file = "./Summary_Data/optimization_human.Rdata")
  }
  
} else {
  load("./Summary_Data/optimization_human.Rdata")
}

```

```{r optimization_mouse, cache = TRUE}


if (!load_summary_data) {
  
  type_list = c(
              "fit_glaze_osc_zeta_v1_mouse", 
              "fit_glaze_osc_zeta_v1_one_amp_mouse",
              "fit_glaze_osc_zeta_v1_LLR_amp_mouse",
              "fit_glaze_osc_zeta_v1_Prior_amp_mouse",
              "fit_glaze_osc_zeta_v1_no_amp_mouse",
              "fit_glaze_osc_zeta_v1_no_integration_mouse"
              ) 


  if (run_optim_mouse){

  # available models
  source("./Functions/fit_glaze_osc_zeta_v1_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_one_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_integration_mouse.R",
  local = knitr::knit_global())

  type_idx = 6
  
  Optim = data.frame()
  for (id in unique(MwData$subject_id)) {
    
  for (session_id in unique(MwData[MwData$subject_id == id,]$session_id)) {
    
    Input_Data = MwData[MwData$subject_id == id & MwData$session_id == session_id, c("Stimulus", "Response", "Difficulty")]
    
    Input_Data$Stimulus <- (Input_Data$Stimulus*2 - 1) * Input_Data$Difficulty
    Input_Data$Stimulus[Input_Data$Difficulty == 0] = 0
    
  if (type_list[type_idx] == "export_data") {
  ## Starting parameters
  write.csv(Input_Data, paste("./Modeling_Data/mouse_subject_", as.character(id), "_session_", as.character(session_id), ".csv", sep = ""),
  row.names = FALSE)
    add_O = data.frame()
  }   
    
    
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
    
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_one_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_LLR_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_Prior_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 5)
  lower = c(0.01, 0.01, 1)
  upper = c(0.99, 20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration_mouse") {
  ## Starting parameters
  par = c(0.01, 5)
  lower = c(0.01, 1)
  upper = c(20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_integration_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }

  add_O$subject_id = id
  add_O$session_id = session_id
  add_O$trial_n = nrow(Input_Data)
  add_O$free_param = length(par)
  add_O$type = type_list[type_idx]
  add_O$AIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 0
  )
  add_O$cAIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 1
  )

  Optim = rbind(Optim, add_O)
    
  print(paste((mean(Optim$p1, na.rm = TRUE)),
  (id / max(MwData$subject_id)) * 100,
  sep = ","
  ))
  }  
    
  }
  write.csv(Optim,
  paste("./Results/", type_list[type_idx], ".csv", sep = ""),
  row.names = FALSE)
 } else {

## Load Modelspace 
M_Compare_Optim <- data.frame()
for (load_idx in c(1:length(type_list))){
load_Optim <- read.csv(paste("./Results/", type_list[load_idx], ".csv", sep = ""))

M_Compare_Optim = rbind(M_Compare_Optim, load_Optim[,c(1,seq(ncol(load_Optim)-14,ncol(load_Optim)))])
}

M_Compare_Optim_Subj <-  ddply(
    M_Compare_Optim[M_Compare_Optim$convcode == 0,], 
    .(subject_id, type),
    summarise,
    sum_value = sum(value, na.rm = TRUE),
    sum_trial_n = sum(trial_n, na.rm = TRUE),
    AIC = sum(AIC,na.rm = TRUE),
    Log = sum(-value, na.rm = TRUE)
    )

M_Compare_Optim_Sum <- ddply(
    M_Compare_Optim_Subj, 
    .(type),
    summarise,
    mean_AIC = mean(AIC,na.rm = TRUE),
    sd_AIC = sd(AIC, na.rm = TRUE)/sqrt(length(AIC)),
    Log = mean(Log, na.rm = TRUE)
    )

M_Wide_Compare_Optim = reshape(M_Compare_Optim_Subj[,c("sum_value", "type","subject_id")], idvar = "subject_id", timevar = "type", direction = "wide")

M_Lbits = data.frame()
for (load_idx in c(1:length(type_list))) {
add_M_Lbits = data.frame(M_Lbits = ((-M_Compare_Optim_Subj[M_Compare_Optim_Subj$type == type_list[load_idx], ]$sum_value)),
model = type_list[load_idx],
subject_id = M_Compare_Optim_Subj[M_Compare_Optim_Subj$type == type_list[load_idx], ]$subject_id)
M_Lbits = rbind(M_Lbits, add_M_Lbits)
}

M_model_AIC = data.frame()
for (load_idx in c(1:length(type_list))) {
add_model_AIC = data.frame(model_AIC = ((M_Compare_Optim_Subj[M_Compare_Optim_Subj$type == type_list[load_idx], ]$AIC)),
model = type_list[load_idx],
subject_id = M_Compare_Optim_Subj[M_Compare_Optim_Subj$type == type_list[load_idx], ]$subject_id)
M_model_AIC = rbind(M_model_AIC, add_model_AIC)
}

Summary_M_Compare_Optim_Subj_AIC <-
  ddply(
    M_model_AIC,
    .(model),
    summarise,
    mean_AIC = mean(model_AIC, na.rm = TRUE),
    error_AIC = sd(model_AIC, na.rm = TRUE) / sqrt(length(model_AIC)))
Summary_M_Compare_Optim_Subj_AIC

M_Optim <- read.csv("./Results/Optim_mouse_free_phase_zeta.csv") 
}

M_Summary_Compare_Optim<-
  ddply(
    M_Lbits,
    .(model),
    summarise,
    mean_Lbits = mean(M_Lbits, na.rm = TRUE),
    error_Lbits = sd(M_Lbits, na.rm = TRUE) / sqrt(length(M_Lbits)))

M_Optim <- M_Optim[M_Optim$subject_id %in% M_Behav$subject_id,]

M_Behav_sess <- 
  ddply(
    MwData, 
    .(subject_id, session_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100, 
    History = mean(History, na.rm = TRUE)*100, 
    Accuracy = mean(Accuracy, na.rm = TRUE)*100
  )

Optim_M_Behav_sess <- 
  ddply(
    M_Optim, 
    .(subject_id, session_id),
    summarise,
    Hazard = mean(p1, na.rm = TRUE), 
    Precision =  mean(p2), 
    amp = mean(p3),
    frequency = mean(p4),
    phase = mean(p5),
    conv = convcode
  )

Optim_M_Behav <-
  ddply(
    M_Optim,
    .(subject_id),
    summarise,
    Hazard = mean(p1, na.rm = TRUE),
    Precision =  mean(p2),
    amp_prior = mean(p3),
    amp_LLR = mean(p4),
    frequency = mean(p5),
    phase = mean(p6),
    zeta = mean(p7)
  )
Optim_M_Behav$Stimulus_History = M_Behav$Stimulus_History
Optim_M_Behav$Accuracy = M_Behav$Accuracy
Optim_M_Behav$History = M_Behav$History
Optim_M_Behav$Under_Estimation = M_Behav$Stimulus_History - (100-Optim_M_Behav$Hazard*100)
##
## STATS Summary Modelling (+ sanity check)
##
Optim_M_Behav_sess$Stimulus_History = M_Behav_sess$Stimulus_History
Optim_M_Behav_sess$Accuracy = M_Behav_sess$Accuracy
Optim_M_Behav_sess$History = M_Behav_sess$History
Optim_M_Behav_sess$Under_Estimation = M_Behav_sess$Stimulus_History - (100-Optim_M_Behav_sess$Hazard*100)

M_Optim_Underestimation <- lmer(Under_Estimation ~ 1 + (1|subject_id), data = Optim_M_Behav_sess)
M_STAT.Optim_Underestimation <- summary(M_Optim_Underestimation)

M_Optim_Accuracy_Precision <- lmer(Accuracy ~ Precision + (1|subject_id), data = Optim_M_Behav_sess)
M_STAT.Optim_Accuracy_Precision <- summary(M_Optim_Accuracy_Precision)

M_Optim_History_Hazard <- lmer(History ~ Hazard + (1|subject_id), data = Optim_M_Behav_sess)
M_STAT.Optim_History_Hazard <- summary(M_Optim_History_Hazard)

##
## get trial-wise posterior log ratio
##
if (generate_output_mouse) {
M_Output = data.frame()
 for (id in unique(MwData$subject_id)) {
    print(id)
  for (session_id in unique(MwData[MwData$subject_id == id,]$session_id)) { 
    
    Input_Data = MwData[MwData$subject_id == id & MwData$session_id == session_id, c("Stimulus", "Response", "Difficulty", "clear_RT","Accuracy","History", "Stimulus_History")]
    
    Input_Data$Stimulus <- (Input_Data$Stimulus*2 - 1) * Input_Data$Difficulty
    Input_Data$Stimulus[Input_Data$Difficulty == 0] = 0
    
   source("./Functions/output_glaze_osc_mouse_zeta_v4_one_amp.R", local = knitr::knit_global())
   add_Output <- output_glaze_osc_mouse_zeta_v4_one_amp(as.double(M_Optim[M_Optim$subject_id == id & M_Optim$session_id == session_id,c(1:7)]), Input_Data)

   Input_Data$clear_RT_minus_1 = c(NA, Input_Data$clear_RT[c(1:length(Input_Data$clear_RT)-1)])
   add_Output$mu_minus_1 = c(NA,   add_Output$mu[c(1:length(add_Output$mu)-1)])

   M_Output = rbind(
     M_Output,
     cbind(
     abs(add_Output$mu),
     Input_Data$clear_RT,
     abs(add_Output$LLR),
     abs(add_Output$muhat),
     # abs(add_Output$muhat * add_Output$oscillation_ei) * (-sign(M_Optim[M_Optim$subject_id == id &
     # M_Optim$session_id == session_id, c(1:7)]$p1 - 0.5)) - log(mean(Input_Data$Stimulus_History, na.rm = TRUE) /
     # (
     # 1 - mean(Input_Data$Stimulus_History, na.rm = TRUE)
     # )),
     abs(add_Output$mu_minus_1),
     Input_Data$clear_RT_minus_1,
     Input_Data$Accuracy,
     Input_Data$History,
     id,
     session_id
     )
     )
    
  }
 }
} else {
   M_Output <- read.csv(paste(root, "M_Output_zeta.csv", sep = ""))

}

colnames(M_Output) <- c("Certainty", "RT", "LLR", "muhat", "Certainty_minus_1","RT_minus_1", "Accuracy","History", "subject_id", "session_id")

M_Optim_RT_Certainty <- lmer(RT ~ Certainty + (1|subject_id/session_id), data = M_Output)
M_STAT.Optim_RT_Certainty <- summary(M_Optim_RT_Certainty)

if (run_mouse_logreg_mu_minus_1){
M_Optim_History_mu_minus_1 <- glmer(History ~ Certainty_minus_1 + (1 + Certainty_minus_1|subject_id/session_id),
        data = M_Output,
        family = binomial)
} else {
M_Optim_History_mu_minus_1 <- readRDS(file = paste(root, "M_Optim_History_mu_minus_1_zeta.rds", sep = ""))
}
M_STAT.Optim_History_mu_minus_1 <- summary(M_Optim_History_mu_minus_1)
  
  if (save_summary_data) {
    save(M_Optim,  
         M_Behav_sess,
         Optim_M_Behav_sess,
         Optim_M_Behav,
         M_STAT.Optim_Underestimation,
         M_STAT.Optim_Accuracy_Precision,
         M_STAT.Optim_History_Hazard,
         M_STAT.Optim_RT_Certainty,
         M_STAT.Optim_History_mu_minus_1,
         file = "./Summary_Data/optimization_mouse.Rdata")
  }
  
} else {
  load("./Summary_Data/optimization_mouse.Rdata")
}
```

Fitting the bimodal inference model outlined above to behavioral data (see Methods for details) characterizes each subject by a sensitivity parameter $\alpha$ that captures how strongly perception is driven by the available sensory information, and a hazard rate parameter $H$ that controls how heavily perception is biased by perceptual history. As a sanity check for model fit, we tested whether the frequency of stimulus- and history-congruent trials in the Confidence database[@Rahnev2020] and IBL database[@Aguillon-Rodriguez2020] correlate with the estimated parameters $\alpha$ and $H$, respectively. As expected, the estimated sensitivity toward stimulus information $\alpha$ was positively correlated with the frequency of stimulus-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r STAT.Optim_Accuracy_Precision$coefficients[2,5]`$; 
mice: $\beta$ = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r M_STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,5]`$). 
Likewise, $H$ was negatively correlated with the frequency of history-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r STAT.Optim_History_Hazard$coefficients[2,5]`$; 
mice:  $\beta$ = $`r M_STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r M_STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r M_STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r M_STAT.Optim_History_Hazard$coefficients[2,5]`$). 
Our behavioral analyses have shown that humans and mice showed significant effects of perceptual history that impaired performance in randomized psychophysical experiments[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] (Figure 2A and 3A). We therefore expected that humans and mice underestimated the true hazard rate $\hat{H}$ of the experimental environments (Confidence database[@Rahnev2020]: $\hat{H}_{Humans}$ = `r mean((100 - Behav$Stimulus_History)/100, na.rm = TRUE)` ± `r sd((100 - Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(Behav)`); IBL database[@Aguillon-Rodriguez2020]: $\hat{H}_{Mice}$ = `r mean((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)` ± `r sd((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(M_Behav)`). Indeed, when fitting the bimodal inference model outlined above to the trial-wise perceptual choices (see Methods), we found that the estimated (i.e., subjective) hazard rate $H$ was lower than $\hat{H}$ for both humans (H = `r mean(Optim_Behav$Hazard, na.rm = TRUE)` ± `r sd(Optim_Behav$Hazard, na.rm = TRUE)/nrow(Optim_Behav)`, $\beta$ = $`r STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r STAT.Optim_Underestimation$coefficients[1,5]`$)
and mice (H = `r mean(Optim_M_Behav$Hazard, na.rm = TRUE)` ± `r sd(Optim_M_Behav$Hazard, na.rm = TRUE)/nrow(Optim_M_Behav)`, $\beta$ = $`r M_STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r M_STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r M_STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r M_STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r M_STAT.Optim_Underestimation$coefficients[1,5]`$).

```{r simulation_reproduce_zeta, cache = TRUE}
##
## Reproduce behavioral data from inverted model in humans
##

if (!load_summary_data) {
  
  
if (run_simulation) {
  
type_list = c(
              "fit_glaze_osc_zeta_v1"
              #"fit_glaze_osc_zeta_v1_LLR_amp"
              ) 
source("./Functions/simulation_glaze_osc_human_zeta.R", local = knitr::knit_global())
source("./Functions/simulation_glaze_osc_human_zeta_LLR_amp.R", local = knitr::knit_global())

Sim <- data.frame()
for (type_idx in c(1:length(type_list))){
  
reproduce_Optim  <- read.csv(paste("./Results/", type_list[type_idx], ".csv", sep = ""))
n_participants =  nrow(reproduce_Optim)
n = runif(n_participants, 1000, 2000)

# outcomes
outcomes = c(0,1)
for (subj_idx in c(1:n_participants)){
  
  if ((subj_idx %% 10) == 0){print(subj_idx)}
  
  random_steps = rnorm(n = n[subj_idx], mean = 0, sd = 0)
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {

  Sim_add <-
  simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  reproduce_Optim$p1[subj_idx],
  reproduce_Optim$p2[subj_idx],
  reproduce_Optim$p3[subj_idx],
  reproduce_Optim$p4[subj_idx],
  reproduce_Optim$p5[subj_idx],
  reproduce_Optim$p6[subj_idx],
  1,
  random_steps,
  sliding_window,
  n_permutations
  )

  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
  Sim_add <-
  simulation_glaze_osc_human_zeta_LLR_amp(
  outcomes,
  n[subj_idx],
  reproduce_Optim$p1[subj_idx],
  reproduce_Optim$p2[subj_idx],
  reproduce_Optim$p3[subj_idx],
  reproduce_Optim$p4[subj_idx],
  reproduce_Optim$p5[subj_idx],
  1,
  random_steps,
  sliding_window,
  n_permutations
  )
    }
  
  Sim_add$diff_acf_Stimulus <- exclude_3SD(Sim_add$acf_Stimulus - Sim_add$random_acf_Stimulus) 
  Sim_add$diff_acf_History <- exclude_3SD(Sim_add$acf_History - Sim_add$random_acf_History) 
  Sim_add$subject_id = subj_idx
  Sim_add$Trial = c(1:n[subj_idx])
  Sim_add$H = reproduce_Optim$p1[subj_idx]
  Sim_add$prec = reproduce_Optim$p2[subj_idx]
  Sim_add$amp = reproduce_Optim$p3[subj_idx]
  Sim_add$amp_LLR = reproduce_Optim$p4[subj_idx]
  Sim_add$frequency = reproduce_Optim$p5[subj_idx]
  Sim_add$phase =  reproduce_Optim$p6[subj_idx]
  Sim_add$zeta =  reproduce_Optim$p7[subj_idx]
  Sim_add$random_steps = random_steps
  Sim_add$type = type_list[type_idx]
  
  Sim = rbind(Sim, Sim_add)
}
}

} else {
  #Sim <- read.csv(paste(root, "Sim_posterior_param_variable_n_human_zeta.csv", sep = ""))
  Sim <- read.csv(paste(root, "Sim_LLR_tryout.csv", sep = ""))
  type_list = unique(Sim$type)
}

## compute summary of simulated behavior
Sim_Behav <- 
  ddply(
    Sim, 
    .(subject_id,type),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100, 
    Min_History = min(History_slider, na.rm = TRUE)*100,
    Max_History = max(History_slider, na.rm = TRUE)*100,
    Accuracy = sum(Accuracy, na.rm = TRUE)/length(Accuracy)*100,
    Min_Accuracy = min(Accuracy_slider, na.rm = TRUE)*100,
    Max_Accuracy = max(Accuracy_slider, na.rm = TRUE)*100
  )

Sim_Behav_diff <- 
   ddply(
    Sim[!is.na(Sim$Accuracy),], 
    .(subject_id, Accuracy),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100
  )

Sim_Behav$null_History = Sim_Behav$History - 50
Sim_STAT.Global_History_Accuracy = t.test(Sim_Behav$null_History)

compare_participants = which(Sim_Behav_diff[Sim_Behav_diff$Accuracy == 1,]$subject_id %in% Sim_Behav_diff[Sim_Behav_diff$Accuracy == 0,]$subject_id)

Sim_STAT.diff_History_Accuracy = t.test(Sim_Behav_diff[Sim_Behav_diff$Accuracy == 0 & Sim_Behav_diff$subject_id %in% compare_participants,]$History, Sim_Behav_diff[Sim_Behav_diff$Accuracy == 1 & Sim_Behav_diff$subject_id %in% compare_participants,]$History, paired = TRUE)

## compute_Summary_acf

source("./Functions/f_compute_sim_group_acf.R", local = knitr::knit_global())

acf_to_test = c('diff_acf_Stimulus', 'diff_acf_History')
max_trial = 99
Sim_Summary_acf <- f_compute_sim_group_acf(Sim, acf_to_test, max_trial, type_list)

Sim_Summary_acf$Trial <- Sim_Summary_acf$Trial - 1


ID_Sim_Summary_acf <- ddply(
    Sim, 
    .(subject_id,type),
    summarise,
    lag_significant_Stimulus = min(which(diff(which(c(0, exceed_acf_Stimulus,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_History = min(which(diff(which(c(0, exceed_acf_History,0) < n_permutations/2)) != 1)) - 1
  )
ID_Sim_Summary_acf$lag_significant_Stimulus[ID_Sim_Summary_acf$lag_significant_Stimulus == Inf] = NA
ID_Sim_Summary_acf$lag_significant_History[ID_Sim_Summary_acf$lag_significant_History == Inf] = NA

##
## Autocorrelations decay over time
##

Sim$min_diff_acf_Stimulus = Sim$diff_acf_Stimulus - min(Sim$diff_acf_Stimulus, na.rm = TRUE)
Sim$min_diff_acf_History = Sim$diff_acf_History - min(Sim$diff_acf_History, na.rm = TRUE)

Sim_lmer_acf_Stimulus <- lmer(log(min_diff_acf_Stimulus) ~ Trial + (1|subject_id), data = Sim[Sim$Trial > 1 & Sim$Trial < 21 & Sim$min_diff_acf_Stimulus > 0,])
Sim_STAT.lmer_acf_Stimulus <- summary(Sim_lmer_acf_Stimulus)

Sim_lmer_acf_History <- lmer(log(min_diff_acf_History) ~ Trial + (1|subject_id), data = Sim[Sim$Trial > 1 & Sim$Trial < 21 & Sim$min_diff_acf_History > 0,])
Sim_STAT.lmer_acf_History <- summary(Sim_lmer_acf_History)


if (compute_power_spectra_simulation) {
source("./Functions/f_compute_power_spectra.R", local = knitr::knit_global())

sliders = c("Accuracy_slider", "History_slider")  
Sim_Power_Spectra <- f_compute_power_spectra(Sim, sliders)  
} else {
  #Sim_Power_Spectra <- read.csv(paste(root, "Sim_power_spectra_smoothed_param_variable_n_human_zeta.csv", sep = ""))
  Sim_Power_Spectra <- read.csv(paste(root, "Power_Spectra_LLR.csv", sep = ""))
}

Sim_Power_Spectra$r_freq = round(Sim_Power_Spectra$freq, digits = 4)

step_size = 0.01
bins <- seq(from = min(Sim_Power_Spectra$freq), to = max(Sim_Power_Spectra$freq), by = step_size)
names <- round(bins, digits = 2)
names = names[1:length(names) - 1]

Sim_Power_Spectra$bin_freq <- cut(Sim_Power_Spectra$freq, breaks = bins, labels = names)
Sim_Power_Spectra$bin_freq <- as.numeric(Sim_Power_Spectra$bin_freq) * step_size
Sim_Power_Spectra$bin_freq[is.na(Sim_Power_Spectra$bin_freq)] = max(Sim_Power_Spectra$freq)

Sim_Power_Spectra$Coherence <- Sim_Power_Spectra$Coherence * 100



gathercol = colnames(Sim_Power_Spectra[, c(2,3)])
Sim_Power_Spectra_long  <-
gather(Sim_Power_Spectra[, c(1,2,3, 6,7,8)],
"Variable",
"Power",
gathercol,
factor_key = TRUE)

Sim_Power_Frequency <-  ddply(
      Sim_Power_Spectra_long,
      .(r_freq, Variable),
      summarise,
      
      mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
      ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE)/sqrt(length(exclude_3SD(Power)))
 )

Sim_Coherence_Phase_Frequency <- ddply(
      Sim_Power_Spectra,
      .(bin_freq),
      summarise,
    
      mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
      ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE)/sqrt(length(exclude_3SD(Coherence))),
      
      mode_phase = getmode(exclude_3SD((abs(Phase)))),
      mean_phase = getmode(exclude_3SD((abs(Phase)))),
      ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE)/sqrt(length(exclude_3SD(Phase)))
 )

##
## mode of coherence and phase
##
Sim_Summary_Power_Spectra <- 
   ddply(
 Sim_Power_Spectra[Sim_Power_Spectra$freq > 0.01 & Sim_Power_Spectra$freq < 0.1,],
       .(subject_id),
       summarise,
       mean_coherence = mean(exclude_3SD((
 Coherence))),
       mode_phase = getmode(exclude_3SD((
 abs(Phase))))
   )

##
## STATS Power vs Frequency (1/f noise)
##

Sim_lmer_power_freq_Stimulus <- lmer(log(Power_Stimulus) ~ log(freq) + (1|subject_id), data = Sim_Power_Spectra[Sim_Power_Spectra$freq > 0.01 & Sim_Power_Spectra$freq < 0.1 & is.finite(log(Sim_Power_Spectra$Power_Stimulus)),])
Sim_STAT.lmer_power_freq_Stimulus <- summary(Sim_lmer_power_freq_Stimulus)

Sim_lmer_power_freq_History <- lmer(log(Power_History) ~ log(freq) + (1|subject_id), data = Sim_Power_Spectra[Sim_Power_Spectra$freq > 0.01 & Sim_Power_Spectra$freq < 0.1 & is.finite(log(Sim_Power_Spectra$Power_History)),])
Sim_STAT.lmer_power_freq_History <- summary(Sim_lmer_power_freq_History)


##
## Simulated modes
##
Sim_Mode_gather <-  ddply(
  Sim,
  .(subject_id),
  summarise,
  
  directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
  scaled_directed_mode = scale(directed_mode),
  strength_mode = abs(scaled_directed_mode)
)

Sim$directed_mode = Sim_Mode_gather$directed_mode
Sim$scaled_directed_mode = Sim_Mode_gather$scaled_directed_mode
Sim$strength_mode = Sim_Mode_gather$strength_mode

Sim_ID_mode <-  ddply(
  Sim,
  .(subject_id),
  summarise,
  strength_mode = mean(strength_mode, na.rm = TRUE),
  directed_mode = mean(directed_mode, na.rm = TRUE)
)


##
## Simulated confidence
##

Sim$Confidence <- abs(Sim$mu) 
Sim$scaled_Confidence <- scale(Sim$Confidence)
Sim$clear_Confidence <- Sim$Confidence
Sim[Sim$clear_Confidence > median(Sim$clear_Confidence, na.rm = TRUE) + 3*median(Sim$clear_Confidence, na.rm = TRUE),]$clear_Confidence <- NA

Sim_Post_Perceptual_Modes = ddply(
  Sim[!is.na(Sim$History) & !is.na(Sim$Accuracy) & !is.na(Sim$clear_Confidence),], 
  .(directed_mode),
  summarise,
  average_Confidence = mean(clear_Confidence, na.rm = TRUE),
  se_Confidence = sd(clear_Confidence, na.rm = TRUE)/sqrt(length(clear_Confidence)),
  n = length(mu),
  n_percent = (sum(!is.na(mu))/nrow(Sim))*100
)

Sim_slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|subject_id), data = Sim)
Sim_STAT.slider_History_vs_Accuracy <- summary(Sim_slider_History_vs_Accuracy)

##
## Confidence vs mode: Prepare plot and stats
##
Sim_Confidence_Behav <- 
  ddply(
    Sim[!is.na(Sim$History) & !is.na(Sim$Accuracy) & !is.na(Sim$clear_Confidence),], 
    .(subject_id),
    summarise,
    diff_Confidence_History = mean(clear_Confidence[History == 1], na.rm = TRUE) -  mean(clear_Confidence[History == 0], na.rm = TRUE),
    diff_Confidence_Stimulus = mean(clear_Confidence[Accuracy == 1], na.rm = TRUE) -  mean(clear_Confidence[Accuracy == 0], na.rm = TRUE)
    )


gathercol = colnames(Sim_Confidence_Behav[, c(2,3)])
Sim_Confidence_Behav_long  <-
  gather(Sim_Confidence_Behav[,c(1,2,3)],
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)

Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_History", "History", Sim_Confidence_Behav_long$Variable)
Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_Stimulus", "Stimulus", Sim_Confidence_Behav_long$Variable)

Sim_diff_Confidence <- lmer(diff ~ Variable + (1|subject_id), data = Sim_Confidence_Behav_long)
Sim_STAT.diff_Confidence <- summary(Sim_diff_Confidence)

## exclude low performance mice
Summary_Sim_Confidence_Behav <- 
  ddply(
    Sim_Confidence_Behav_long, 
    .(Variable),
    summarise,
    Mean = mean(diff, na.rm = TRUE),
    Error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))

Sim_lmer_Confidence_Accuracy_History <- lmer(clear_Confidence ~ Accuracy + History + (1|subject_id), data = Sim)
Sim_STAT.lmer_Confidence_Accuracy_History <- summary(Sim_lmer_Confidence_Accuracy_History)


##
## Confidence vs. mode
##

Sim_Confidence_vs_mode <- lmer(clear_Confidence ~ poly(directed_mode, 2) + (1|subject_id), data = Sim)
Sim_STAT.Confidence_vs_mode <- summary(Sim_Confidence_vs_mode)
  
  if (save_summary_data) {
    save(Sim_Behav, 
         Sim_Behav_diff,
         Sim_STAT.Global_History_Accuracy,
         Sim_STAT.diff_History_Accuracy,
         Sim_Summary_acf,
         ID_Sim_Summary_acf,
         Sim_STAT.lmer_acf_Stimulus,
         Sim_STAT.lmer_acf_History,
         Sim_Power_Spectra_long,
         Sim_Power_Frequency,
         Sim_Coherence_Phase_Frequency,
         Sim_Summary_Power_Spectra,
         Sim_STAT.lmer_power_freq_Stimulus,
         Sim_STAT.lmer_power_freq_History,
         Sim_Post_Perceptual_Modes,
         Sim_STAT.slider_History_vs_Accuracy,
         Sim_Confidence_Behav,
         Sim_Confidence_Behav_long,
         Sim_STAT.diff_Confidence,
         Summary_Sim_Confidence_Behav,
         Sim_STAT.lmer_Confidence_Accuracy_History,
         Sim_STAT.Confidence_vs_mode,
         Sim_Power_Spectra,
        file = "./Summary_Data/simulation_reproduce_zeta.Rdata")
  }
  
} else {
  load("./Summary_Data/simulation_reproduce_zeta.Rdata")
}
```


```{r evaluate_human, cache = TRUE}

type_list = c(
  "fit_glaze_osc_zeta_v1",
  "fit_glaze_osc_zeta_v1_one_amp",
  "fit_glaze_osc_zeta_v1_LLR_amp",
  "fit_glaze_osc_zeta_v1_Prior_amp",
  "fit_glaze_osc_zeta_v1_no_amp",
  "fit_glaze_osc_zeta_v1_no_integration"
  #"fit_glaze_osc_zeta_v1_random_walk_mouse"
  )


  # available models
  source("./Functions/fit_glaze_osc_zeta_v1.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_one_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_integration.R",
  local = knitr::knit_global())


block_size = 10
Optim_eval = data.frame()
#for (id in unique(PwData$subject_id)) {
for (id in unique(PwData$subject_id[PwData$subject_id > max(PwData$subject_id)/2])) {
  print(paste("human:", id, sep = " "))
  
    Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_RT", "clear_Confidence")]
  
  
    Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
                              !is.na(Input_Data$Response), ]
    
    if (nrow(Input_Data)  > block_size) {
      for (trial_idx in seq(from = 1, to = nrow(Input_Data) - block_size  + 1, by = block_size)) {
      #for (trial_idx  in c(1)) {
        train =  Input_Data[seq(from = trial_idx,
                                to = trial_idx + block_size  - 1,
                                by = 1), ]
        
        for (type_idx in c(1:length(type_list))) {
        #print(type_list[type_idx])
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {
            ## Starting parameters
            par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 5)
            lower = c(0.01, 0.01, 1)
            upper = c(0.99, 20, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration") {
            ## Starting parameters
            par = c(0.01, 5)
            lower = c(0.01, 1)
            upper = c(20, 10)
          }
        
          suppressWarnings({
            add_O =  optimx(
              par,
              match.fun(type_list[type_idx]),
              Input_Data = train,
              lower = lower,
              upper = upper
            )
          })
    
          
          
          add_O$subject_id = id
          add_O$block_id = trial_idx
          add_O$trial_n = nrow(train)
          add_O$free_param = length(par)
          add_O$type = type_list[type_idx]
          add_O$AIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 0
          )
          add_O$cAIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 1
          )
          
          Optim_eval = rbind(Optim_eval, add_O[, c('subject_id', 'block_id','value', 'AIC', 'type', 'free_param', 'trial_n', 'convcode')])
          #Optim_eval = rbind(Optim_eval, add_O)
        }
      
    }
    }
    
    write.csv(Optim_eval, "./Results/Optim_eval_human_interim_min_4021.csv", row.names = FALSE)
    Sum <-   ddply(Optim_eval,
                   .(type),
                   summarise,
                   error = mean(value, na.rm = TRUE))
    print(Sum)
}
```

```{r evaluate_mouse, cache = TRUE}
type_list = c(
  "fit_glaze_osc_zeta_v1_mouse",
  "fit_glaze_osc_zeta_v1_one_amp_mouse",
  "fit_glaze_osc_zeta_v1_LLR_amp_mouse",
  "fit_glaze_osc_zeta_v1_Prior_amp_mouse",
  "fit_glaze_osc_zeta_v1_no_amp_mouse",
  "fit_glaze_osc_zeta_v1_no_integration_mouse"
  #"fit_glaze_osc_zeta_v1_random_walk_mouse"
  )


source("./Functions/fit_glaze_osc_zeta_v1_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_one_amp_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_no_amp_mouse.R",
       local = knitr::knit_global())
source( "./Functions/fit_glaze_osc_zeta_v1_no_integration_mouse.R",
  local = knitr::knit_global()
)
source("./Functions/fit_glaze_osc_zeta_v1_random_walk_mouse.R",
  local = knitr::knit_global()
)


block_size = 10
Optim_eval = data.frame()
#for (id in unique(MwData$subject_id)) {
for (id in unique(MwData$subject_id[MwData$subject_id > max(MwData$subject_id)/2])) {
  print(paste("mouse:", id, sep = " "))
  for (session_id in unique(MwData[MwData$subject_id == id, ]$session_id)) {
    print(paste("session:", session_id, sep = " "))
    Input_Data = MwData[MwData$subject_id == id &
                          MwData$session_id == session_id, c("Stimulus", "Response", "Difficulty")]
    
    Input_Data$Stimulus <-
      (Input_Data$Stimulus * 2 - 1) * Input_Data$Difficulty
    Input_Data$Stimulus[Input_Data$Difficulty == 0] = 0
    Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
                              !is.na(Input_Data$Response), ]
    
    if (nrow(Input_Data)  > block_size) {
      for (trial_idx in seq(from = 1, to = nrow(Input_Data) - block_size  + 1, by = block_size)) {
      #for (trial_idx  in c(1)) {
        train =  Input_Data[seq(from = trial_idx,
                                to = trial_idx + block_size  - 1,
                                by = 1), ]
        
        for (type_idx in c(1:length(type_list))) {

          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 5)
            lower = c(0.01, 0.01, 1)
            upper = c(0.99, 20, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration_mouse") {
            ## Starting parameters
            par = c(0.01, 5)
            lower = c(0.01, 1)
            upper = c(20, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_random_walk_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 3, 0.1, 5)
            lower = c(0.01, 0.01,-5, 0.01, 1)
            upper = c(0.99, 20,+5, 1, 10)
          }
          
          add_O =  optimx(
            par,
            match.fun(type_list[type_idx]),
            Input_Data = train,
            lower = lower,
            upper = upper
          )
          
          
          add_O$subject_id = id
          add_O$session_id = session_id
          add_O$block_id = trial_idx
          add_O$trial_n = nrow(train)
          add_O$free_param = length(par)
          add_O$type = type_list[type_idx]
          add_O$AIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 0
          )
          add_O$cAIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 1
          )
       Optim_eval = rbind(Optim_eval, add_O[, c('subject_id', 'session_id', 'block_id', 'value', 'type', 'free_param', 'trial_n', 'convcode')])   
        }
      }
    }
  }
  
          write.csv(Optim_eval,
              "./Results/Optim_eval_mouse_interim_min_100.csv",
              row.names = FALSE)
          
          # Sum <- ddply(Optim_eval,
          #          .(type),
          #          summarise,
          #          error = mean(abs(value), na.rm = TRUE))
          # print(Sum)
}
```

Simulations from the bimodal inference model (based on the posterior model parameters obtained in humans; see Methods for details) closely matched the empirical results outlined above: Simulated perceptual decisions resulted from a competition of perceptual history with incoming sensory signals (Figure 6A). Stimulus- and history-congruence were significantly auto-correlated (Figure 6B-C), fluctuating in anti-phase as a scale-invariant process with a 1/f power law (Figure 6D-F). Simulated posterior certainty[@Kepecs2008; @Urai2017; @Braun2018] (i.e., the absolute of the posterior log ratio $|L_t|$) showed a quadratic relationship to the mode of sensory processing (Figure 6H), mirroring the relation of RTs and confidence reports to external and internal biases in perception (Figure 2G-H and Figure 4G-H). Crucially, the overlap between empirical and simulated data broke down when we removed the anti-phase oscillations ($\omega_{LLR}$ and/or $\omega_{\psi}$) or the accumulation of evidence over time (i.e., setting $H$ to 0.5) from the bimodal inference model (see Supplemental Figure S7-10).

<!-- These qualitative results were confirmed by a formal comparison in which we tested the explanatory power of the bimodal inference model against the reduced versions of itself. We conducted group-level random-effects Bayesian model selection using Variational Bayesian Analysis (VBA[@Stephan2009]), which indicated that the bimodal inference model achieved the highest log model evidence, yielding an exceedance probability of 100% in humans and mice (see Supplemental Figure SX2C for a visualization of the log model evidence in bits per trial across models).  -->

To further probe the validity of the bimodal inference model, we tested whether posterior model quantities could explain aspects of the behavioral data that the model was not fitted to. First, we predicted that the posterior decision variable $L_t$ not only encodes perceptual choices (i.e., the variable used for model estimation), but should also predict the speed of response and subjective confidence[@Kepecs2008; @Braun2018]. Indeed, the estimated trial-wise posterior decision certainty $|L_t|$ correlated negatively with RTs in humans ($\beta$ = $`r STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_RT_Certainty$coefficients[2,5]`$) 
and TDs mice ($\beta$ = $`r M_STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r M_STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r M_STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r M_STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r M_STAT.Optim_RT_Certainty$coefficients[2,5]`$). 
Likewise, subjective confidence was positively correlated with the estimated posterior decision certainty in humans ($\beta$ = $`r STAT.Optim_Confidence_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_Confidence_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_Confidence_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_Confidence_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_Confidence_Certainty$coefficients[2,5]`$). 

Second, the dynamic accumulation of information inherent to our model entails that biases toward perceptual history are stronger when the posterior decision certainty at the preceding trial is high[@Glaze2015; @Braun2018; @Bergen2019]. Due to the link between posterior decision certainty and confidence, we reasoned that confident perceptual choices should be more likely to induce history-congruent perception at the subsequent trial[@Braun2018; @Bergen2019]. Indeed, logistic regression indicated that history-congruence was predicted by the posterior decision certainty $|L_{t-1}|$  (humans: $\beta$ = $`r STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_mu_minus_1$coefficients[2,4]`$; 
mice: $\beta$ = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,4]`$) and subjective confidence (humans: $\beta$ = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,4]`$) at the preceding trial.

In sum, computational modeling thus suggested that between-mode fluctuations are best explained by two interlinked processes (Figure 1E): (i), the dynamic accumulation of information across successive trials (i.e., following the estimated hazard rate $H$) and, (ii), ongoing anti-phase oscillations in the impact of external and internal information (i.e., as determined by $\omega_{\psi}$ and $\omega_{LLR}$).

## Bimodal inference improves learning and perceptual metacognition in the absence of feedback

```{r Simulation_adaptive_feedback, cache = TRUE} 

if (!load_summary_data) {
  
source("./Functions/simulation_glaze_osc_human_zeta_v1_adaptive.R", local = knitr::knit_global())

##
## generate environment Settings
##
n_participants_adaptive = 5000

n_blocks = 20
block_length  <- runif(min = 100, max = 100, n_blocks)


var_prec = 0
zeta = 1

outcomes = c(0,1)

# learning rates
alpha_switch = sample(seq(0.05,0.25,0.05),n_participants_adaptive, replace = TRUE)
alpha_stay = alpha_switch
alpha_percept = sample(seq(0.05,0.25,0.05),n_participants_adaptive, replace = TRUE)

# specifics of bimodal inference
frequency = sample(seq(0.05,0.15,0.025),n_participants_adaptive, replace = TRUE)
amp = sample(seq(1,1,1),n_participants_adaptive, replace = TRUE)

# starting values for H and P
H_logit = sample(seq(-0.25,0,0.01),n_participants_adaptive, replace = TRUE)
P_logit = sample(seq(0.25,2,0.25),n_participants_adaptive, replace = TRUE)
if (run_adaptive_simulation){
  Summary_Sim_adaptive = data.frame()
for (feedback_level in seq(from = 0.6, to = 1, by = 0.1)){

#Sim <- data.frame()

for (subj_idx in c(1:n_participants_adaptive)){
  {print(subj_idx)}
  
# hazard rate and encoding precision of environment  
block_probs <- sample(seq(0.1,1,0.2),n_blocks, replace = TRUE)
block_precs <- sample(seq(2,8,1),n_blocks, replace = TRUE)

  for (amp_on_idx in c(0,1)){
    Sim_add <-
      simulation_glaze_osc_human_zeta_v1_adaptive(
      n_blocks,
      block_length,
      block_probs,
      block_precs,
      var_prec,
      H_logit[subj_idx],
      P_logit[subj_idx],
      outcomes,
      alpha_switch[subj_idx],
      alpha_stay[subj_idx],
      alpha_percept[subj_idx],
      amp[subj_idx],
      amp_on_idx,
      frequency[subj_idx],
      zeta,
      sliding_window,
      n_permutations,
      feedback_level
      ) 
  
  Sim_add$diff_acf_Stimulus <- exclude_3SD(Sim_add$acf_Stimulus - Sim_add$random_acf_Stimulus) 
  Sim_add$diff_acf_History <- exclude_3SD(Sim_add$acf_History - Sim_add$random_acf_History) 
  Sim_add$subject_id = subj_idx
  Sim_add$amp = amp[subj_idx]
  Sim_add$amp_on = amp_on_idx
  Sim_add$alpha_switch = alpha_switch[subj_idx]
  Sim_add$alpha_stay = alpha_stay[subj_idx]
  Sim_add$alpha_percept = alpha_percept[subj_idx]
  Sim_add$frequency = frequency[subj_idx]
  Sim_add$zeta =  zeta
  Sim_add$feedback_level = feedback_level
  
  
gathercol = colnames(Sim_add[,c(25,33,34, 41,42)])
Sim_long  <-
gather(Sim_add[,c(25,33,34,41,42,43,44,45,51)],
"Variable",
"Value",
gathercol,
factor_key = TRUE)

Sim_long$Variable <- gsub("error_", "", Sim_long$Variable)


add_Summary_Sim_adaptive <-
  ddply(
    Sim_long[(Sim_long$Variable == "Accuracy" | Sim_long$Variable == "H" | Sim_long$Variable == "P"),],
    .(amp_on, amp, feedback_level, Variable, subject_id),
    summarise,
    Mean = mean(abs(Value), na.rm = TRUE),
    Error = sd(abs(Value), na.rm = TRUE)/sqrt(length(abs(Value))))

Summary_Sim_adaptive = rbind(Summary_Sim_adaptive, add_Summary_Sim_adaptive)  
  }
}

write.csv(Summary_Sim_adaptive, paste(root, "Sim_adaptive_v1_feedback_level_", as.character(feedback_level), "n_", as.character(n_participants_adaptive) ,".csv", sep = ""), row.names = FALSE)
}
} else {

Summary_Sim_adaptive <- read.csv( "./Results/Summary_Sim_adaptive_v1.csv")  
}
Summary_Sim_adaptive[Summary_Sim_adaptive$Variable == "Accuracy",]$Mean = Summary_Sim_adaptive[Summary_Sim_adaptive$Variable == "Accuracy",]$Mean*100 


Summary_Sim_adaptive$amp <- as.factor(Summary_Sim_adaptive$amp)
Summary_Sim_adaptive$amp_on <- as.factor(Summary_Sim_adaptive$amp_on)
Summary_Sim_adaptive$feedback_level <- as.factor(Summary_Sim_adaptive$feedback_level*100)
Summary_Sim_adaptive$Variable <- as.factor(Summary_Sim_adaptive$Variable)

Summary_Sim_adaptive_diff <-
  ddply(
    Summary_Sim_adaptive,
    .(Variable, amp, feedback_level, subject_id),
    summarise,
    diff = Mean[amp_on == 1] - Mean[amp_on == 0])

Group_Summary_Sim_adaptive_diff <-
  ddply(
    Summary_Sim_adaptive_diff,
    .(Variable, amp, feedback_level),
    summarise,
   mean = mean(diff, na.rm = TRUE),
   error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))

## STATS Adaptive Stimulation ## 
Sim_STAT.diff_Mode_H = t.test(Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff == "H" & Summary_Sim_adaptive_diff$feedback_level == 0,]$diff)
Sim_STAT.diff_Mode_P = t.test(Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff == "P" & Summary_Sim_adaptive_diff$feedback_level == 0,]$diff)
Sim_STAT.diff_Mode_Accuracy = t.test(Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff == "Accuracy" & Summary_Sim_adaptive_diff$feedback_level == 0,]$diff)

Sim_H_vs_feedback <- lmer(diff ~ as.numeric(feedback_level) + (1|subject_id), data = Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff == "H",])
Sim_STAT.H_vs_feedback <- summary(Sim_H_vs_feedback)
Sim_P_vs_feedback <- lmer(diff ~ as.numeric(feedback_level) + (1|subject_id), data = Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff == "P",])
Sim_STAT.P_vs_feedback <- summary(Sim_P_vs_feedback)
Sim_Accuracy_vs_feedback <- lmer(diff ~ as.numeric(feedback_level) + (1|subject_id), data = Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff == "Accuracy",])
Sim_STAT.Accuracy_vs_feedback <- summary(Sim_Accuracy_vs_feedback)  
  
  if (save_summary_data) {
    save(n_participants_adaptive, n_blocks, block_length, H_logit, P_logit,
         Summary_Sim_adaptive,Summary_Sim_adaptive_diff,
         Group_Summary_Sim_adaptive_diff,
         Sim_STAT.diff_Mode_H, Sim_STAT.diff_Mode_P, Sim_STAT.diff_Mode_Accuracy,
         Sim_STAT.H_vs_feedback, Sim_STAT.P_vs_feedback, Sim_STAT.Accuracy_vs_feedback,
         file = "./Summary_Data/Simulation_adaptive_feedback.Rdata")
  }
  
} else {
  load("./Summary_Data/Simulation_adaptive_feedback.Rdata")
}
```

Is there a computational benefit to be gained from temporarily down-regulating biases toward preceding choices (Figure 2-3 B and C), instead of combining them with external sensory information at a constant weight (Supplemental Figure S7)? In their adaptive function for perceptual decision-making, internal predictions critically depend on error-driven learning to remain aligned with the current state of the world[@Rao1999]. Yet when the same network processes external and internal information in parallel, inferences may become circular and maladaptive[@Jardri2017]: Ongoing decision-related activity may be distorted by noise in external sensory signals that are fed forward from the periphery or, alternatively, by aberrant internal predictions about the environment that are fed back form higher cortical levels[@Jardri2017; @Honey2017]. 

Purely parallel processing therefore creates at least two challenges for perception: First, due to the sequential integration of inputs over time, internal predictions may progressively override sensory information[@Wexler2015], leading to false inferences about the presented stimuli[@Weilnhammer2021a]. As a consequence, purely parallel processing may also lead to false inferences about the statistical regularities of volatile environments, where the underlying hazard rate $\hat{H} = P(s_t \neq s_{t-1})$ (i.e., the probability of a change in the state of the environment between two trials) may change over time. In the absence of feedback, agents have to update the estimate about $\hat{H}$ solely on the grounds of their experience, which is determined by the posterior log ratio $L_t$. Yet $L_t$ depends not only on external information from the environment (the log likelihood ratio $LLR_t$), but also on internal predictions, i.e., the log prior ratio $L_{t-1}$ and the assumed hazard rate $H_t$. This circularity may impair the ability to learn about changes in $H$ that occur in volatile environments (Figure 7A).  

Second, purely parallel processing may also reduce the capacity to calibrate metacognitive beliefs about ongoing changes in the precision at which sensory signals are encoded. In the absence of feedback, agents depend on internal confidence signals[@Guggenmos2016] (i.e., the absolute of the posterior log ratio $|L_t|$) to update beliefs $M_t$ about the precision of sensory encoding $\hat{M} = 1 - |s_t-u_t|$. While $\hat{M}$ depends only on the likelihood $LLR_t$, the estimate $M_t$ is informed by the posterior $L_t$, which, in turn, is additionally modulated by the prior $L_{t-1}$ and $H_t$. Relying on internal predictions may thus distort metacognitive beliefs about the precision of sensory encoding (Figure 7B). This problem becomes particularly relevant when agents do not have full insight into the strength at which external and internal sources of information contribute to perceptual inference (i.e., when confidence is high during both internally- and externally-biased processing; Figure 2I-J; Figure 6G-H).

Here, we propose that bimodal inference may provide potential solutions to these problems of circular inference. By intermittently decoupling the decision variable $L_t$ from internal predictions, between-mode fluctuations may create unambiguous error signals that adaptively update estimates about the hazard rate $\hat{H}$ and the precision of sensory encoding $\hat{M}$. 

To illustrate this hypothesis, we simulated data for a total of $`r (length(unique(Summary_Sim_adaptive$subject_id)))`$
participants who performed binary perceptual decisions for a total of $`r n_blocks`$ blocks of $`r mean(block_length)`$ trials each. Each block differed with respect to the true hazard rate $\hat{H}$ (either 0.1, 0.3, 0.5, 0.7 or 0.9) and the sensitivity parameter $\alpha$ (either 2, 3, 4, 5 or 6, determining $\hat{M}$ via the absolute of the log likelihood ratio $|LLR_t|$, Figure 7A-B, upper panel). Importantly, the synthetic participants did not receive feedback on whether their perceptual decisions were correct. 

We initialized each participant at a random value of $H'_t$ (ranging from $`r min(H_logit)`$ to $`r max(H_logit)`$) and $M'_t$ (ranging from $`r min(P_logit)`$ to $`r max(P_logit)`$), which were transformed into the unit interval to yield trial-wise estimates for $H_t$ and $M_t$:

\begin{equation}
H_t = \frac{1}{1+exp(-(H'_t))}
\end{equation}

\begin{equation}
M_t = \frac{1}{1+exp(-(M'_t))}
\end{equation}

For each block, we generated stimuli $s_t$ using the true hazard rate $\hat{H}$. Detected inputs $u_t$ were computed according to the block-wise sensitivity parameter $\alpha$. Perceptual decisions $y_t$ were generated using the bimodal inference model with ($a_{\psi}$ = $a_{LLR}$ = 1, $\zeta$ = 1 and $f$ between 0.05 and 0.15) and a unimodal control model ($a_{\psi}$ = $a_{LLR}$ = 0, $\zeta$ = 1).

Leaning about $H$ was driven by the error-term $\epsilon_H$ (Figure 7A, lower panel), reflecting the difference between $H_t$ and presence of a perceived change in the environment $|y_t - y_{t-1}|$:

\begin{equation}
\epsilon_H = |y_t - y_{t-1}| - H_t
\end{equation}

Trial-wise updates to $H$ were provided by a Resorla-Wagner-rule with learning rate $\beta_H$ (ranging from 0.05 to 0.25). Since $y_t$ is more likely to accurately reflect the state of the environment during external mode, updates to $H$ were additionally modulated by $\omega_{LLR}$:

\begin{equation}
H'_t = H'_{t-1} + \beta_H *\omega_{LLR} * \epsilon_H
\end{equation}  
    
Learning about $\hat{M}$ was driven by error-term $\epsilon_M$ (Figure 7B, lower panel), reflecting the difference between $M_t$ and the posterior decision-certainty $(1-|y_t - P(y_t = 1)|)$: 

\begin{equation}
\epsilon_M = (1-|y_t - P(y_t = 1)|) - M_t
\end{equation} 

In analogy to $H$, we modeled trial-wise updates to $M$ using a Rescorla-Wagner-rule with learning rate $\beta_M$ (ranging from 0.05 to 0.25). Since $y_t$ reflects the log likelihood ratio $LLR_t$ (and therefore the precision of sensory encoding) more closely during external mode, updates to $P$ were additionally modulated by $\omega_{LLR}$:

\begin{equation}
M'_t = M'_{t-1} + \beta_M *\omega_{LLR} * \epsilon_M
\end{equation}  

For each participant, we simulated data using both the bimodal inference model described above and a unimodal control model, in which the between-mode fluctuations were removed by setting the amplitude parameter $a$ to zero ($a_{\psi}$ = $a_{LLR}$ = 0). We compared the bimodal model of perceptual inference to the unimodal control model in terms of three dependent variables: the probability of stimulus-congruent perceptual choices, the error in the estimate about $H$ (i.e., $|H - \hat{H}|$) and the error in the estimate about $M$ (i.e., $|M - \hat{M}|$, with $\hat{M} = 1- (|s_t-u_t|)$). 

We found that the bimodal inference model achieved lower stimulus-congruence in comparison to the unimodal control model ($\beta_1$ = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,1]`$ ± $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,2]`$, T($`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,3]`$) = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,4]`$, p = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,5]`$, Figure 7C). 
At the same time, the bimodal inference model yielded lower errors in the estimated hazard rate $H$ ($\beta_1$ = $`r Sim_STAT.H_vs_feedback$coefficients[1,1]`$ ± $`r Sim_STAT.H_vs_feedback$coefficients[1,2]`$, T($`r Sim_STAT.H_vs_feedback$coefficients[1,3]`$) = $`r Sim_STAT.H_vs_feedback$coefficients[1,4]`$, p = $`r Sim_STAT.H_vs_feedback$coefficients[1,5]`$) 
and probability of stimulus-congruent choices $P$ ($\beta_1$ = $`r Sim_STAT.P_vs_feedback$coefficients[1,1]`$ ± $`r Sim_STAT.P_vs_feedback$coefficients[1,2]`$, T($`r Sim_STAT.P_vs_feedback$coefficients[1,3]`$) = $`r Sim_STAT.P_vs_feedback$coefficients[1,4]`$, p = $`r Sim_STAT.P_vs_feedback$coefficients[1,5]`$, Figure 7E). This suggests that between-mode fluctuations may play an adaptive role for learning and perceptual metacognition by supporting robust inferences about the statistical regularities of volatile environments and ongoing changes in the precision of sensory encoding.

Finally, we asked whether differences between the bimodal inference model the unimodal control model depend on the presence of external feedback. We predicted that the benefits of the bimodal inference model over the unimodal control model should be lost when feedback is provided: With feedback, the error terms that induce updates in $H$ and $P$ can be informed by the true state of the environment $s_t$ instead of posterior stimulus probabilities that are distorted by circular inferences: 

\begin{equation}
\epsilon_H = |s_t - s_{t-1}| - H_t
\end{equation}

\begin{equation}
\epsilon_M = (1- (|y_t - s_t|)) - M_t
\end{equation} 

We repeated the above simulation for each participant while providing feedback on a subset of trials (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% and 100%). With increasing availability of external feedback, the bimodal inference model lost its advantage over the unimodal control model in terms of, (i), the estimated hazard rate $H$ ($\beta_2$ = $`r Sim_STAT.H_vs_feedback$coefficients[2,1]`$ ± $`r Sim_STAT.H_vs_feedback$coefficients[2,2]`$, T($`r Sim_STAT.H_vs_feedback$coefficients[2,3]`$) = $`r Sim_STAT.H_vs_feedback$coefficients[2,4]`$, p = $`r Sim_STAT.H_vs_feedback$coefficients[2,5]`$) 
and, (ii), the estimated probability of stimulus-congruent choices $M$ ($\beta_2$ = $`r Sim_STAT.P_vs_feedback$coefficients[2,1]`$ ± $`r Sim_STAT.P_vs_feedback$coefficients[2,2]`$, T($`r Sim_STAT.P_vs_feedback$coefficients[2,3]`$) = $`r Sim_STAT.P_vs_feedback$coefficients[2,4]`$, p = $`r Sim_STAT.P_vs_feedback$coefficients[2,5]`$, Figure 7F). 
This indicates that the benefits of bimodal inference are limited to situations in which external feedback is sparse. 

\newpage
# Discussion

This work investigates the behavioral and computational characteristics of ongoing fluctuations in perceptual decision-making using two large-scale datasets in humans[@Rahnev2020] and mice[@Aguillon-Rodriguez2020]. We found that humans and mice cycle through recurring intervals of reduced sensitivity to external sensory information, during which they relied more strongly on perceptual history, i.e., an internal prediction that is provided by the sequence of preceding choices. Computational modeling indicated that these slow periodicities are governed by two interlinked factors: (i), the dynamic integration of sensory inputs over time and, (ii), anti-phase oscillations in the strength at which perception is driven by internal versus external sources of information. These cross-species results suggest that ongoing fluctuations in perceptual decision-making arise not merely as a noise-related epiphenomenon of limited processing capacity, but result from a structured and adaptive mechanism that fluctuates between internally- and externally-oriented modes of sensory analysis.

## Serial dependencies represent a pervasive and adaptive aspect of perceptual decision-making in humans and mice

A growing body of literature has highlighted that perception is modulated by preceding choices[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Urai2019; @Hsu2020; @Braun2018]. Our work provides converging cross-species evidence supporting the notion that such serial dependencies are a pervasive and general phenomenon of perceptual decision-making (Figures 2 and 4, Supplemental Figures 1 and 3). While introducing errors in randomized psychophysical designs[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] (Figures 2 and 4A), we found that perceptual history facilitates post-perceptual processes such as speed of response[@Cicchini2018] (Figure 2G) and subjective confidence in humans (Figure 2I). 

At the level of individual traits, increased biases toward preceding choices were associated with reduced sensitivity to external information (Supplemental Figure 1C-D) and lower metacognitive efficiency. When investigating how serial dependencies evolve over time, we observed dynamic changes in the strength of perceptual history (Figures 2 and 4B) that created wavering biases toward internally- and externally-biased modes of sensory processing. Between-mode fluctuations may thus provide a new explanation for ongoing changes in perceptual performance[@Verplanck1952; @Atkinson1963; @Dehaene1993; @Gilden1995; @Gilden1995a; @Monto2008]. 

In computational terms, serial dependencies may leverage the temporal autocorrelation of natural environments[@St.John-Saaltink2016; @Bergen2019] to increase the efficiency of decision-making[@Burr2014; @Kiyonaga2017]. Such temporal smoothing[@St.John-Saaltink2016] of sensory inputs may be achieved by updating dynamic predictions about the world based on the sequence of noisy perceptual experiences[@fischer_serial_2014; @Bergen2019], using algorithms such as Kalman filtering[@Burr2014], Hierarchical Gaussian filtering[@Mathys2014a] or sequential Bayes[@Cicchini2014; @Glaze2015; @Cicchini2018]. At the level of neural mechanisms, the integration of internal with external information may be realized by combining feedback from higher levels in the cortical hierarchy with incoming sensory signals that are fed forward from lower levels[@Sterzer2018]. 

Yet relying too strongly on serial dependencies may come at a cost: When accumulating over time, internal predictions may eventually override external information, leading to circular and false inferences about the state of the environment. In this work, we used model simulations to show that, akin to the wake-sleep-algorithm in machine learning[@Bengio2015], bimodal inference may help to determine whether errors result from external input or from internally-stored predictions (Figure 7): During internal mode, sensory processing is more strongly constrained by predictive processes that auto-encode the agent's environment. Conversely, during external mode, the network is driven predominantly by sensory inputs[@Honey2017]. Between-mode fluctuations may thus generate an unambiguous error signal that aligns internal predictions with the current state of the environment in iterative test-update-cycles[@Bengio2015]. On a broader scale, between-mode fluctuations may thus regulate the balance between feedforward versus feedback contributions to perception and thereby play a adaptive role in metacognition and reality monitoring[@Dijkstra2021].

## Arousal, attentional lapses, general response biases, insufficient training and metacognitive strategies as alternative explanations for between-mode fluctuations

These functional explanations for external and internal modes share the idea that, in order to form stable internal predictions about the statistical properties of the world (e.g., tracking the hazard rate of the environment) or metacognitive beliefs about processes occurring within the agent (e.g., monitoring ongoing changes in the reliability of feedback and feedforward processing), perception needs to temporarily disengage from internal predictions. By the same token, they presuppose that fluctuations in mode occur at the level of perceptual processing[@St.John-Saaltink2016; @Cicchini2017; @Braun2018; @Cicchini2021], and are not a passive phenomenon that is primarily driven by factors situated up- or downstream of sensory analysis. 

First, it may be argued that agents stereotypically repeat preceding choices when less alert. Our analyses address this alternative driver of serial dependencies by building on the association between RTs and arousal[@Rosenberg2013; @Prado2011]. We found that RTs do not map linearly onto the mode of sensory processing, but become shorter for stronger biases toward both externally- and internally-oriented mode (Figure 2G-H; Figure 4I). These observations argue against the view that biases toward internal mode can be explained solely on the ground of ongoing changes in tonic arousal or fatigue[@McGinley2015b]. 

However, internal modes of sensory processing may also be attributed to attentional lapses[@Andrillon2021], which are caused by mind-wandering or mind-blanking and show a more complex relation to RTs[@Andrillon2021]: While episodes of mind-blanking are characterized by an absence of subjective mental activity, more frequent misses, a relative increase in slow waves over posterior EEG electrodes and increased RTs, episodes of mind-wandering come along which rich inner experiences, more frequent false alarms, a relative increase of slow-wave amplitudes over frontal electrodes and decreased RTs[@Andrillon2021]. 

Yet in contrast to gradual between-mode fluctuations, engaging in mind-wandering as opposed to on-task attention seems to be an all-or-nothing phenomenon[@Andrillon2021]. In addition, internally-biased processing did not increase either false alarms or misses, but induced choice errors through an enhanced impact of perceptual history (Figure 2 and 4A) that unfolded in alternating *streaks*[@Gilden1995; @Gilden2001] of elevated stimulus- and history-congruence. Finally, the increase in lapse rates during internal mode was not general, but history-dependent (Figures 3 and 5). While these observations clearly distinguishes between-mode fluctuations from unspecific effects of lapses on decision-making, it remains an intriguing question for future research how mind-wandering and -blanking can be differentiated from internally-oriented modes of sensory processing in terms of their phenomenology, behavioral characteristics, neural signatures and noise profiles[@Gilden1995a; @Andrillon2021].

Second, it may be proposed that humans and mice apply a metacognitive response strategy that repeats preceding choices when less confident about their responses or when insufficiently trained on the task. In humans, however, confidence increased for stronger biases toward both external and internal mode (Figure 2I-J). For humans and mice, history-effects grew stronger with increasing exposure to (and expertise in) the task (Supplemental Figure S6). In addition, the existence of external and internal modes in murine perceptual decision-making (Figure 4) implies that between-mode fluctuations do not depend exclusively on the rich cognitive functions associated with human prefrontal cortex[@Passingham].

Third, our computational modeling results provide further evidence against both of the above caveats: Simulations based on estimated model parameters closely matched the empirical data (Figure 6), reproduced aspects of behavior it was not fitted to (such as trial-wise confidence reports and RTs/TD for human and mice, respectively), and predicted that history-congruent choices occur more frequently after high-confidence trials[@Braun2018; @Bergen2019]. These findings suggest that perceptual choices and post-perceptual processes such as response behavior or metacognition are jointly driven by a dynamic decision variable[@Kepecs2008] that encodes uncertainty[@Cicchini2014; @Urai2017; @Cicchini2018; @Braun2018, @Bergen2019] and is affected by ongoing changes in the integration of external versus internal information.

Of note, a recent computational study[@Ashwood2021] has used a Hidden Markov Model (HMM) to investigate perceptual decision-making in the IBL database[@Aguillon-Rodriguez2020]. In analogy to our findings, the authors observed that mice switch between temporally extended *strategies* that last for more than 100 trials: During *engaged* states, perception was highly sensitive to external sensory information. During *disengaged* states, in turn, choice behavior was prone to errors due to enhanced biases toward one of the two perceptual outcomes[@Ashwood2021]. Despite the conceptual differences to our approach (discrete states in a HMM that correspond to switches between distinct decision-making strategies[@Ashwood2021] vs. gradual changes in mode that emerge from sequential Bayesian inference and ongoing fluctuations in the impact of external relative to internal information), it is tempting to speculate that engaged/disengaged states and between-mode fluctuations might tap into the same underlying phenomenon. 

## Dopamine-dependent changes in E-I-balance as a neural mechanism of between-mode fluctuations 

The link to self-organized criticality suggests that balanced cortical excitation and inhibition[@Deneve2016] (E-I), which may enable efficient coding[@Deneve2016] by maintaining neural networks in critical states[@Beggs2003], could provide a potential neural mechanism of between-mode fluctuations. Previous work has proposed that the balance between glutamatergic excitation and GABA-ergic inhibition is regulated by activity-dependent feedback through NMDA receptors[@Wang1999]. Such NMDA-mediated feedback has been related to the integration of external inputs over time[@Deneve2016] (model component (i), Figure 1E), thereby generating serial dependencies in decision-making[@Wang2001; @Wang2013; @Bliss2017; @Stein2020]. Intriguingly, slow neuromodulation by dopamine enhances NMDA-dependent signaling[@Durstewitz2000; @Wang2001; @Seamans2001] and fluctuates at slow frequencies[@Kobayashi2017; @Chew2019] that match the temporal dynamics of between-mode fluctuations observed in humans (Figure 2) and mice (Figure 4). Ongoing fluctuations in the impact of external versus internal information (model component (ii)) may thus by caused by phasic changes in E-I-balance that are induced by dopaminergic neuromodulation.

## Limitations and open questions

In this study, we show that perception is attracted toward preceding choices in mice[@Aguillon-Rodriguez2020] (Figure 4A) and humans (Figure 2A; see Supplemental Figure S1 for analyses within individual studies of the Confidence database[@Rahnev2020]). Of note, previous work has shown that perceptual decision-making is concurrently affected by both attractive and repulsive serial biases that operate on distinct time-scales and serve complementary functions for sensory processing[@Fritsche2017; @Gekas2019; @Fritsche2020]: Short-term attraction may serve the decoding of noisy sensory inputs and increase the stability of perception, whereas long-term repulsion may enable efficient encoding and sensitivity to change[@Fritsche2020]. 

Importantly, repulsive biases operate in parallel to attractive biases[@Fritsche2020] and are therefore unlikely to account for the ongoing changes in mode that occur in alternating cycles of internally- and externally-oriented processing. To elucidate whether attraction and repulsion both fluctuate in their impact on perceptual decision-making will be an important task for future research, since this would help to understand whether attractive and repulsive biases are linked in terms of their computational function and neural implementation[@Fritsche2020].

A second open question concerns the neurobiological underpinnings of ongoing changes in mode. Albeit purely behavioral, our results tentatively suggest dopaminergic neuromodulation of NMDA-mediated feedback as one potential mechanism of externally- and internally-biased modes. Since between-mode fluctuations were found in both humans and mice, future studies can apply both non-invasive and invasive neuro-imaging and electrophysiology to better understand the neural mechanisms that generate ongoing changes in mode in terms of neuro-anatomy, -chemistry and -circuitry. 

Finally, establishing the neural correlates of externally- an internally-biased modes will enable exiting opportunities to investigate their role for adaptive perception and decision-making. Causal interventions via pharmacological challenges, optogenetic manipulations or (non-)invasive brain stimulation will help to understand whether between-mode fluctuations are implicated in resolving credit-assignment problems[@Honey2017; @Weilnhammer2020] or in calibrating metacognition and reality monitoring[@Dijkstra2021]. Addressing these questions may therefore provide new insight into the pathophysiology of hallucinations and delusions, which have been characterized by an imbalance in the impact of external versus internal information[@Fletcher2009; @Sterzer2018; @Corlett2019a] and are typically associated with metacognitive failures and a departure from consensual reality[@Corlett2019a]. 

<!-- \newpage -->
<!-- # Acknowledgements -->

<!-- The authors would like to thank XYZ for helpfull discussions on this manuscript.  -->

\newpage
# Methods

## Ressource availability

### Lead contact

Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Veith Weilnhammer (veith.weilnhammer@gmail.com).

### Materials availability

This study did not generate new unique reagents.

### Data and code availability

All custom code and behavioral data are available on <https://github.com/veithweilnhammer/Modes>. This manuscript was created using the *R Markdown* framework, which integrates all data-related computations and the formatted text within one document. With this, we wish to make our approach fully transparent and reproducible for reviewers and future readers.

## Experimental model and subject details

### Confidence database
We downloaded the human data from the Confidence database[@Rahnev2020] on 21/10/2020, limiting our analyses to the database category *perception*. Within this category, we selected studies in which participants made binary perceptual decision between two alternative outcomes (see Supplemental Table 1). We excluded two studies in which the average perceptual accuracy fell below 50%. After excluding these studies, our sample consisted of $`r n_trials_human/100000`$ million trials obtained from `r n_participants_human` human participants and `r n_studies_human` individual studies. 

### IBL database
We downloaded the murine data from the IBL database[@Aguillon-Rodriguez2020] on 28/04/2021. We limited our analyses to the *basic task*, during which mice responded to gratings that appeared with equal probability in the left or right hemifield. Within each mouse, we excluded sessions in which perceptual accuracy was below 80% for stimuli presented at a contrast $\geq$ 50%. After exclusion, our sample consisted of $`r n_trials_mice/1000000`$ million trials trials obtained from N = `r n_participants_mice` mice.

## Method details

### Variables of interest

**Primary variables of interest:** We extracted trial-wise data on the presented stimulus and the associated perceptual decision. Stimulus-congruent choices were defined by perceptual decisions that matched the presented stimuli. History-congruent choices were defined by perceptual choices that matched the perceptual choice at the immediately preceding trial. The dynamic probabilities of stimulus- and history-congruence were computed in sliding windows of ±`r sliding_window/2` trials.

The *mode* of sensory processing was derived by subtracting the dynamic probability of history-congruence from the dynamic probability of stimulus-congruence, such that positive values indicate externally-oriented processing, whereas negative values indicate internally-oriented processing. When visualizing the relation of the mode of sensory processing to confidence, response times or trial duration (see below), we binned the mode variable in 10% intervals. We excluded bins than contained less than 0.5% of the total number of available data-points.

**Secondary variables of interest**: From the Confidence Database[@Rahnev2020], we furthermore extracted trial-wise confidence reports and response times (RTs; if RTs were available for both the perceptual decision and the confidence report, we only extracted the RT associated with the perceptual decision). To enable comparability between studies, we normalized RTs and confidence reports within individual studies using the *scale* R function. If not available for a particular study, RTs and confidence reports were treated as missing variables.
From the IBL database[@Aguillon-Rodriguez2020], we extracted trial durations (TDs) as defined by interval between stimulus onset and feedback, which represents a coarse measure of RT[@Aguillon-Rodriguez2020]. 

**Exclusion criteria for individual data-points:** For non-normalized data (TDs from the IBL database[@Aguillon-Rodriguez2020]; d-prime, meta-dprime and M-ratio from the Confidence database[@Rahnev2020] and simulated confidence reports), we excluded data-points that differed from the median by more than 3 x MAD (median absolute distance[@Leys2013]). For normalized data (RTs and confidence reports from the Confidence database[@Rahnev2020]), we excluded data-points that differed from the mean by more than 3 x SD (standard deviation). 

### Control variables

Next to the sequence of presented stimuli, we assessed the autocorrelation of task difficulty as an alternative explanation for any autocorrelation in stimulus- and history-congruence. For the Confidence Database[@Rahnev2020], task difficulty was indicated by one of the following labels: *Difficulty*, *Difference*, *Signal-to-Noise*, *Dot-Difference*, *Congruency*, *Coherence(-Level)*, *Dot-Proportion*, *Contrast(-Difference)*, *Validity*, *Setsize*, *Noise-Level(-Degree)* or *Temporal Distance*. When none of the above was available for a given study, task difficulty was treated as a missing variable. In analogy to RTs and confidence, difficulty levels were normalized within individual studies. For the IBL Database[@Aguillon-Rodriguez2020], task difficulty was defined by the contrast of the presented grating.

### Autocorrelations

For each participant, trial-wise autocorrelation coefficients were estimated using the R-function *acf* with a maximum lag defined by the number of trials available per subject. Autocorrelation coefficients are displayed against the lag (in numbers of trials, ranging from 1 to 20) relative to the index trial (t = 0, see Figure 2B-C, 4B-C and 6B-C). To account for spurious autocorrelations that occur due to imbalances in the analyzed variables, we estimated autocorrelations for randomly permuted data (100 iterations). For group-level autocorrelations, we computed the differences between the true autocorrelation coefficients and the mean autocorrelation observed for randomly permuted data and averaged across participants. 

At a given trial, group-level autocorrelation coefficients were considered significant when linear mixed effects modeling indicated that the difference between real and permuted autocorrelation coefficients was above zero at an alpha level of 0.05%. To test whether the autocorrelation of stimulus- and history-congruence remained significant when controlling for task difficulty and the sequence of presented stimuli, we added the respective autocorrelation as an additional factor to the linear mixed effects model that computed the group-level statistics (see also *Mixed effects modeling*).

To assess autocorrelations at the level of individual participants, we counted the number of subsequent trials (starting at the first trial after the index trial) for which less than 50% of the  permuted autocorrelation coefficients exceeded the true autocorrelation coefficient. For example, a count of zero indicates that the true autocorrelation coefficients exceeded *less than 50%* of the autocorrelation coefficients computed for randomly permuted data at the first trial following the index trial. A count of five indicates that, for the first five trials following the index trial, the true autocorrelation coefficients exceeded *more than 50%* of the respective autocorrelation coefficients for the randomly permuted data; at the 6th trial following the index trial, however, *less than 50%* of the autocorrelation coefficients exceeded the respective permuted autocorrelation coefficients. 

### Spectral analysis

We used the R function *spectrum* to compute the spectral densities for the dynamic probabilities of stimulus- and history-congruence as well as the phase (i.e., frequency-specific shift between the two time-series ranging from 0 to $2*\pi$) and squared coherence (frequency-specific variable that denotes the degree to which the shift between the two time-series in constant, ranging from 0 to 100%). Periodograms were smoothed using modified Daniell smoothers at a width of 50. 

Since the dynamic probabilities of history- and stimulus-congruence were computed using a sliding windows of ±`r sliding_window/2` trials (i.e., intervals containing a total of `r sliding_window + 1` trials), we report the spectral density, coherence and phase for frequencies below  1/`r sliding_window + 1` 1/$N_{trials}$. Spectral densities have one value per subject and frequency (data shown in Figures 2D and 4D). To assess the relation between stimulus- and history-congruence in this frequency range, we report average phase and average squared coherence for all frequencies below 1/`r sliding_window + 1` 1/$N_{trials}$ (i.e., one value per subject; data shown in Figure 2E-F and 4E-F).

Since the data extracted from the Confidence Database[@Rahnev2020] consist of a large set of individual studies that differ with respect to inter-trial intervals, we defined the variable *frequency* in the dimension of cycles per trial 1/$N_{trials}$ rather than cycles per second (Hz). For consistency, we chose 1/$N_{trials}$ as the unit of frequency for the IBL database[@Aguillon-Rodriguez2020] as well.

## Quantification and statistical procedures

All aggregate data are reported and displayed with errorbars as mean ± standard error of the mean. 

### Mixed effects modeling

Unless indicated otherwise, we performed group-level inference using the R-packages *lmer* and *afex* for linear mixed effects modeling and *glmer* with a binomial link-function for logistic regression. We compared models based on Akaike Information Criteria (AIC). To account for variability between the studies available from the Confidence Database[@Rahnev2020], mixed modeling was conducted using random intercepts defined for each study. To account for variability across experimental session within the IBL database[@Aguillon-Rodriguez2020], mixed modeling was conducted using random intercepts defined for each individual session. When multiple within-participant datapoints were analyzed, we estimated random intercepts for each participant that were *nested* within the respective study of the Confidence database[@Rahnev2020]. By analogy, for the IBL database[@Aguillon-Rodriguez2020], we estimated random intercepts for each session that were nested within the respective mouse. We report $\beta$ values referring to the estimates provided by mixed effects modeling, followed by the respective T statistic (linear models) or z statistic (logistic models).  

The effects of stimulus- and history-congruence on RTs and confidence reports (Figure 2, 4 and 6, subpanels G-I) were assessed in linear mixed effects models that tested for main effects of both stimulus- and history-congruence as well as the between-factor interaction. Thus, the significance of any effect of history-congruence on RTs and confidence reports was assessed while controlling for the respective effect of stimulus-congruence (and vice versa).

### Psychometric function

We obtained psychometric curves by fitting the following error function to the behavioral data:

\begin{equation}
y_p = \gamma + (1 - \gamma - \delta) *  (erf(\frac{s_w + \mu}{t}) + 1) / 2
\end{equation}

We used the Broyden–Fletcher–Goldfarb–Shanno algorithm in maximum likelihood estimation[@Nash2011] to predict individual choices $y$ (outcome A: $y = 0$; outcome B: $y = 1$) from the choice probability $y_p$. In humans, we computed $s_w$ multiplying the inputs $s$ (stimulus A: 0; outcome B: 1) with the task difficulty $D_b$ (binarized across 7 levels):

\begin{equation}
s_w = (s - 0.5) * D_b
\end{equation}

In mice, $s_w$ was defined by the respective stimulus contrast in the two hemifields:

\begin{equation}
s_w = Contrast_{Right} - Contrast_{Left}
\end{equation}

Parameters of the psychometric error function were fitted using the R-package *optimx*[@Nash2011]. The psychometric error function was defined via the parameters $\gamma$ (lower lapse; lower bound = 0, upper bound = 0.5), $\delta$ (upper lapse; lower bound = 0, upper bound = 0.5), $\mu$ (bias; lower bound humans = -5; upper bound humans = 5, lower bound mice = -0.5, upper bound mice = 0.5) and threshold $t$ (lower bound humans = 0.5, upper bound humans = 25; lower bound mice = 0.01, upper bound mice = 1.5).

### Computational modeling

**Model definition**: Our modeling analysis is an extension of a model proposed by Glaze et al.[@Glaze2015], who defined a normative account of evidence accumulation for decision-making. In this model, trial-wise choices are explained by applying Bayes theorem to infer moment-by-moment changes in the state of environment from trial-wise noisy observations across trials.

Following Glaze et al.[@Glaze2015], we applied Bayes rule to compute the posterior evidence for the two alternative choices (i.e., the log posterior ratio $L$) from the sensory evidence available at time-point $t$ (i.e., the log likelihood ratio $LLR$) with the prior probability $\psi$, **weighted by the respective precision terms $\omega_{LLR}$ and $\omega_{\psi}$**:

\begin{equation}
L_t = LLR_t * \omega_{LLR} + \psi_t(L_{t-1}, H) * \omega_{\psi}
\end{equation}

In the trial-wise design studied here, a transition between the two states of the environment (i.e., the sources generating the noisy observations available to the participant) can occur at any time. Despite the random nature of the psychophysical paradigms studied here[@Rahnev2020; @Aguillon-Rodriguez2020], humans and mice showed significant biases toward preceding choices (Figure 2A and 4A). We thus assumed that the prior probability of the two possible outcomes depends on the posterior choice probability at the preceding trial and the hazard rate $H$ assumed by the participant. Following Glaze et al.[@Glaze2015], the prior $\psi$ is thus computed as follows: 

\begin{equation}
\psi_t(L_{t-1}, H)  = L_{t-1} + log(\frac{1-H}{H} + exp(-L_{t-1})) - log(\frac{1-H}{H} + exp(L_{t-1}))
\end{equation}

In this model, humans, mice and simulated agents make perceptual choices based on noisy observations $u$. The are computed by applying a sensitivity parameter $\alpha$ to the content of external sensory information $s$. For humans, we defined the input $s$ by the two alternative states of the environment (stimulus A: $s = 0$; stimulus B: $s = 1$), which generated the observations $u$ through a sigmoid function that applied a sensitivity parameter $\alpha$:

\begin{equation}
u_t = \frac{1}{1+exp(-\alpha*(s_t-0.5))}
\end{equation}

In mice, the inputs $s$ were defined by the respective stimulus contrast in the two hemifields:

\begin{equation}
s_t = Contrast_{Right} - Contrast_{Left}
\end{equation}

As in humans, we derived the input $u$ by applying a sigmoid function with a sensitivity parameter $\alpha$ to input $s$: 

\begin{equation}
u_t = \frac{1}{1 + exp(-\alpha * s_t)}
\end{equation}

For humans, mice and in simulations, the log likelihood ratio $LLR$ was computed from $u$ as follows: 

\begin{equation}
LLR_t = log(\frac{u_t}{1-u_t})
\end{equation}

To allow for long-range autocorrelation in stimulus- and history-congruence (Figure 2B and 4B), our modeling approach differed from Glaze et al.[@Glaze2015] in that it allowed for systematic fluctuation in the impact of sensory information (i.e., $LLR$) and the prior probability of choices $\psi$ on the posterior probability $L$. This was achieved by multiplying the log likelihood ratio and the log prior ratio with coherent anti-phase fluctuations according to $\omega_{LLR} = a_{LLR} * sin(f * t + phase) + 1$ and $\omega_{\psi} = a_{\psi} * sin(f * t + phase + \pi) + 1$.

**Model fitting**: In model fitting, we predicted the trial-wise choices $y_t$ (option A: 0; option B: 1) from inputs $s$. To this end, we minimized the log loss between $y_t$ and the choice probability $y_{p t}$ in the unit interval. $y_{p t}$ was derived from $L_t$ using a sigmoid function defined by the inverse decision temperature $\zeta$: 

\begin{equation}
y_{p t} = \frac{1}{1 + exp(-\zeta * L_t)}
\end{equation}

This allowed us to infer the free parameters $H$ (lower bound = 0, upper bound = 1; human posterior = `r mean(Optim_Behav$Hazard, na.rm = TRUE)` ± `r sd(Optim_Behav$Hazard, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$Hazard, na.rm = TRUE)` ± `r sd(Optim_M_Behav$Hazard, na.rm = TRUE)/nrow(Optim_M_Behav)`),
$\alpha$ (lower bound = 0, upper bound = 5; human posterior = `r mean(Optim_Behav$Precision, na.rm = TRUE)` ± `r sd(Optim_Behav$Precision, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$Precision, na.rm = TRUE)` ± `r sd(Optim_M_Behav$Precision, na.rm = TRUE)/nrow(Optim_M_Behav)`), 
$a_{\psi}$ (lower bound = 0, upper bound = 10; human posterior = `r mean(Optim_Behav$amp_prior, na.rm = TRUE)` ± `r sd(Optim_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$amp_prior, na.rm = TRUE)` ± `r sd(Optim_M_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_M_Behav)`), $amp_{LLR}$ (lower bound = 0, upper bound = 10; human posterior = `r mean(Optim_Behav$amp_LLR, na.rm = TRUE)` ± `r sd(Optim_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$amp_LLR, na.rm = TRUE)` ± `r sd(Optim_M_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_M_Behav)`),
frequency $f$ (lower bound = 1/40, upper bound = 1/5; human posterior = `r mean(Optim_Behav$frequency, na.rm = TRUE)` ± `r sd(Optim_Behav$frequency, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$frequency, na.rm = TRUE)` ± `r sd(Optim_M_Behav$frequency, na.rm = TRUE)/nrow(Optim_M_Behav)`), 
$p$ (lower bound = 0, upper bound = $2*\pi$; human posterior = `r mean(Optim_Behav$phase, na.rm = TRUE)` ± `r sd(Optim_Behav$phase, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$phase, na.rm = TRUE)` ± `r sd(Optim_M_Behav$phase, na.rm = TRUE)/nrow(Optim_M_Behav)`) 
and inverse decision temperature $\zeta$ (lower bound = 1, upper bound = 10; human posterior = `r mean(Optim_Behav$zeta, na.rm = TRUE)` ± `r sd(Optim_Behav$zeta, na.rm = TRUE)/nrow(Optim_Behav)`; murine posterior = `r mean(Optim_M_Behav$zeta, na.rm = TRUE)` ± `r sd(Optim_M_Behav$zeta, na.rm = TRUE)/nrow(Optim_M_Behav)`) using maximum likelihood estimation with the Broyden–Fletcher–Goldfarb–Shanno algorithm as implemented in the R-function *optimx*[@Nash2011] **(see Supplemental Table T2 for a description of our model parameters)**.

To validate our model, we correlated individual posterior parameter estimates with the respective conventional variables. We assumed that, (i), the estimated hazard rate $H$ should correlate negatively with the frequency of history-congruent choices and that, (ii), the estimated $\alpha$ should correlate positively with the frequency of stimulus-congruent choices. In addition, we tested whether the posterior decision certainty (i..e. the absolute of the posterior log ratio) correlated negatively with RTs and positively with subjective confidence. This allowed us to assess whether our model could explain aspects of the data it was not fitted to (i.e., RTs and confidence). Finally, we used simulations (see below) to show that all model components, including the anti-phase oscillations governed by $a_{\psi}$, $a_{LLR}$, $f$ and $p$, were necessary for our model to reproduce the empirical data observed for the Confidence database[@Rahnev2020] and IBL database[@Aguillon-Rodriguez2020]. 

**Model simulation 1: Data recovery**: We used the posterior model parameters observed for humans ($H$, $\alpha$, $a_{\psi}$, $a_{LLR}$ and $f$) to define individual parameters for simulation in `r nrow(Optim_Behav)` simulated participants (i.e., equivalent to the number of human participants). For each participant, the number of simulated choices was drawn from a uniform distribution ranging from 300 to 700 trials. Inputs $s$ were drawn at random for each trial, such that the sequence of inputs to the simulation did not contain any systematic seriality. Noisy observations $u$ were generated by applying the posterior parameter $\alpha$ to inputs $s$, thus generating stimulus-congruent choices in `r mean(Sim_Behav$Accuracy, na.rm = TRUE)` ± `r sd(Sim_Behav$Accuracy, na.rm = TRUE)/nrow(Sim_Behav)`% of trials. Choices were simulated based on the trial-wise choice probabilities $y_{p}$. Simulated data were analyzed in analogy to the human and murine data. As a substitute of subjective confidence, we computed the absolute of the trial-wise posterior log ratio $|L|$ (i.e., the posterior decision certainty).

**Model simulation 2: Testing the adaptive benefits of bimodal inference**: In contrast to the model applied to the behavioral data, our second set of simulations considered a situation in which agents learn about the properties of the environment from experience. We modeled dynamic updates in the trial-wise estimates $H_t$ about the true hazard rate $\hat{H} = P(s_t \neq s_{t-1})$ and trial-wise estimates $M_t$ about the precision of sensory encoding $\hat{M} = 1 - (|s_t-u_t|)$.

In the absence of feedback, leaning about $\hat{H}$ was driven by the error-term $\epsilon_H$, which reflected the difference between the currently assumed hazard rate $H_t$ and the presence of a *perceived* change in the environment $|y_t - y_{t-1}|$: 

\begin{equation}
\epsilon_H = |y_t - y_{t-1}| - H_t
\end{equation}

In the presence of feedback, $\epsilon_H$ reflected the difference between the currently assumed hazard rate $H_t$ and an presence of a *true* change in the environment $|s_t - s_{t-1}|$:

\begin{equation}
\epsilon_H = |s_t - s_{t-1}| - H_t
\end{equation}

In the absence of feedback, learning about $\hat{M}$ was driven by the error-term $\epsilon_M$, reflecting the difference between $M_t$ and the posterior decision-certainty $(1-|y_t - P(y_t = 1)|)$: 

\begin{equation}
\epsilon_M = (1-|y_t - P(y_t = 1)|) - M_t
\end{equation} 

In the presence of feedback, $\epsilon_M$ reflected the difference between $M_t$ and the stimulus-congruence of the current response $(1- (|y_t - s_t|))$:

\begin{equation}
\epsilon_M = (1- (|y_t - s_t|)) - M_t
\end{equation} 

Updates to $H$ and $M$ were computed in logit-space using a Rescorla-Wagner-rule with learning rates defined by the product of $\beta_{H/M}$ and $\omega_{LLR}$. $H_t$ and $M_t$ are computed by transforming $H'_t$ and $M'_t$ into the unit interval using a sigmoid function:

\begin{equation}
H'_t = H'_{t-1} + \beta_H *\omega_{LLR} * \epsilon_H
\end{equation}  

\begin{equation}
H_t = \frac{1}{1+exp(-(H'_t))}
\end{equation}

\begin{equation}
M'_t = M'_{t-1} + \beta_M *\omega_{LLR} * \epsilon_M
\end{equation}  

\begin{equation}
M_t = \frac{1}{1+exp(-(M'_t))}
\end{equation}

We simulated data for a total of $`r (length(unique(Summary_Sim_adaptive$subject_id)))`$ participants for a total of $`r n_blocks`$ blocks of $`r mean(block_length)`$ trials each. 
Each block differed with respect to the true hazard rate $\hat{H}$ (either 0.1, 0.3, 0.5, 0.7 or 0.9) and the sensitivity parameter $\alpha$ (either 2, 3, 4, 5 or 6, corresponding to values of $\hat{M}$ of 0.73, 0.82, 0.88, 0.92 or 0.95).
Across participants, model parameters were set as follows: $H'_1$ initialized at random in a unit interval between $`r min(H_logit)`$ to $`r max(H_logit)`$; $P'_1$ initialized at random in a unit interval between $`r min(P_logit)`$ to $`r max(P_logit)`$; $a$ = 1; $f$ between 0.05 and 0.15 1/$N_{trials}$; $\zeta$ = 1; $\beta_H$ and $\beta_M$ between 0.05 and 0.25. For each participant, we ran separate simulations with external feedback provided in 0%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% and 100% of trials.

\newpage
# Figures

## Figure 1
```{r Figure_1}
##
## Timecourse plot
##
Individual_TC_long <- read.csv("./Results/Individual_TC_long.csv")
  
p_Individual_TC_0 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Input" | Individual_TC_long$Slider == "Response",],
aes(x = Trial, y = Variable, linetype = Slider),
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"Congruency", linetype = "", subtitle = "A") + xlim(0, 300) +
theme(legend.position = "none") + scale_y_continuous(name="Stimulus", breaks = c(0,1), labels = c("B", "A"), limits=c(-0.1, 1.1))

p_Individual_TC_1 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Stimulus" | Individual_TC_long$Slider == "History",],
aes(x = Trial, y = Variable, color = Slider),
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"Congruency", color = "Congruence to", subtitle = "B") + xlim(0, 300) +
theme(legend.position = "none") + scale_y_continuous(name="Congruent", breaks = c(0,1), labels = c("no", "yes"), limits=c(-0.1, 1.1))


p_Individual_TC_2 <- ggplot() +
geom_line(
data = Individual_TC_long[Individual_TC_long$Slider == "Stimulus_slider" | Individual_TC_long$Slider == "History_slider",],
aes(x = Trial, y = Variable, color = Slider),
size = 0.5,
 alpha = 0.5
) + geom_hline(
yintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"z(Probability)", subtitle = "C") + xlim(0, 300) +
theme(legend.position = "none")  + scale_y_continuous(name="Probability")

p_Individual_TC_3 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Mode",],
aes(x = Trial, y = Variable, color = Slider),
color = "darkorchid4",
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Set1", direction = 1) +
theme_classic(base_size = 6) + labs(x = "Trial",  y =
"Int.< Mode (%) > Ext.", subtitle = "D") + xlim(0, 300)  +
theme(legend.position = "none") +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

p_Individual_TC_4 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Prior" | Individual_TC_long$Slider == "LLR" | Individual_TC_long$Slider == "Posterior",],
aes(x = Trial, y = Variable, color = Slider),
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Dark2", direction = 1) +
theme_classic(base_size = 6) + labs(x = "Trial",  y =
"Log ratio", subtitle = "E", color = " ") + xlim(0, 300)  +
theme(legend.position = "none") +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

lay <- rbind(c(1), c(2), c(3), c(4), c(5))
grid.arrange(
p_Individual_TC_0,
p_Individual_TC_1,
p_Individual_TC_2,
p_Individual_TC_3,
p_Individual_TC_4,
layout_matrix = lay,
heights = c(0.75,0.75,1,1,1),
widths = c(1))
```

**Figure 1. Concept.** 

A. In binary perceptual decision-making, a participant is presented with stimuli from two categories (A vs. B; dotted line) and reports consecutive perceptual choices via button presses (sold line). All panels below refer to this example data. 

B. When the response matched the external stimulus information (i.e., overlap between dotted and solid line in panel A), perceptual choices are *stimulus-congruent* (red line). When the response matches the response at the preceding trial, perceptual choices are *history-congruent* (blue line).

C. The dynamic probabilities of stimulus- and history-congruence (i.e., computed in sliding windows of ±`r sliding_window/2` trials) fluctuate over time. 

D. The *mode* of perceptual processing is derived by computing the difference between the dynamic probabilities of stimulus- and history-congruence. Values above 0% indicate a bias toward external information, whereas values below 0% indicate a bias toward internal information. 

E. In computational modeling, internal mode is caused by an enhanced impact of perceptual history. This causes the posterior (black line) to be close to the prior (blue line). Conversely, during external mode, the posterior is close to the sensory information (log likelihood ratio, red line).

\newpage
## Figure 2

```{r Figure_2}
##
## Global probability Stimulus- and History-congruence
##


gathercol <- colnames(Behav[,c(3,6)])

Behav_long  <-
gather(Behav[, c(1,2,3,6)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

Behav_long$Frequency <-
gsub("Accuracy", "Stimulus", Behav_long$Frequency)

p_distribution_Behav <- ggplot(Behav_long, aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
   # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
   geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
    geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

##
## Probability of History-congruence depending on perceptual accuracy
##


gathercol <- colnames(Behav_diff[,c(4)])

Behav_diff_long  <-
gather(Behav_diff[, c(1,2,3,4)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

Behav_diff_long$Accuracy <- as.character(Behav_diff_long$Accuracy) 
Behav_diff_long$Accuracy <-
gsub("0", "error", Behav_diff_long$Accuracy)
Behav_diff_long$Accuracy <-
gsub("1", "correct", Behav_diff_long$Accuracy)

p_distribution_Behav_diff <- ggplot(Behav_diff_long, aes(x = History, color = -Accuracy, fill = Accuracy)) + 
   # geom_histogram(aes(y=..density..), colour="white", fill="black", position="ide ntity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
   geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
    geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "top")

##
## Acf Plot
##
Plot_Acf_Data =  Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus" | Summary_acf$Variable == "diff_acf_History" ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

y_min = -0.002
p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = y_min,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
   
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocorrelation coefficient"), subtitle = "B", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25)) + ylim(y_min, 0.035)   


##
## Exceed Plot
##


gathercol <- colnames(ID_Summary_acf[,c(3,4)])

ID_Summary_acf_long  <-
gather(ID_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

p_exceed_acf <- ggplot(ID_Summary_acf_long, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + ylim(0,0.35) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~
Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())


##
## Power Spectra
##

p_power_frequency <-
  ggplot(data = Power_Frequency[Power_Frequency$r_freq > 0.01 & Power_Frequency$r_freq < 0.09,],
    mapping = aes(x = (r_freq), y = (mean_power), ymin = mean_power - ci_power, ymax = mean_power - ci_power, color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.5, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.4) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency (1/Trials)",  y =
"Spectral Density", subtitle = "D")

p_density_phase <-
ggplot(Power_Spectra[Power_Spectra$r_freq > 0.01 & Power_Spectra$r_freq < 0.09,], aes(
x = abs(Phase)
))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Phase",  y =
"Density", subtitle = "E") + theme_classic(base_size = 6) + theme(legend.position =
"none") +
geom_vline(xintercept = pi, linetype = "dashed",
    color = "black",
    size = 0.25) + xlim(-0.2, 3.4)

p_density_coherence <-
ggplot(Power_Spectra[Power_Spectra$bin_freq < 0.09,], aes(
x = Coherence
))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.1) + labs(x = "Squared coherence (%)",  y =
"Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position =
"none") + geom_vline(xintercept = mean(Power_Spectra[Power_Frequency$r_freq > 0.01 & Power_Frequency$r_freq <= 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",
    color = "black",
    size = 0.25) + xlim(-1, 26)

p_RT_mode <- ggplot(data = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_RT,
      ymin = average_RT - se_RT,
      ymax = average_RT + se_RT)) +
  geom_point(
    alpha = 0.8,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"z(RT)", subtitle = "H")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = max(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5 & Post_Perceptual_Modes$average_RT == max(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

p_Confidence_mode <- ggplot(data = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_Confidence,
      ymin = average_Confidence - se_Confidence,
      ymax = average_Confidence + se_Confidence)) +
  geom_point(
    alpha = 0.5,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"z(Confidence)", subtitle = "J")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = min(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5 & Post_Perceptual_Modes$average_Confidence == min(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )


p_RT <- ggplot() +
  geom_point(data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "RT",],
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Summary_RT_Confidence_Behav[ Summary_RT_Confidence_Behav$Type == "RT",],
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff RT (zscore)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.25, 0.05)

p_Confidence <- ggplot() +
  geom_point(data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "Confidence",],
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Summary_RT_Confidence_Behav[ Summary_RT_Confidence_Behav$Type == "Confidence",],
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff Confidence (zscore)", subtitle = "I") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.25, 1)

lay <- rbind(c(1,2,2,3), c(9,2,2,3), c(4,5, 10,7), c(4,6,11,8))
grid.arrange(
p_distribution_Behav, p_acf, p_exceed_acf,
p_power_frequency, p_density_phase, p_density_coherence,
p_RT_mode, p_Confidence_mode, p_distribution_Behav_diff, p_RT, p_Confidence,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Figure 2. Internal and external modes in human perceptual decision-making.** 

A. In humans, perception was stimulus-congruent in `r mean(Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Accuracy, na.rm = TRUE)/sqrt(length(Behav$Accuracy))`% (in red) and history-congruent in `r mean(Behav$History, na.rm = TRUE)`% ± `r sd(Behav$History, na.rm = TRUE)/sqrt(length(Behav$History))`% of trials (in blue; upper panel). History-congruent perceptual choices were more frequent when perception was stimulus-incongruent (i.e., on *error* trials; lower panel), indicating that history effects impair performance in randomized psychophysical designs. 

B. Relative to randomly permuted data, we found highly significant autocorrelations of stimulus-congruence and history-congruence (dots indicate intercepts $\neq$ 0 in trial-wise linear mixed effects modeling at p < 0.05). Across trials, the autocorrelation coefficients were best fit by an exponential function (adjusted $R^2$ for stimulus-congruence: `r Stimulus_exponential$adj.r.squared`; history-congruence: `r History_exponential$adj.r.squared`) as compared to a linear function (adjusted $R^2$ for stimulus-congruence: `r Stimulus_linear$adj.r.squared`; history-congruence: `r History_linear$adj.r.squared`). 

C. Here, we depict the number of consecutive trials at which autocorrelation coefficients exceeded the respective autocorrelation of randomly permuted data within individual participants. For stimulus-congruence (upper panel), the lag of positive autocorrelation amounted to `r mean(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_Summary_acf)` on average, showing a peak at trial t+1 after the index trial. For history-congruence (lower panel), the lag of positive autocorrelation amounted to `r mean(ID_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_Summary_acf)` on average, peaking at trial t+2 after the index trial.

D. The smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**, i.e., at power densities that were inversely proportional to the frequency. 

E. The distribution of phase shift between fluctuations in stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). 

F. The average squared coherence between fluctuations in stimulus- and history-congruence (black dottet line) amounted to `r mean(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)` ± `r sd(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(Summary_Power_Spectra$mean_coherence)`%

G. We observed faster response times (RTs) for both stimulus-congruence (as opposed to stimulus-incongruence, $\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$) 
and history-congruence ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$).

H. The mode of perceptual processing (i.e., the difference between the smoothed probability of stimulus- vs. history-congruence) showed a quadratic relationship to RTs, with faster response times for stronger biases toward both external sensory information and internal predictions provided by perceptual history ($\beta_2$ = $`r STAT.RT_vs_mode$coefficients[3,1]`$ ± $`r STAT.RT_vs_mode$coefficients[3,2]`$, T($`r STAT.RT_vs_mode$coefficients[3,3]`$) = $`r STAT.RT_vs_mode$coefficients[3,4]`$, p = $`r STAT.RT_vs_mode$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate maximum RT and the associated mode, respectively.

I. Confidence was enhanced for both stimulus-congruence (as opposed to stimulus-incongruence, $\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$) 
and history-congruence ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$). 

J. In analogy to RTs, we found a quadratic relationship between the mode of perceptual processing and confidence, which increased when both externally- and internally-biased modes grew stronger ($\beta_2$ = $`r STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum confidence and the associated mode, respectively.

\newpage
## Figure 3

```{r Figure_3}
p_pm_estimates_lower_lapse <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("lower_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.06,
    size = 0.5
  ) + 
  labs(x = "Lower lapses",  y =
         "Density", linetype = "", subtitle = "C") +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_upper_lapse <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("higher_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.06,
    size = 0.5
  )  + 
  labs(x = "Higher lapses",  y =
         "Density", linetype = "", subtitle = "D") +
  theme_classic(base_size = 6) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)


p_pm_estimates_bias <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("bias"), ], aes(x = -Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.25,
    size = 0.5
  )  + 
  labs(x = "Bias",  y =
         "Density", linetype = "", subtitle = "B") +
  theme_classic(base_size = 6) + 
  xlim(-3, 3) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_threshold <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("threshold"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.6,
    size = 0.15
  ) + 
  labs(x = "Threshold",  y =
         "Density", linetype = "", subtitle = "E") +
  theme_classic(base_size = 6) + xlim(0, 10) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_simulation <-
  ggplot(data = Sim_PM_group, 
         aes(x = Stimulus, y = mean_y_prob, ymax = mean_y_prob + error_y_prob, ymin = mean_y_prob - error_y_prob, color = Type, fill = Type)) +
  geom_ribbon(
    color = "white",
    alpha = 0.25,
    size = 0.1
  ) +  geom_line(size = 0.1) +
  labs(x = "Stimulus",  y =
         "P (y = 1)", linetype = "", subtitle = "A") +
  theme_classic(base_size = 6) +
  xlim(-5, 5) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "left") + facet_wrap( ~ Conditioned, ncol = 1) 

lay <- rbind(c(5, 1, 2, 3, 4))
grid.arrange(
  p_pm_estimates_bias,
  p_pm_estimates_lower_lapse,
  p_pm_estimates_upper_lapse,
  p_pm_estimates_threshold,
  p_pm_simulation,
  layout_matrix = lay,
  heights = c(1),
  widths = c(3, 1, 1, 1, 1)
)




```

**Figure 3. Full and history-conditioned psychometric functions across modes in humans.** 
 
A. Here, we show average psychometric functions for the full dataset (upper panel) and conditioned on perceptual history ($y_{t-1} = 1$ and $y_{t-1} = 0$; middle and lower panel) across modes (green line) and for internal mode (blue line) and external mode (red line) separately. 

B. Across the full dataset, biases $\mu$ were distributed around zero ($\beta_0$ = $`r STAT.PM_zero_bias_b$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_b$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_b$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_b$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_b$coefficients[1,5]`$; upper panel), 
with larger absolute biases $|\mu|$ for internal as compared to external mode ($\beta_0$ = $`r STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r STAT.PM_diff_bias$coefficients[1,2]`$, T($`r STAT.PM_diff_bias$coefficients[1,3]`$) = $`r STAT.PM_diff_bias$coefficients[1,4]`$, p = $`r STAT.PM_diff_bias$coefficients[1,5]`$; controlling for differences in lapses and thresholds).
When conditioned on perceptual history, we observed negative biases for $y_{t-1} = 0$ ($\beta_0$ = $`r STAT.PM_zero_bias_0$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_0$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_0$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_0$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_0$coefficients[1,5]`$; middle panel) 
and positive biases for $y_{t-1} = 1$ ($\beta_0$ = $`r STAT.PM_zero_bias_1$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_1$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_1$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_1$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_1$coefficients[1,5]`$; lower panel).

C. Lapse rates were higher in internal mode as compared to external mode ($\beta_0$ = $`r STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse$coefficients[1,5]`$; controlling for differences in biases and thresholds; see upper panel and subplot D). 
Importantly, the between-mode difference in lapses depended on perceptual history: We found no significant difference in lower lapses $\gamma$ for $y_{t-1} = 0$ ($\beta_0$ = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,5]`$; middle panel), 
but a significant difference for $y_{t-1} = 1$ ($\beta_0$ = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,5]`$; lower panel).

D. Conversely, higher lapses $\delta$ were significantly increased for $y_{t-1} = 0$ ($\beta_0$ = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,5]`$; middle panel), 
but not for $y_{t-1} = 1$ ($\beta_0$ = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,5]`$; lower panel).

E. The thresholds $t$ were larger in internal as compared to external mode ($\beta_0$ = $`r STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold$coefficients[1,2]`$, T($`r STAT.PM_diff_threshold$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold$coefficients[1,5]`$; controlling for differences in biases and lapses) and were not modulated by perceptual history ($\beta_0$ = $`r STAT.PM_diff_threshold_0_1$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold_0_1$coefficients[1,2]`$, T($`r   STAT.PM_diff_threshold_0_1$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold_0_1$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold_0_1$coefficients[1,5]`$).

\newpage
## Figure 4

```{r Figure_4}
mouse_p_distribution_Behav <- ggplot(M_Behav_long, aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

mouse_p_distribution_Behav_diff <- ggplot(M_Behav_diff_long, aes(x = History, color = -Accuracy, fill = Accuracy)) + 
   # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
   geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
    geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "top")



Plot_Acf_Data =  M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" | M_Summary_acf$Variable == "diff_acf_History",]
# Plot_Acf_Data =  M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" | M_Summary_acf$Variable == "diff_acf_History",]
# Plot_Acf_Data =  M_Summary_acf

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_Difficulty", "Difficulty", Plot_Acf_Data$Variable)

y_min = -0.002
mouse_p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 1,
    fill = "white",
    position = position_dodge(width = 0.5)
  )  + 
  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = y_min,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
   
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocorrelation coefficient"), subtitle = "B", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(-0.5, 25)) #+ ylim(y_min, 0.03)  



mouse_p_exceed_acf <- ggplot(ID_M_Summary_acf_long, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + ylim(0,0.35) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~
Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())

mouse_p_power_frequency <-
  ggplot(data = M_Power_Frequency[M_Power_Frequency$r_freq > 0.01 & M_Power_Frequency$r_freq < 0.09,],
         mapping = aes(x = (r_freq), y = (mean_power), ymin = mean_power - ci_power, ymax = mean_power - ci_power, color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.25, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE, level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.35) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency",  y = "Spectral Density", subtitle = "D")

mouse_p_density_phase <-
  ggplot(M_Power_Spectra[M_Power_Spectra$r_freq > 0.01 & M_Power_Spectra$r_freq < 0.09,], aes(
    x = abs(Phase)
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Phase",  y =
                                                                                  "Density", subtitle = "E") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") +
  geom_vline(xintercept = pi, linetype = "dashed",
             color = "black",
             size = 0.25) #+ xlim(-0.2, 3.4)

mouse_p_density_coherence <-
  ggplot(M_Power_Spectra[M_Power_Spectra$bin_freq < 0.09,], aes(
    x = Coherence
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Squred coherence (%)",  y =
                                                                                  "Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") + geom_vline(xintercept = mean(M_Power_Spectra[M_Power_Frequency$r_freq > 0.01 & M_Power_Frequency$r_freq < 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",
                                                                                                                                                                            color = "black",
                                                                                                                                                                            size = 0.25) + xlim(-1, 26)

mouse_p_RT_mode <- ggplot(data = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_RT,
      ymin = average_RT - se_RT,
      ymax = average_RT + se_RT)) +
  geom_point(
    alpha = 0.8,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"TD (msec)", subtitle = "H")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5 & M_Post_Perceptual_Modes$average_RT == max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

mouse_p_RT_mode2 <- ggplot(data = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_RT2,
      ymin = average_RT2 - se_RT2,
      ymax = average_RT2 + se_RT2)) +
  geom_point(
    alpha = 0.8,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"engaged TD (msec)", subtitle = "I")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT2),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5 & M_Post_Perceptual_Modes$average_RT2 == max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT2),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

p_mouse_RT_Congruency <- ggplot() +
  geom_point(data = M_RT_Behav_long,
    aes(
      x = Variable,
      y = diff_RT,
      color = Variable),
      alpha = 0.25,
      size = 0.01,
    fill = "white",
    position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = M_Summary_RT_Behav,
    aes(
      x = Variable,
      ymin = Mean - Error,
      ymax = Mean + Error,
      color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff TD (msec)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

lay <- rbind(c(1,2,2,3), c(9,2,2,3), c(4,5, 7,8), c(4,6,7,10))
grid.arrange(
mouse_p_distribution_Behav, mouse_p_acf, mouse_p_exceed_acf,
mouse_p_power_frequency, mouse_p_density_phase, mouse_p_density_coherence,
p_mouse_RT_Congruency, mouse_p_RT_mode, mouse_p_distribution_Behav_diff, mouse_p_RT_mode2,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))

```

**Figure 4. Internal and external modes in murine perceptual decision-making.** 

A. In mice, `r mean(M_Behav$Accuracy, na.rm = TRUE)`% ± `r sd(M_Behav$Accuracy, na.rm = TRUE)/sqrt(length(M_Behav$Accuracy))`% of trials were stimulus-congruent (in red) and `r mean(M_Behav$History, na.rm = TRUE)`% ± `r sd(M_Behav$History, na.rm = TRUE)/sqrt(length(M_Behav$History))`% of trials were history-congruent (in blue; upper panel). History-congruent perceptual choices were not a consequence of the experimental design, but a source of error, as they were more frequent on stimulus-incongruent trials (lower panel).

B. Relative to randomly permuted data, we found highly significant autocorrelations of stimulus-congruence and history-congruence (dots indicate intercepts $\neq$ 0 in trial-wise linear mixed effects modeling at p < 0.05). Please note that the negative autocorrelation of stimulus-congruence at trial 2 was a consequence of the experimental design (see Supplemental Figure 2D-F). As in humans, autocorrelation coefficients were best fit by an exponential function (adjusted $R^2$ for stimulus-congruence: `r M_Stimulus_exponential$adj.r.squared`; history-congruence: `r M_History_exponential$adj.r.squared`) as compared to a linear function (adjusted $R^2$ for stimulus-congruence: `r M_Stimulus_linear$adj.r.squared`; history-congruence: `r M_History_linear$adj.r.squared`). 

C. For stimulus-congruence (upper panel), the lag of positive autocorrelation was longer in comparison to humans (`r mean(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_M_Summary_acf)` on average). For history-congruence (lower panel), the lag of positive autocorrelation was slightly shorter relative to humans (`r mean(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_M_Summary_acf)` on average, peaking at trial t+2 after the index trial).

D. In mice, the dynamic probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**.

E. The distribution of phase shift between fluctuations in stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). 

F. The average squared coherence between fluctuations in stimulus- and history-congruence (black dottet line) amounted to `r mean(M_Summary_Power_Spectra$mean_coherence, na.rm = TRUE)` ± `r sd(M_Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(M_Summary_Power_Spectra$mean_coherence)`%

G. We observed shorter trial durations (TDs) for stimulus-congruence (as opposed to stimulus-incongruence, $\beta$ = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$), 
but longer TDs for history-congruence ($\beta$ = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$). 

H. TDs decreased monotonically for stronger biases toward external mode ($\beta_1$ = $`r M_STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r M_STAT.RT_vs_mode$coefficients[2,2]`$, T($`r M_STAT.RT_vs_mode$coefficients[2,3]`$) = $`r M_STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r M_STAT.RT_vs_mode$coefficients[2,5]`$). The horizontal and vertical dotted lines indicate maximum TD and the associated mode, respectively.

I. For TDs that differed from the median TD by no more than 1.5 x MAD (median absolute distance[@Leys2013]), mice exhibited a quadratic component in the relationship between the mode of sensory processing and TDs ($\beta_2$ = $`r M_STAT.RT_vs_mode2$coefficients[3,1]`$ ± $`r M_STAT.RT_vs_mode2$coefficients[3,2]`$, T($`r M_STAT.RT_vs_mode2$coefficients[3,3]`$) = $`r M_STAT.RT_vs_mode2$coefficients[3,4]`$, p = $`r M_STAT.RT_vs_mode2$coefficients[3,5]`$, Figure 4I). This explorative post-hoc analysis focuses on trials at which mice engage more swiftly with the experimental task. The horizontal and vertical dotted lines indicate maximum TD and the associated mode, respectively.

\newpage
## Figure 5

```{r Figure_5}
p_pm_estimates_lower_lapse_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("lower_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.03,
    size = 0.5
  ) + 
  labs(x = "Lower lapses",  y =
         "Density", linetype = "", subtitle = "C") +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_upper_lapse_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("higher_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.03,
    size = 0.5
  )  + 
  labs(x = "Higher lapses",  y =
         "Density", linetype = "", subtitle = "D") +
  theme_classic(base_size = 6) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)


p_pm_estimates_bias_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("bias"), ], aes(x = -Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.05,
    size = 0.5
  )  + 
  labs(x = "Bias",  y =
         "Density", linetype = "", subtitle = "B") +
  theme_classic(base_size = 6) + 
  xlim(-0.5, 0.5) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_threshold_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("threshold"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.1,
    size = 0.15
  ) + 
  labs(x = "Threshold",  y =
         "Density", linetype = "", subtitle = "E") +
  theme_classic(base_size = 6) + xlim(0, 1) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_simulation_mouse <-
  ggplot(data = Sim_PM_M_group, 
         aes(x = Stimulus, y = mean_y_prob, ymax = mean_y_prob + error_y_prob, ymin = mean_y_prob - error_y_prob, color = Type, fill = Type)) +
  geom_ribbon(
    color = "white",
    alpha = 0.25,
    size = 0.1
  ) +  geom_line(size = 0.1) +
  labs(x = "Stimulus",  y =
         "P (y = 1)", linetype = "", subtitle = "A") +
  theme_classic(base_size = 6) +
  xlim(-0.5, 0.5) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "left") + facet_wrap( ~ Conditioned, ncol = 1) 

lay <- rbind(c(5, 1, 2, 3, 4))
grid.arrange(
  p_pm_estimates_bias_mouse,
  p_pm_estimates_lower_lapse_mouse,
  p_pm_estimates_upper_lapse_mouse,
  p_pm_estimates_threshold_mouse,
  p_pm_simulation_mouse,
  layout_matrix = lay,
  heights = c(1),
  widths = c(2, 1, 1, 1, 1)
)
```

**Figure 5. Full and history-conditioned psychometric functions across modes in mice.** 
 
A. Here, we show average psychometric functions for the full IBL dataset (upper panel) and conditioned on perceptual history ($y_{t-1} = 1$ and $y_{t-1} = 0$; middle and lower panel) across modes (green line) and for internal mode (blue line) and external mode (red line) separately. 

B. Across the full dataset, biases $\mu$ were distributed around zero (T(`r M_STAT.PM_zero_bias_b$parameter`) = `r M_STAT.PM_zero_bias_b$statistic`, p = $`r M_STAT.PM_zero_bias_b$p.value`$; upper panel), 
with larger absolute biases $|\mu|$ for internal as compared to external mode ($\beta_0$ = $`r M_STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_bias$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_bias$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_bias$coefficients[1,4]`$; controlling for differences in lapses and thresholds).
When conditioned on perceptual history, we observed negative biases for $y_{t-1} = 0$ (T(`r M_STAT.PM_zero_bias_0$parameter`) = `r -M_STAT.PM_zero_bias_0$statistic`, p = $`r M_STAT.PM_zero_bias_0$p.value`$; middle panel) 
and positive biases for $y_{t-1} = 1$ (T(`r M_STAT.PM_zero_bias_1$parameter`) = `r -M_STAT.PM_zero_bias_1$statistic`, p = $`r M_STAT.PM_zero_bias_1$p.value`$; lower panel).

C. Lapse rates were higher in internal as compared to external mode ($\beta_0$ = $`r M_STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_lapse$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_lapse$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_lapse$coefficients[1,4]`$; controlling for differences in biases and thresholds; upper panel, see also subplot D). 
For $y_{t-1} = 1$, the difference between internal and external mode was more pronounced for lower lapses $\gamma$ (T(`r M_STAT.PM_diff_lapse_0_higher_vs_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_higher_vs_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher_vs_lower$p.value`$) as compared to higher lapses $\delta$ (see subplot D). 
In mice, lower lapses $\gamma$ were significantly elevated during internal mode irrespective of the preceding perceptual choice 
(middle panel: lower lapses $\gamma$ for $y_{t-1} = 0$; T(`r M_STAT.PM_diff_lapse_0_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_lower$p.value`$,
lower panel: lower lapses $\gamma$ for $y_{t-1} = 1$; T(`r M_STAT.PM_diff_lapse_1_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_lower$p.value`$).

D. For $y_{t-1} = 0$, the difference between internal and external mode was more pronounced for higher lapses $\delta$ (T(`r M_STAT.PM_diff_lapse_1_higher_vs_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_higher_vs_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher_vs_lower$p.value`$, see subplot C). Higher lapses were significantly elevated during internal mode irrespective of the preceding perceptual choice 
(middle panel: higher lapses $\delta$ for $y_{t-1} = 0$; T(`r M_STAT.PM_diff_lapse_0_higher$parameter`) = `r M_STAT.PM_diff_lapse_0_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher$p.value`$
lower panel: higher lapses $\delta$ for $y_{t-1} = 1$; T(`r M_STAT.PM_diff_lapse_1_higher$parameter`) = `r M_STAT.PM_diff_lapse_1_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher$p.value`$;
).

E. Thresholds $t$ were higher in internal as compared to external mode ($\beta_0$ = $`r M_STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_threshold$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_threshold$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_threshold$coefficients[1,4]`$; controlling for differences in biases and lapses) and were not modulated by perceptual history (T(`r M_STAT.PM_diff_threshold_0_1$parameter`) = `r M_STAT.PM_diff_threshold_0_1$statistic`, p = $`r M_STAT.PM_diff_threshold_0_1$p.value`$).

\newpage
## Figure 6

```{r Figure_6}
gathercol <- colnames(Sim_Behav[,c(3,6)])

Sim_Behav_long  <-
  gather(Sim_Behav[, c(1,2,3,6)],
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Sim_Behav_long$Frequency <-
  gsub("Accuracy", "Stimulus", Sim_Behav_long$Frequency)

sim_p_distribution_Behav <- ggplot(Sim_Behav_long, aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

##
## Probability of History-congruence depending on perceptual accuracy
##


gathercol <- colnames(Sim_Behav_diff[3])

Sim_Behav_diff_long  <-
  gather(Sim_Behav_diff,
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Sim_Behav_diff_long$Accuracy <- as.character(Sim_Behav_diff_long$Accuracy) 
Sim_Behav_diff_long$Accuracy <-
  gsub("0", "error", Sim_Behav_diff_long$Accuracy)
Sim_Behav_diff_long$Accuracy <-
  gsub("1", "correct", Sim_Behav_diff_long$Accuracy)

sim_p_distribution_Behav_diff <- ggplot(Sim_Behav_diff_long, aes(x = Frequency, color = -Accuracy, fill = Accuracy)) + 
  # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "bottom")


##
## Acf Plot
##
Plot_Acf_Data =  Sim_Summary_acf[(Sim_Summary_acf$Variable == "diff_acf_Stimulus" | Sim_Summary_acf$Variable == "diff_acf_History") ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

y_min = -0.002
sim_p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
             aes(
               x = Trial,
               y = Mean,
               fill = Variable,
               color = Variable),
             alpha = 0.8,
             size = 1,
             fill = "white",
             position = position_dodge(width = 0.5)
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = y_min,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
                aes(
                  x = Trial,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  fill = Variable,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
                                                                                                                               aes(
                                                                                                                                 x = Trial,
                                                                                                                                 y = Mean,
                                                                                                                                 fill = Variable,
                                                                                                                                 color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
  
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
                                        paste("Autocorrelation coefficient"), subtitle = "B", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.8, 0.9), legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25)) #+ ylim(-0.004, 0.03)  


##
## Exceed Plot
##


gathercol <- colnames(ID_Sim_Summary_acf[,c(3,4)])

ID_Sim_Summary_acf_long  <-
  gather(ID_Sim_Summary_acf,
         "Variable",
         "Lag",
         gathercol,
         factor_key = TRUE)

sim_p_exceed_acf <- ggplot(ID_Sim_Summary_acf_long, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
  scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())

##
## Power Spectra
##

Sim_p_power_frequency <-
  ggplot(data = Sim_Power_Frequency[Sim_Power_Frequency$r_freq > 0.01 & Sim_Power_Frequency$r_freq < 0.09,],
         mapping = aes(x = (r_freq), y = (mean_power), ymin = (mean_power - ci_power), ymax = (mean_power - ci_power), color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.5, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
                                                level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.2) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency",  y = "Spectral Density", subtitle = "D")

Sim_p_density_phase <-
  ggplot(Sim_Power_Spectra[Sim_Power_Spectra$r_freq > 0.01 & Sim_Power_Spectra$r_freq < 0.09,], aes(
    x = abs(Phase)
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Phase",  y =
                                                                                  "Density", subtitle = "E") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") +
  geom_vline(xintercept = pi, linetype = "dashed",
             color = "black",
             size = 0.25) #+ xlim(-0.2, 3.4)

Sim_p_density_coherence <-
  ggplot(Sim_Power_Spectra[Sim_Power_Spectra$bin_freq < 0.09,], aes(
    x = Coherence
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Squared coherence (%)",  y =
                                                                                  "Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") + geom_vline(xintercept = mean(Sim_Power_Spectra[Sim_Power_Frequency$r_freq > 0.01 & Sim_Power_Frequency$r_freq < 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",
                                                                                                                                                                            color = "black",
                                                                                                                                                                            size = 0.25) + xlim(-1, 26)


sim_p_Confidence_mode <- ggplot(data = Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0.5,], aes(
  x = directed_mode,
  y = average_Confidence,
  ymin = average_Confidence - se_Confidence,
  ymax = average_Confidence + se_Confidence)) +
  geom_point(
    alpha = 0.5,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
         "Simulated posterior certainty", subtitle = "H")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
                                                                                                                                                                                                                                                                     level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = min(Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0,]$average_Confidence),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0 & Sim_Post_Perceptual_Modes$average_Confidence == min(Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0,]$average_Confidence),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

sim_p_Confidence <- ggplot() +
  geom_point(data = Sim_Confidence_Behav_long,
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Summary_Sim_Confidence_Behav,
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff posterior certainty (zscore)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.1, 0.25)

lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
grid.arrange(
sim_p_distribution_Behav, sim_p_acf, sim_p_exceed_acf,
Sim_p_power_frequency, Sim_p_density_phase, Sim_p_density_coherence,
sim_p_Confidence_mode, sim_p_distribution_Behav_diff, sim_p_Confidence,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Figure 6. Internal and external modes in simulated perceptual decision-making.**

A. Simulated perceptual choices were stimulus-congruent in `r mean(Sim_Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Sim_Behav$Accuracy, na.rm = TRUE)/sqrt(length(Sim_Behav$Accuracy))`% (in red) and history-congruent in `r mean(Sim_Behav$History, na.rm = TRUE)`% ± `r sd(Sim_Behav$History, na.rm = TRUE)/sqrt(length(Sim_Behav$History))`% of trials (in blue; T(`r Sim_STAT.Global_History_Accuracy$parameter`) = `r Sim_STAT.Global_History_Accuracy$statistic`, p = $`r Sim_STAT.Global_History_Accuracy$p.value`$; upper panel). 
Due to the competition between stimulus- and history-congruence, history-congruent perceptual choices were more frequent when perception was stimulus-incongruent (i.e., on *error* trials; T(`r Sim_STAT.diff_History_Accuracy$parameter`) = `r Sim_STAT.diff_History_Accuracy$statistic`, p = $`r Sim_STAT.diff_History_Accuracy$p.value`$; lower panel) and thus impaired performance in the randomized psychophysical design simulated here. 

B. At the simulated group level, we found significant autocorrelations in both stimulus-congruence (`r min(which(Sim_Summary_acf[Sim_Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials) and history-congruence (`r min(which(Sim_Summary_acf[Sim_Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials). 
<!-- In line with humans and mice, the autocorrelation of history-congruence peaked at the first trial, after which is decayed exponentially (rate $\gamma$ = $`r Sim_STAT.lmer_acf_History$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_acf_History$coefficients[2,2]`$, T($`r Sim_STAT.lmer_acf_History$coefficients[2,3]`$) = $`r Sim_STAT.lmer_acf_History$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_acf_History$coefficients[2,5]`$). -->

C. On the level of individual simulated participants, autocorrelation coefficients exceeded the autocorrelation coefficients of randomly permuted data within a lag of `r mean(ID_Sim_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_Sim_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_Sim_Summary_acf)` trials for stimulus-congruence and `r mean(ID_Sim_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_Sim_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_Sim_Summary_acf)` trials for history-congruence.

D. The smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,2]`$, T($`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,3]`$) = $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Sim_STAT.lmer_power_freq_History$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_power_freq_History$coefficients[2,2]`$, T($`r Sim_STAT.lmer_power_freq_History$coefficients[2,3]`$) = $`r Sim_STAT.lmer_power_freq_History$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_power_freq_History$coefficients[2,5]`$).

E. The distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). The dynamic probabilities of simulated stimulus- and history-congruence were therefore were strongly anti-correlated ($\beta$ = $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,1]`$ ± $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,2]`$, T($`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,3]`$) = $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,4]`$, p = $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,5]`$).

F. The average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) amounted to `r mean(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)` ± `r sd(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(Summary_Power_Spectra$mean_coherence)`%.

G. Simulated confidence was enhanced for stimulus-congruence ($\beta$ = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$) 
and history-congruence ($\beta$ = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$). 

H. In analogy to humans, the simulated data showed a quadratic relationship between the mode of perceptual processing and posterior certainty, which increased for stronger external and internal biases ($\beta_2$ = $`r Sim_STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r Sim_STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r Sim_STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r Sim_STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r Sim_STAT.Confidence_vs_mode$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Figure 7

```{r Figure_7}

if (visualize_circular_inference == TRUE){
source("./Functions/simulation_glaze_osc_human_zeta_v1_adaptive.R", local = knitr::knit_global())
##
## generate environment Settings
##
n_participants = 1

n_blocks = 10
block_length  <- runif(min = 100, max = 100, n_blocks)


var_prec = 0
zeta = 1

outcomes = c(0,1)

# learning rates
alpha_switch = sample(seq(0.05,0.25,0.05),n_participants, replace = TRUE)
alpha_stay = alpha_switch
alpha_percept = sample(seq(0.05,0.25,0.05),n_participants, replace = TRUE)

# specifics of bimodal inference
frequency = sample(seq(0.05,0.05,0.025),n_participants, replace = TRUE)
amp = sample(seq(1,1,1),n_participants, replace = TRUE)

# starting values for H and P
H_logit = sample(seq(-0.25,0,0.01),n_participants, replace = TRUE)
P_logit = sample(seq(0.25,2,0.25),n_participants, replace = TRUE)

Sim = data.frame()
for (subj_idx in c(1:n_participants)){
  {print(subj_idx)}
  
# hazard rate and encoding precision of environment  
block_probs <- sample(seq(0.1,1,0.2),n_blocks, replace = TRUE)
block_precs <- sample(seq(2,8,1),n_blocks, replace = TRUE)

feedback_level = 0

  for (amp_on_idx in c(0,1)){
    Sim_add <-
      simulation_glaze_osc_human_zeta_v1_adaptive(
      n_blocks,
      block_length,
      block_probs,
      block_precs,
      var_prec,
      H_logit[subj_idx],
      P_logit[subj_idx],
      outcomes,
      alpha_switch[subj_idx],
      alpha_stay[subj_idx],
      alpha_percept[subj_idx],
      amp[subj_idx],
      amp_on_idx,
      frequency[subj_idx],
      zeta,
      sliding_window,
      n_permutations,
      feedback_level
      ) 
  
  Sim_add$diff_acf_Stimulus <- exclude_3SD(Sim_add$acf_Stimulus - Sim_add$random_acf_Stimulus) 
  Sim_add$diff_acf_History <- exclude_3SD(Sim_add$acf_History - Sim_add$random_acf_History) 
  Sim_add$subject_id = subj_idx
  Sim_add$amp = amp[subj_idx]
  Sim_add$amp_on = amp_on_idx
  Sim_add$alpha_switch = alpha_switch[subj_idx]
  Sim_add$alpha_stay = alpha_stay[subj_idx]
  Sim_add$alpha_percept = alpha_percept[subj_idx]
  Sim_add$frequency = frequency[subj_idx]
  Sim_add$zeta =  zeta
  Sim_add$feedback_level = feedback_level
  Sim_add$epsilon_H = abs(Sim_add$alpha_stay*Sim_add$oscillation_ei_LLR * Sim_add$da_H)
  Sim_add$epsilon_M = abs(Sim_add$alpha_percept*Sim_add$oscillation_ei * Sim_add$da_P)
 

gathercol = colnames(Sim_add[,c(25,33,34, 41,42)])
Sim_long  <-
gather(Sim_add[,c(25,33,34,41,42,43,44,45,51)],
"Variable",
"Value",
gathercol,
factor_key = TRUE)

Sim_long$Variable <- gsub("error_", "", Sim_long$Variable)

Sim = rbind(Sim, Sim_add)
  }
}
} else {
  Sim <- read.csv("./Results/Sim_visualize.csv")
}

Sim$amp_on[Sim$amp_on == 1] = "Bimodal"
Sim$amp_on[Sim$amp_on == 0] = "Unimodal"


gathercol = colnames(Sim[,c(1:7,9:16, 18:32, 52:53)])
Sim_long  <-
gather(Sim,
"Variable",
"Value",
gathercol,
factor_key = TRUE)

Sim_long$Variable <- gsub("probs", "true_H", Sim_long$Variable)
Sim_long$Variable <- gsub("L_Prob", "true_M", Sim_long$Variable)
Sim_long$Variable <- gsub("P", "M", Sim_long$Variable)
# green: "#66C2A5"
# orange:  "#FC8D62"
p_H <- ggplot() +
geom_line(
data = Sim_long[Sim_long$Variable == "H"  | Sim_long$Variable == "true_H",],
aes(x = trial, y = Value, linetype = Variable),
size = 0.5,
 alpha = 0.75,
color = "#66C2A5"
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"H", linetype = "", subtitle = "A") +
theme(legend.position = "top")  + facet_wrap(~amp_on)

p_epsilon_H <- ggplot() +
geom_line(
data = Sim_long[Sim_long$Variable == "epsilon_H",],
aes(x = trial, y = Value),
size = 0.25,
 alpha = 0.75,
color = "#66C2A5"
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = "Trial",  y =
"epsilon_H", linetype = "", subtitle = NULL) +
theme(legend.position = "none")  + facet_wrap(~amp_on) + theme(
  strip.background = element_blank(),
  strip.text.x = element_blank()
)

p_M <- ggplot() +
geom_line(
data = Sim_long[Sim_long$Variable == "M" | Sim_long$Variable == "true_M",],
aes(x = trial, y = Value, linetype = Variable),
size = 0.5,
 alpha = 0.75,
color = "#FC8D62"
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"M", linetype = "", subtitle = "B") +
theme(legend.position = "top")  + facet_wrap(~amp_on)

p_epsilon_M <- ggplot() +
geom_line(
data = Sim_long[Sim_long$Variable == "epsilon_M",],
aes(x = trial, y = Value),
size = 0.25,
 alpha = 0.75,
color = "#FC8D62"
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = "Trial",  y =
"epsilon_M", linetype = "", subtitle = NULL) +
theme(legend.position = "none")  + facet_wrap(~amp_on) + 
theme(
  strip.background = element_blank(),
  strip.text.x = element_blank()
)

p_adaptive_sim_accuracy <- ggplot() +
geom_jitter(data = Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff$Variable == "Accuracy",],
            aes(x = feedback_level, y = diff), width = 0.1,  alpha = 0.01,
    size = 0.1, color = "#E41A1C") +
   geom_errorbar(data = Group_Summary_Sim_adaptive_diff[Group_Summary_Sim_adaptive_diff$Variable == "Accuracy",],
    aes(
      x = feedback_level,
      ymin = mean - error,
      ymax = mean + error), width = 0.25, alpha = 1, size = 0.5, color = "#E41A1C") + 
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Feedback (%)",  y = "Stimulus-congruence (%, Bimodal-unimodal)", subtitle = "D") + scale_color_brewer(palette = "Set2") + theme(legend.position = "left")  + coord_flip()

p_adaptive_sim_errors <- ggplot() +
geom_point(data = Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff$Variable != "Accuracy",],
            aes(x = feedback_level, y = diff, color = Variable, fill = Variable), width = 0.1,  alpha = 0.01,
    size = 0.1, position = position_jitterdodge(dodge.width = 0.5)) +
   geom_errorbar(data = Group_Summary_Sim_adaptive_diff[Group_Summary_Sim_adaptive_diff$Variable != "Accuracy",],
    aes(
      x = feedback_level,
      ymin = mean - error,
      ymax = mean + error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 1, size = 0.5) + 
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Feedback (%)",  y = "Error (Bimodal-unimodal)", subtitle = "F") + scale_color_brewer(palette = "Set2") + theme(legend.position = "none") + coord_flip()

p_adaptive_sim_accuracy_density <- ggplot(Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff$Variable == "Accuracy" & Summary_Sim_adaptive_diff$feedback_level == 0,], aes(x= diff)) + 
   geom_density(colour = "white", fill = "#E41A1C", alpha=.4, bw = 0.25) + 
  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Stim.-congr. (%, Bimodal-unimodal)", y = "Density", subtitle = "C") + scale_fill_brewer(palette = "Set2") + theme(legend.position = "none")

p_adaptive_sim_errors_density <- ggplot(Summary_Sim_adaptive_diff[Summary_Sim_adaptive_diff$Variable != "Accuracy" & Summary_Sim_adaptive_diff$feedback_level == 0,], aes(x= diff, fill = Variable, color = Variable)) + 
   geom_density(colour = "white", alpha=.75, bw = 0.0025) + 
  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Error (Bimodal-unimodal)", y = "Density", subtitle = "E") + scale_fill_brewer(palette = "Set2") + theme(legend.position = "top")

lay <- rbind(c(1,1,3,3), c(2,2,4,4),c(5,6,6,6), c(7,8,8,8))
grid.arrange(
p_H,
p_epsilon_H,
p_M,
p_epsilon_M,
p_adaptive_sim_accuracy_density,
p_adaptive_sim_accuracy,
p_adaptive_sim_errors_density,
p_adaptive_sim_errors,
layout_matrix = lay,
heights = c(1.25,0.5,1,1),
widths = c(1,1,1,1))
```

**Figure 7. Adaptive benefits of bimodal inference.**

A. When the sensory environment changes unpredictably over time, agents have to update estimates $H_t$ (solid green line, upper panel) about the true hazard rate $\hat{H_t}$ from experience (dotted green line, upper panel). Updates to $H_t$ are driven by an error term $\epsilon_H$ (solid green line, lower panel) that is defined by the difference between $H_t$ and the presence of a perceived change in the environment. In contrast to the unimodal model (right panels), $\epsilon_H$ of the bimodal model (left panels) is modulated by a phasic component reflecting ongoing fluctuations between internal and external mode.

B. When the precision of sensory encoding changes unpredictably over time, agents have to update estimates $M_t$ (solid orange line, upper panel) about the true precision of sensory encoding $\hat{M_t}$ from experience (dotted orange line, upper panel). Updates to $M_t$ are driven by an error term $\epsilon_M$ (red line, lower panel) that is defined by the difference between $M_t$ and the posterior decision-certainty. In contrast to the unimodal model (right panels), $\epsilon_M$ of the bimodal model (left panels) is modulated by a phasic component reflecting ongoing fluctuations between internal and external mode.  

C. In the absence of feedback, the bimodal inference model achieved lower stimulus-congruence as compared the unimodal control model ($\beta_1$ = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,1]`$ ± $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,2]`$, T($`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,3]`$) = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,4]`$, p = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[1,5]`$). 

D. The unimodal control model benefited more strongly from the presence of external feedback, leading to a relative decrease in stimulus-congruence for the bimodal inference model at higher feedback levels ($\beta_2$ = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[2,1]`$ ± $`r Sim_STAT.Accuracy_vs_feedback$coefficients[2,2]`$, T($`r Sim_STAT.Accuracy_vs_feedback$coefficients[2,3]`$) = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[2,4]`$, p = $`r Sim_STAT.Accuracy_vs_feedback$coefficients[2,5]`$).

E. In the absence of feedback, the bimodal inference model achieved lower errors in the estimated hazard rate $H$ ($\beta_1$ = $`r Sim_STAT.H_vs_feedback$coefficients[1,1]`$ ± $`r Sim_STAT.H_vs_feedback$coefficients[1,2]`$, T($`r Sim_STAT.H_vs_feedback$coefficients[1,3]`$) = $`r Sim_STAT.H_vs_feedback$coefficients[1,4]`$, p = $`r Sim_STAT.H_vs_feedback$coefficients[1,5]`$) 
as well as lower errors in the estimated probability of stimulus-congruent choices $M$ ($\beta_1$ = $`r Sim_STAT.P_vs_feedback$coefficients[1,1]`$ ± $`r Sim_STAT.P_vs_feedback$coefficients[1,2]`$, T($`r Sim_STAT.P_vs_feedback$coefficients[1,3]`$) = $`r Sim_STAT.P_vs_feedback$coefficients[1,4]`$, p = $`r Sim_STAT.P_vs_feedback$coefficients[1,5]`$). 

F. With an increasing availability of feedback, the advantage of the bimodal inference model was lost with respect to $H$ ($\beta_2$ = $`r Sim_STAT.H_vs_feedback$coefficients[2,1]`$ ± $`r Sim_STAT.H_vs_feedback$coefficients[2,2]`$, T($`r Sim_STAT.H_vs_feedback$coefficients[2,3]`$) = $`r Sim_STAT.H_vs_feedback$coefficients[2,4]`$, p = $`r Sim_STAT.H_vs_feedback$coefficients[2,5]`$) 
and $M$ ($\beta_2$ = $`r Sim_STAT.P_vs_feedback$coefficients[2,1]`$ ± $`r Sim_STAT.P_vs_feedback$coefficients[2,2]`$, T($`r Sim_STAT.P_vs_feedback$coefficients[2,3]`$) = $`r Sim_STAT.P_vs_feedback$coefficients[2,4]`$, p = $`r Sim_STAT.P_vs_feedback$coefficients[2,5]`$).

\newpage
# Supplemental Items

## Supplemental Figure S1
```{r Supplememtal_Figure_S1}

p_Accuracy <-
  ggplot() + geom_jitter(
    data = Behav,
    mapping = aes(x = as.factor(study_id), y = Accuracy, color = as.factor(study_id)),
    alpha = 0.1,
    width = 0.5,
    size = 0.05
  ) + geom_errorbar(
data = Study_Behav,
mapping = aes(
ymin = mean_Accuracy - error_Accuracy,
ymax = mean_Accuracy + error_Accuracy,
x = as.factor(study_id),
color = as.factor(study_id)
),
width = 0.75,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = "Experiments",  y = "Stimulus-congruence (%)", subtitle = "A")

p_study_Accuracy <-
ggplot(Behav, aes(
x = Accuracy,
y = study_id,
group = study_id,
fill = study_id
)) + geom_density_ridges(scale = 6, alpha = 1, rel_min_height = 0, size = 0.25) + theme_ridges(font_size = 8,
grid = FALSE,
center_axis_labels = TRUE) + xlim(25, 100) + labs(x = NULL,  y =
"Experiments", subtitle = NULL) + theme_classic(base_size = 6) + theme(legend.position =
"none", axis.text.y = element_blank()) + geom_vline(xintercept = 50, color = "black", linetype = 2, size = 0.25) +  scale_x_continuous(name= NULL, breaks = c(50,100), limits=c(30, 100))


p_History <-
  ggplot() + geom_jitter(
    data = Behav,
    mapping = aes(x = as.factor(study_id), y = History, color = as.factor(study_id)),
    alpha = 0.1,
    width = 0.15,
    size = 0.05
  ) + geom_errorbar(
data = Study_Behav,
mapping = aes(
ymin = mean_History - error_History,
ymax = mean_History + error_History,
x = as.factor(study_id),
color = as.factor(study_id)
),
width = 0.75,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = "Experiments",  y = "History-congruence (%)", subtitle = "B")

p_study_History <-
ggplot(Behav, aes(
x = History,
y = study_id,
group = study_id,
fill = study_id
)) + geom_density_ridges(scale = 6, alpha = 1, rel_min_height = 0, size = 0.25) + theme_ridges(font_size = 8,
grid = FALSE,
center_axis_labels = TRUE) + xlim(25, 100) + labs(x =  NULL ,  y =
"Experiments", subtitle = NULL) + theme_classic(base_size = 6) + theme(legend.position =
"none", axis.text.y = element_blank()) + geom_vline(xintercept = 50, color = "black", linetype = 2, size = 0.25) +  scale_x_continuous(name= NULL, breaks = c(50,100), limits=c(30, 100))

d1 <- effect("Accuracy", Global_History_Accuracy, xlevels= list(Accuracy=seq(0, 100, 5)))
d1 <-as.data.frame(d1)
d1[,c(2,4,5)] <- 50 + d1[,c(2,4,5)]

Global_History_vs_Accuracy <- ggplot() + geom_line(data = d1, aes(x = Accuracy, y = fit), size = 0.2) + geom_ribbon(data = d1, aes(x = Accuracy, ymin =  lower, ymax = upper), color = "white", alpha = 0.2) +
  geom_point(data = Behav,  aes(x = Accuracy, y = History, color = study_id), shape = 20, size = 0.05, alpha = 0.3) +
  theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=8),
        axis.title.y = element_text(size=8), axis.text.y  = element_text(size=8)) +
  theme_classic(base_size = 6) + theme(legend.position =
"none") +  labs(x = "Stimulus-congruence (%)",  y =
"History-congruence (%)", subtitle = "C") + xlim(40, 100) + ylim(40,80) + geom_hline(
yintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) + geom_vline(
xintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
)

Model_History_vs_Accuracy <- plot_model(true_Global_History_Accuracy, type = "est", axis.lim = c(-0.05, 2.5), show.values = FALSE, show.p = FALSE, vline.color = "white", dot.size = 1, line.size = 0.1, transform = NULL)
Model_History_vs_Accuracy <- Model_History_vs_Accuracy + 
  theme_classic(base_size = 6) + labs(x = " ",  y =
"History-congruence ~ Stimulus-congruence (%)", subtitle = "E", title = NULL)  + scale_x_discrete(breaks=c("Accuracy"),
        labels=c(expression(paste(beta, "(S)"))))  + ylim(-0.22, 0.005) + geom_vline(
xintercept = 0,
linetype = "dashed",
color = "black",
size = 0.25
) +
  theme_classic(base_size = 6) 

M_STAT.Global_History_Accuracy <- cor.test(M_Behav$History,M_Behav$Accuracy)

M_Global_History_vs_Accuracy <- ggplot(data = M_Behav,  aes(x = Accuracy, y = History)) +
  geom_point(shape = 20, size = 0.05, alpha = 0.3) +
  theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=8),
        axis.title.y = element_text(size=8), axis.text.y  = element_text(size=8)) +
  theme_classic(base_size = 6) + theme(legend.position =
"none") +  labs(x = "Stimulus-congruence (%)",  y =
"History-congruence (%)", subtitle = "D") + geom_hline(
yintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) + geom_vline(
xintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) + geom_smooth(method = "lm", se =TRUE, color = "black", size = 0.2)

lay <- rbind(c(1, 2, 3, 4), c(5,5,6, 6))
grid.arrange(
p_Accuracy, p_study_Accuracy, p_History, p_study_History, 
Global_History_vs_Accuracy, M_Global_History_vs_Accuracy,
layout_matrix = lay,
heights = c(2,2),
widths = c(0.25,0.1,0.25, 0.1))
```

**Supplemental Figure S1. Stimulus- and history-congruence.** 

A. Stimulus-congruent choices in humans amounted to  `r mean(Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Accuracy, na.rm = TRUE)/sqrt(length(Behav$Accuracy))`% of trials and were highly consistent across the experiments selected from the Confidence Database. 

B. History-congruent choices in humans amounted to  `r mean(Behav$History, na.rm = TRUE)`% ± `r sd(Behav$History, na.rm = TRUE)/sqrt(length(Behav$History))`% of trials. In analogy to stimulus-congruence, the prevalence of history-congruence was highly consistent across the experiments selected from the Confidence Database. `r 100*sum(Study_Behav$p < 0.05 & Study_Behav$estimate > 0, na.rm = TRUE)/sum(!is.na(Study_Behav$study_id))`% of experiments showed significant (p < 0.05) attractive biases toward preceding choices, whereas `r 100*sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)/sum(!is.na(Study_Behav$study_id))`% of experiments showed significant repulsive biases.

C. In humans, we found an enhanced impact of perceptual history in participants who were less sensitive to external sensory information (T($`r STAT.Global_History_Accuracy$coefficients[2,3]`$) = $`r STAT.Global_History_Accuracy$coefficients[2,4]`$, p = $`r STAT.Global_History_Accuracy$coefficients[2,5]`$), suggesting that perception results from the competition of external with internal information. 

D. In analogy to humans, mice that were less sensitive to external sensory information showed stronger biases toward perceptual history (T(`r M_STAT.Global_History_Accuracy$parameter`) = `r M_STAT.Global_History_Accuracy$statistic`, p = $`r M_STAT.Global_History_Accuracy$p.value`$, Pearson correlation). 

\newpage
## Supplemental Figure S2
```{r Supplememtal_Figure_S2}

# STAT.Phase_History <- summary(lm(C_history ~ levels, data = Phase_Bins))
# STAT.Phase_Stimulus <- summary(lm(C_stimulus ~ levels, data = Phase_Bins))


##
## Human Data
## 

acf_to_plot = rbind(c("diff_acf_Difficulty", "diff_acf_External"), 
                c("./Results/Summary_acf_study_Difficulty2_corrected_normalized_3SD.csv", "./Results/Summary_acf_study_External2_corrected_normalized_3SD.csv"),
                c("A           Difficulty (Humans)", "B           External stimulation (Humans)"))

for (plot_idx in c(1:ncol(acf_to_plot))){
 
##
## Acf Plot
##
Plot_Acf_Data =  Summary_acf[Summary_acf$Variable == acf_to_plot[1,plot_idx],]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_HardEasy", "HardEasy", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_External", "External", Plot_Acf_Data$Variable)

y_min = -0.05
p_acf <- ggplot() +
  geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean
    ), color = "#4DAF4A",linetype = "dotted") + 
    
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean), color = "#4DAF4A",
    alpha = 0.8,
    size = 0.5,
    fill = "white"
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.02),
          fill = "#4DAF4A",
      color = "#4DAF4A",
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error
    ),  color = "#4DAF4A", width = 0.5, alpha = 0.5, size = 0.5) +
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = NULL,  y =
paste("Autocorr coef."), subtitle = acf_to_plot[3,plot_idx]) + ylim(-0.05, 0.08) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

assign(paste("plot", acf_to_plot[1,plot_idx], sep = "_"), p_acf)  


Summary_acf_Corrected <- read.csv(acf_to_plot[2,plot_idx])
Summary_acf_Corrected$Trial <- Summary_acf_Corrected$Trial - 1
##
## Acf Plot
##
Plot_Acf_Data =  Summary_acf_Corrected[Summary_acf_Corrected$Variable == "diff_acf_Stimulus" | Summary_acf_Corrected$Variable == "diff_acf_History" ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

y_min = -0.005
p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.005,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
   
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocorr. coef."), subtitle = " ", color = "Congruence to", shape = "p value") + ylim(-0.01, 0.03)  + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

assign(paste("plot_controlled", acf_to_plot[1,plot_idx], sep = "_"), p_acf)
 assign(paste("Summary_acf_Corrected", acf_to_plot[1,plot_idx], sep = "_"), Summary_acf_Corrected)
}

##
## murine Data
## 

acf_to_plot = rbind(c("diff_acf_Difficulty", "diff_acf_External"), 
                c("./Results/M_Summary_acf_session_Difficulty_corrected_normalized_3SD_new_preproc.csv", "./Results/M_Summary_acf_session_External_corrected_normalized_3SD_new_preproc.csv"),
                c("C           Difficulty (Mice)", "D           External stimulation (Mice)"))

for (plot_idx in c(1:ncol(acf_to_plot))){
 
##
## Acf Plot
##
Plot_Acf_Data =  M_Summary_acf[M_Summary_acf$Variable == acf_to_plot[1,plot_idx],]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_HardEasy", "HardEasy", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_External", "External", Plot_Acf_Data$Variable)

ylim = -0.01
p_acf <- ggplot() +
  geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean
    ),  color = "#4DAF4A", linetype = "dotted") + 
    
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      shape = p
    ),  color = "#4DAF4A",
    alpha = 0.8,
    size = 0.5,
    fill = "white"
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.01
  ),  fill = "#4DAF4A",
      color ="#4DAF4A",
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error
    ),  color = "#4DAF4A", width = 0.5, alpha = 0.5, size = 0.5) +
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = NULL,  y =
paste("Autocorr. coef."), subtitle = acf_to_plot[3,plot_idx]) + ylim(-0.02, 0.03) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

assign(paste("mouse_plot", acf_to_plot[1,plot_idx], sep = "_"), p_acf)  


M_Summary_acf_Corrected <- read.csv(acf_to_plot[2,plot_idx])
M_Summary_acf_Corrected$Trial <- M_Summary_acf_Corrected$Trial - 1
##
## Acf Plot
##
Plot_Acf_Data =  M_Summary_acf_Corrected[M_Summary_acf_Corrected$Variable == "diff_acf_Stimulus" | M_Summary_acf_Corrected$Variable == "diff_acf_History" ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.005,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") +

  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocor. coef."), subtitle = " ", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

 assign(paste("mouse_plot_controlled", acf_to_plot[1,plot_idx], sep = "_"), p_acf)
 assign(paste("M_Summary_acf_Corrected", acf_to_plot[1,plot_idx], sep = "_"), M_Summary_acf_Corrected)
}

##
## Exceed Plot Humans
##

gathercol <- colnames(ID_Summary_acf[,c(3,4,5)])

ID_Summary_acf_long_control  <-
gather(ID_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

p_exceed_acf_Difficulty <- ggplot(ID_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())

gathercol <- colnames(ID_Summary_acf[,c(3,4,6)])

ID_Summary_acf_long_control  <-
gather(ID_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

p_exceed_acf_External <- ggplot(ID_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())


##
## Exceed Plot murines
##
gathercol <- colnames(ID_M_Summary_acf[,c(2,3,4)])

ID_M_Summary_acf_long_control  <-
gather(ID_M_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

mouse_p_exceed_acf_Difficulty <- ggplot(ID_M_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())

gathercol <- colnames(ID_M_Summary_acf[,c(2,3,5)])

ID_M_Summary_acf_long_control  <-
gather(ID_M_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

mouse_p_exceed_acf_External <- ggplot(ID_M_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())

lay <- rbind(c(1,3), c(2,4), c(9,10), c(5,7), c(6,8), c(11,12))
grid.arrange(
plot_diff_acf_Difficulty, plot_controlled_diff_acf_Difficulty, 
plot_diff_acf_External, plot_controlled_diff_acf_External,
mouse_plot_diff_acf_Difficulty, mouse_plot_controlled_diff_acf_Difficulty, 
mouse_plot_diff_acf_External, mouse_plot_controlled_diff_acf_External,
p_exceed_acf_Difficulty, p_exceed_acf_External,
mouse_p_exceed_acf_Difficulty, mouse_p_exceed_acf_External,
layout_matrix = lay,
heights = c(0.9,1.25,0.75,0.9,1.25,0.75),
widths = c(1,1))
```

**Supplemental Figure S2. Controlling for task difficulty and external stimulation.** 

In this study, we found highly significant autocorrelations of stimulus- and history-congruence in humans as well as in mice. Here, we show that these autocorrelations are not a trivial consequence of task difficulty or the sequence external stimulation. In addition, we computed trial-wise logistic regression coefficients as an alternative approach to assessing serial dependencies in stimulus- and history-congruence. 

A. In humans, task difficulty (in green) showed a significant autocorrelated starting at the `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_Difficulty",]$p == "< 0.05"))`th trial (upper panel, dots at the bottom indicate intercepts $\neq$ 0 in trial-wise linear mixed effects modeling at p < 0.05). 
When controlling for task difficulty, linear mixed effects modeling indicated a significant auto-correlation of stimulus-congruence (in red) for the first `r min(which(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (middle panel). 
`r (sum(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus" & Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window remained significantly autocorrelated.
The autocorrelation of history-congruence (in blue) remained significant for the first `r min(which(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History" & Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% significantly autocorrelated trials within the displayed time window).
At the level of individual participants, the autocorrelation of task difficulty exceeded the respective autocorrelation of randomly permuted within a lag of `r mean(ID_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)` ± `r sd(ID_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)/nrow(ID_Summary_acf)` trials (lower panel).

B. The sequence of external stimulation (i.e., which of the two binary outcomes was supported by the presented stimuli; depicted in green) was negatively autocorrelated for `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_External",]$p == "> 0.05")) - 1` trial. 
When controlling for the autocorrelation of external stimulation, stimulus-congruence remained significantly autocorrelated for `r min(which(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus" & Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window; lower panel)
and history-congruence remained significantly autocorrelated for `r min(which(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History" & Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window).
At the level of individual participants, the autocorrelation of external stimulation exceeded the respective autocorrelation of randomly permuted within a lag of `r mean(ID_Summary_acf$lag_significant_External, na.rm = TRUE)` ± `r sd(ID_Summary_acf$lag_significant_External, na.rm = TRUE)/nrow(ID_Summary_acf)` consecutive trials (lower panel).

C. In mice, task difficulty showed an significant autocorrelated for the first `r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Difficulty",]$p == "> 0.05")) - 1` consecutive trials (upper panel). 
When controlling for task difficulty, linear mixed effects modeling indicated a significant auto-correlation of stimulus-congruence for the first `r min(which(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (middle panel). 
In total, `r (sum(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus" & M_Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window remained significantly autocorrelated.
The autocorrelation of history-congruence remained significant for the first `r min(which(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials, with `r (sum(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History" & M_Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% significantly autocorrelated trials within the displayed time window. 
At the level of individual mice, autocorrelation coefficients for difficulty were elevated above randomly permuted data within a lag of `r mean(ID_M_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)/nrow(ID_M_Summary_acf)` consecutive trials (lower panel).

D. In mice, the sequence of external stimulation (i.e., which of the two binary outcomes was supported by the presented stimuli) was negatively autocorrelated for `r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_External",]$p == "> 0.05")) - 1` consecutive trials (upper panel). 
When controlling for the autocorrelation of external stimulation, stimulus-congruence remained significantly autocorrelated for `r min(which(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus" & M_Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window; middle)
and history-congruence remained significantly autocorrelated for `r min(which(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History" & M_Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window).
At the level of individual mice, autocorrelation coefficients for external stimulation were elevated above randomly permuted data within a lag of `r mean(ID_M_Summary_acf$lag_significant_External, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_External, na.rm = TRUE)/nrow(ID_M_Summary_acf)` consecutive trials (lower panel).

\newpage
## Supplemental Figure S3
```{r Supplememtal_Figure_S3}

y_min = -0.02
p_trialwise_logreg <- ggplot() +
  geom_point(
    data = Group_Tw_LogReg_long,
    aes(
      x = -lag,
      y = Mean,
      fill = Variable,
      color = Variable
    ),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  geom_errorbar(
    data = Group_Tw_LogReg_long,
    aes(
      x = -lag,
      ymin = Mean - Error,
      ymax = Mean + Error,
      color = Variable
    ),
    position = position_dodge(width = 0.5),
    width = 0.5,
    alpha = 0.5,
    size = 0.5
  ) +
  geom_point(
    data = Group_Tw_LogReg_long[Group_Tw_LogReg_long$p_value < 0.05, ],
    aes(
      x = -lag,
      y = y_min,
      fill = Variable,
      color = Variable
    ),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  
  scale_color_brewer(palette = "Set1", direction = -1) +
  
  geom_line(
    data = Group_Tw_LogReg_long,
    aes(x = -lag,
        y = Mean,
        color = Variable),
    position = position_dodge(width = 0.5),
    linetype = "dotted"
  ) +
  
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
    
  ) +  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(
    x = "Lag (Trials)",
    y = "Regression coefficient: Congruence",
    subtitle = "A",
    color = "Regression weight"
  ) + theme(legend.position = "top", legend.box = "horizontal")  +  scale_x_continuous(
    breaks = seq(-25, 0, by = 1),
    labels = seq(-25, 0, by = 1),
    limits = c(-25.5, 0)
  )

mouse_p_trialwise_logreg <- ggplot() +
  geom_point(
    data = M_Group_Tw_LogReg_long,
    aes(
      x = -lag,
      y = Mean,
      fill = Variable,
      color = Variable
    ),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  
  geom_errorbar(
    data = M_Group_Tw_LogReg_long,
    aes(
      x = -lag,
      ymin = Mean - Error,
      ymax = Mean + Error,
      color = Variable
    ),
    position = position_dodge(width = 0.5),
    width = 0.5,
    alpha = 0.5,
    size = 0.5
  ) + scale_color_brewer(palette = "Set1", direction = -1) +
  
  geom_line(
    data = M_Group_Tw_LogReg_long,
    aes(x = -lag,
        y = Mean,
        color = Variable),
    position = position_dodge(width = 0.5),
    linetype = "dotted"
  ) +
  
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
    
  ) +  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(
    x = "Lag (Trials)",
    y = "Regression coefficient: Congruence",
    subtitle = "B",
    color = "Congruence to"
  ) + theme(legend.position = "none", legend.box = "horizontal")  +  scale_x_continuous(
    breaks = seq(-10, 0, by = 1),
    labels = seq(-10, 0, by = 1),
    limits = c(-10.5, 0)
  )

lay <- rbind(c(1), c(2))
grid.arrange(
  p_trialwise_logreg,
  mouse_p_trialwise_logreg,
  layout_matrix = lay,
  heights = c(1, 1),
  widths = c(1)
)
```

**Supplemental Figure S3. Reproducing group-level autocorrelations using logistic regression.**

A. As an alternative to group-level autocorrelation coefficients, we used trial-wise logistic regression to quantify serial dependencies in stimulus- and history-congruence. This analysis predicted stimulus- and history-congruence at the index trial (trial $t = 0$, vertical line) based on stimulus- and history-congruence at the `r n_back` preceding trials. Mirroring the shape of the group-level autocorrelations, trial-wise regression coefficients (depicted as mean ± SEM, dots mark trials with regression weights significantly greater than zero at p < 0.05) increased toward the index trial $t = 0$ for the human data.

B. Following our results in human data, regression coefficients that predicted history-congruence at the index trial (trial t = 0, vertical line) increased exponentially for trials closer to the index trial in mice. In contrast to history-congruence, stimulus-congruence showed a negative regression weight (or autocorrelation coefficient, see Figure 4B) at trial -2. This was due to the experimental design (see also the autocorrelations of difficulty and external stimulation in Supplemental Figure S2C and D): When mice made errors at easy trials (contrast $\geq$ 50%), the upcoming stimulus was shown at the same spatial location and at high contrast. This increased the probability of stimulus-congruent perceptual choices after stimulus-incongruent perceptual choices at easy trials, thereby creating a negative regression weight (or autocorrelation coefficient) of stimulus-congruence at trial -2. 

\newpage
## Supplemental Figure S4

```{r Supplememtal_Figure_S4}

p_AIC <-
  ggplot() + geom_jitter(
    data = LogAggr,
    mapping = aes(x = as.factor(study_id), y = diff_AIC, color = study_id),
    alpha = 0.1,
    width = 0.15,
    size = 0.05
  ) + geom_errorbar(
data = Study_LogAggr,
mapping = aes(
ymin = mean_diff_AIC - error_diff_AIC,
ymax = mean_diff_AIC + error_diff_AIC,
x = as.factor(study_id),
color = study_id
),
width = 0.75,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = "Experiments",  y = "rel. AIC (Stim. only - Stim. + Hist.)", subtitle = "A") + ylim(-60, 60)

p_study_AIC <-
ggplot(LogAggr[!is.na(LogAggr$diff_AIC),], aes(
x = diff_AIC,
y = study_id,
group = study_id,
fill = study_id
)) + geom_density_ridges(scale = 6, alpha = 1, rel_min_height = 0, size = 0.25) + theme_ridges(font_size = 8,
grid = FALSE,
center_axis_labels = TRUE) + labs(x = NULL,  y =
"Experiments", subtitle = NULL) + theme_classic(base_size = 6) + theme(legend.position =
"none", axis.text.y = element_blank()) + geom_vline(xintercept = 0, color = "black", linetype = 2, size = 0.25)  + xlim(-10,50)

gathercol <- colnames(LogAggr[, c(6:7)])

LogAggr_long  <-
gather(LogAggr[, c(3,6:7)],
"Variable",
"beta",
gathercol,
factor_key = TRUE)

LogAggr_long$Variable <-
gsub("beta_History", "History", LogAggr_long$Variable)
LogAggr_long$Variable  <-
gsub("beta_Stimulus", "Stimulus",LogAggr_long$Variable)
LogAggr_long$Variable <- as.factor(LogAggr_long$Variable)

p_distribution_logit <- ggplot(LogAggr_long, aes(x= beta, color = Variable, fill = Variable)) + 
   geom_density(colour = "white", alpha=.6, bw = 0.10, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(-2.5 ,5) + ylim(0,0.8) + labs(x = "Logistic regression: Perceptual choice ~ Stimulus + History",  y = "Density", fill = expression(paste(beta, " for")), subtitle = "B") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
    geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "right")


Summary_M_LogAggr <- data.frame(
mean_diff_AIC = mean(M_LogAggr$diff_AIC, na.rm = TRUE),
error_diff_AIC = sd(M_LogAggr$diff_AIC, na.rm = TRUE)/sqrt(nrow(M_LogAggr))
)

mouse_p_AIC <-
  ggplot() + geom_jitter(
    data = M_LogAggr,
    mapping = aes(x = subject_id, y = -diff_AIC),
    alpha = 0.15,
    width = 0.05,
    size = 0.1
  ) + geom_errorbar(
data = Summary_M_LogAggr,
mapping = aes(
ymin = -mean_diff_AIC - error_diff_AIC,
ymax = -mean_diff_AIC + error_diff_AIC,
x = mean(M_LogAggr$subject_id)
),
width = 7.5,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none") + labs(x = "Mice",  y = "rel. AIC (Stim. only - Stim. + Hist.)", subtitle = "C")+ ylim(-500, 500)


gathercol <- colnames(M_LogAggr[, c(4:5)])

M_LogAggr_long  <-
gather(M_LogAggr[, c(1,4:5)],
"Variable",
"beta",
gathercol,
factor_key = TRUE)

M_LogAggr_long$Variable <-
gsub("beta_History", "History", M_LogAggr_long$Variable)

M_LogAggr_long$Variable  <-
gsub("beta_Stimulus", "Stimulus", M_LogAggr_long$Variable)

M_LogAggr_long$Variable <- as.factor(M_LogAggr_long$Variable)

mouse_p_distribution_logit <- ggplot(M_LogAggr_long, aes(x= beta, color = Variable, fill = Variable)) + 
   geom_density(colour = "white", alpha=.6, bw = 0.30, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(-2.5 ,10) + labs(x = "Logistic regression: Perceptual choice ~ Stimulus + History",  y = "Density", fill = expression(paste(beta, " for")), subtitle = "D") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
    geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

lay <- rbind(c(1,1, 2), c(3,3, 3), c(4,5, 5))
grid.arrange(
p_AIC, p_study_AIC, p_distribution_logit, mouse_p_AIC, mouse_p_distribution_logit, 
layout_matrix = lay,
heights = c(1,1,1),
widths = c(0.5, 0.5,0.25))
```

**Supplemental Figure S4. History-congruence in logistic regression.**

A. To ensure that perceptual history played a significant role in perception despite the ongoing stream of external information, we tested whether human perceptual decision-making was better explained by the combination of external and internal information or, alternatively, by external information alone. To this end, we compared Aikake information criteria between logistic regression models that predicted trial-wise perceptual responses either by both current external sensory information and the preceding percept, or by external sensory information alone (values above zero indicate a superiority of the full model). With high consistency across the experiments selected from the Confidence Database, this model-comparison confirmed that perceptual history contributed significantly to perception (difference in AIC = `r mean(LogAggr$diff_AIC, na.rm = TRUE)`  ± `r sd(LogAggr$diff_AIC, na.rm = TRUE)/sqrt(length(LogAggr$diff_AIC))`, T($`r STAT.LogReg$coefficients[1,3]`$) = $`r STAT.LogReg$coefficients[1,4]`$, p = $`r STAT.LogReg$coefficients[1,5]`$).

B. Participant-wise regression coefficients amount to `r mean(LogAggr$beta_History, na.rm = TRUE)` ± `r sd(LogAggr$beta_History, na.rm = TRUE)/sqrt(length(LogAggr$beta_History))` for the effect of perceptual history and `r mean(LogAggr$beta_Stimulus, na.rm = TRUE)`  ± `r sd(LogAggr$beta_Stimulus, na.rm = TRUE)/sqrt(length(LogAggr$beta_Stimulus))` for external sensory stimulation. 

C. In mice, an AIC-based model comparison indicated that perception was better explained by logistic regression models that predicted trial-wise perceptual responses based on both current external sensory information and the preceding percept (difference in AIC = `r -mean(M_LogAggr$diff_AIC, na.rm = TRUE)`  ± `r sd(M_LogAggr$diff_AIC, na.rm = TRUE)/sqrt(length(M_LogAggr$diff_AIC))`, T($`r M_STAT.LogReg$parameter`$) = $`r M_STAT.LogReg$statistic`$, p = $`r M_STAT.LogReg$p.value`$).

D. In mice, individual regression coefficients amounted to `r mean(M_LogAggr$beta_History, na.rm = TRUE)` ± `r sd(M_LogAggr$beta_History, na.rm = TRUE)/sqrt(length(M_LogAggr$beta_History))` for the effect of perceptual history and `r mean(M_LogAggr$beta_Stimulus, na.rm = TRUE)`  ± `r sd(M_LogAggr$beta_Stimulus, na.rm = TRUE)/sqrt(length(M_LogAggr$beta_Stimulus))` for external sensory stimulation. 

\newpage
## Supplemental Figure S5

```{r Supplemental_Figure_S5, cache = TRUE}
##
## Visualize correction procedure
##
if (run_visualize_bias_sim){
source("./Functions/visualize_bias_correction.R",
local = knitr::knit_global())

n_trials = 100
levels = seq(0.6, 0.9, 0.1)
n_participants = 1000

Bias_Sim = data.frame()
for (level_idx in c(1:length(levels))) {
print(level_idx)
for (subj_idx in c(1:n_participants)) {
add_Bias_Sim <- visualize_bias_correction(n_trials, levels[level_idx])
add_Bias_Sim$subj_idx = subj_idx

Bias_Sim = rbind(Bias_Sim, add_Bias_Sim)
}
}

Bias_Sim$corrected <- Bias_Sim$acf - Bias_Sim$random


Sum_Bias_Sim <-  ddply(
Bias_Sim,
.(trial, bias_level),
summarise,

mean = mean(corrected, na.rm = TRUE),
error = sd(corrected, na.rm = TRUE) / sqrt(length(corrected))
)

Sum_Bias_Sim <- Sum_Bias_Sim[Sum_Bias_Sim$trial != 1,]
Sum_Bias_Sim$trial = Sum_Bias_Sim$trial-1
} else {Sum_Bias_Sim <- read.csv("./Results/Sum_Bias_Sim.csv")}

Sum_Bias_Sim$bias_level = paste("General response bias: ", as.character(Sum_Bias_Sim$bias_level), "%")

p_Sum_Bias_Sim <- ggplot() +
geom_errorbar(
data = Sum_Bias_Sim,
aes(
x = trial,
ymin = mean - error,
ymax = mean + error
),
width = 0.5,
alpha = 0.5,
size = 0.5,
color = "#377EB8"
) +
geom_line(
data = Sum_Bias_Sim,
aes(x = trial,
y = mean),
color = "#377EB8",
linetype = "dotted"
) +

geom_hline(
yintercept = 0,
linetype = "dashed",
color = "black",
size = 0.25
) +
theme_classic(base_size = 6) + labs(
x = "Lag (Trials)",
y =
paste("Autocorrelation coefficient: History congruence"),
subtitle = NULL
) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(
breaks = seq(1, 25, by = 4),
labels = seq(1, 25, by = 4),
limits = c(0, 25)
) + ylim(-0.01, 0.035) + facet_wrap( ~ bias_level)
p_Sum_Bias_Sim
```

**Supplemental Figure S5. Correcting for general response biases.**

Here, we ask whether the autocorrelation of history-congruence (as shown in Figure 2-3C) may be driven by general response biases (i.e., a general propensity to choose one of the two possible outcomes more frequently than the alternative). To this end, we generated sequences of `r max(Sum_Bias_Sim$trial) + 1` perceptual choices with general response biases ranging from 60 to 90% for 1000 simulated participants each. We then computed the autocorrelation of history-congruence for these simulated data. Crucially, we used the correction procedure that is applied to all autocorrelation curves shown in this manuscript: All reported autocorrelation coefficients are computed relative to the average autocorrelation coefficients obtained for `r n_permutations` iterations of randomly permuted trial sequences. The above simulation show that this correction procedure removes any potential contribution of general response biases to the auto-correlation of history-congruence. This indicates that the autocorrelation of history-congruence (as shown in Figure 2-3C) is not driven by general response biases that were present in the empirical data at a level of `r mean(Behav$Bias, na.rm = TRUE)`% ± `r sd(Behav$Bias, na.rm = TRUE)/sqrt(length(Behav$Bias))`% in humans and `r mean(M_Behav$Bias, na.rm = TRUE)`% ± `r sd(M_Behav$Bias, na.rm = TRUE)/sqrt(length(M_Behav$Bias))`% in mice. 

\newpage
## Supplemental Figure S6

```{r Supplemental_Figure_S6, cache = TRUE}
p_training_mouse <- ggplot() +
  geom_point(data = M_Behav_training_long[M_Behav_training_long$Variable != "RT",],
    aes(
      x = session_id,
      y = Frequency,
      fill = Variable,
      color = Variable),
    alpha = 0.1,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Summary_M_Behav_training_long[Summary_M_Behav_training_long$Variable != "RT",],
    aes(
      x =  session_id,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.75, alpha = 1, size = 0.5) +
   
  geom_hline(
    yintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Session",  y = "Frequency", subtitle = NULL, color = "Congruence to") + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "top") + ylim(40, 100) + xlim(0,65) + facet_wrap(~Variable) + theme(strip.text.x = element_blank())

p_training_mouse_RT <- ggplot() +
  geom_point(data = M_Behav_training_long[M_Behav_training_long$Variable == "RT",],
    aes(
      x = session_id,
      y = Frequency),
    color = "#4DAF4A",
    alpha = 0.1,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Summary_M_Behav_training_long[Summary_M_Behav_training_long$Variable == "RT",],
    aes(
      x =  session_id,
      ymin = Mean - Error,
      ymax = Mean + Error), color = "#4DAF4A", position = position_dodge(width = 0.5), width = 0.75, alpha = 1, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Session",  y = "TD (ms)", subtitle = NULL, color = "Congruence to") + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "top") + xlim(0,65) + ylim(1000,4000) + theme(strip.text.x = element_blank())

lay <- rbind(c(1,2))
grid.arrange(
p_training_mouse,p_training_mouse_RT,
layout_matrix = lay,
heights = c(1),
widths = c(2, 1))
```

**Supplemental Figure S6. History-/stimulus-congruence and TDs during training of the basic task.**

Here, we depict the progression of history- and stimulus-congruence (depicted in blue and red, respectively; left panel) as well as TDs (in green; right panel) across training sessions in mice that achieved proficiency (i.e., stimulus-congruence $\geq$ 80%) in the *basic* task of the IBL dataset.
We found that both history-congruent perceptual choices ($\beta$ = $`r M_STAT.lmer_History_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_History_training$coefficients[2,2]`$, T($`r M_STAT.lmer_History_training$coefficients[2,3]`$) = $`r M_STAT.lmer_History_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_History_training$coefficients[2,5]`$) 
and stimulus-congruent perceptual choices ($\beta$ = $`r M_STAT.lmer_Stimulus_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_Stimulus_training$coefficients[2,2]`$, T($`r M_STAT.lmer_Stimulus_training$coefficients[2,3]`$) = $`r M_STAT.lmer_Stimulus_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$) became more frequent with training. 
As in humans, mice showed shorter TDs with increase exposure to the task ($\beta$ = $`r M_STAT.lmer_RT_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_RT_training$coefficients[2,2]`$, T($`r M_STAT.lmer_RT_training$coefficients[2,3]`$) = $`r M_STAT.lmer_RT_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$).


\newpage
## Supplemental Figure S7

```{r prepare_Supplemental_Figure_S7_8_9_10, cache = TRUE}

if (!load_summary_data) {
  
  ##
## run control simulation
##

if (run_control_simulation) {
  
n_participants = nrow(Optim)
n = runif(n_participants, 300, 700)#500

# outcomes
outcomes = c(0,1)

source("./Functions/simulation_glaze_osc_human_zeta.R", local = knitr::knit_global())

type_list = c("glaze_no_osc", "glaze_only_p_osc", "glaze_only_l_osc", "glaze_osc_no_H")

Control_Sim <- data.frame()
for (type_idx in c(4)){
for (subj_idx in c(1:n_participants)){

  if ((subj_idx %% 10) == 0){print(subj_idx)}
  
  random_steps = rnorm(n = n[subj_idx], mean = 0, sd = 0)
  
  if (type_list[type_idx] == "glaze_no_osc") {
  Control_Sim_add <-
  simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  Optim$p1[subj_idx],
  Optim$p2[subj_idx],
  0,
  0,
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )}
  else if (type_list[type_idx] == "glaze_only_l_osc") {
  Control_Sim_add <-
   simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  Optim$p1[subj_idx],
  Optim$p2[subj_idx],
  0,
  Optim$p4[subj_idx],
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )}
  else if (type_list[type_idx] == "glaze_only_p_osc") {
  Control_Sim_add <-
   simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  Optim$p1[subj_idx],
  Optim$p2[subj_idx],
  Optim$p3[subj_idx],
  0,
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )}
  else if (type_list[type_idx] == "glaze_osc_no_H") {
  Control_Sim_add <-
  simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  0.5,
  Optim$p2[subj_idx],
  Optim$p3[subj_idx],
  Optim$p4[subj_idx],
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )
  } else {
  }
  
  Control_Sim_add$diff_acf_Stimulus <- exclude_3SD(Control_Sim_add$acf_Stimulus - Control_Sim_add$random_acf_Stimulus) 
  Control_Sim_add$diff_acf_History <- exclude_3SD(Control_Sim_add$acf_History - Control_Sim_add$random_acf_History) 
  Control_Sim_add$subject_id = subj_idx
  Control_Sim_add$Trial = c(1:n[subj_idx])
  Control_Sim_add$H = Optim$p1[subj_idx]
  Control_Sim_add$prec = Optim$p2[subj_idx]
  Control_Sim_add$amp = Optim$p3[subj_idx]
  Control_Sim_add$amp_LLR = Optim$p4[subj_idx]
  Control_Sim_add$frequency = Optim$p5[subj_idx]
  Control_Sim_add$phase =  Optim$p6[subj_idx]
  Control_Sim_add$zeta =  Optim$p7[subj_idx]
  Control_Sim_add$random_steps = random_steps
  Control_Sim_add$type = type_list[type_idx]
  
  Control_Sim = rbind(Control_Sim, Control_Sim_add)
}
}

} else {
Control_Sim <- read.csv(paste(root, "Control_Sim_param_variable_n_human_zeta_corr_smaller.csv", sep = ""))
#Control_Sim <- read.csv(paste(root, "Control_Sim_param_variable_n_human.csv", sep = ""))
type_list = unique(Control_Sim$type)

}

Control_Sim_Behav <- 
  ddply(
    Control_Sim, 
    .(subject_id,type),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100, 
    # Min_History = min(History_slider, na.rm = TRUE)*100,
    # Max_History = max(History_slider, na.rm = TRUE)*100,
    Accuracy = sum(Accuracy, na.rm = TRUE)/length(Accuracy)*100
    # Min_Accuracy = min(Accuracy_slider, na.rm = TRUE)*100,
    # Max_Accuracy = max(Accuracy_slider, na.rm = TRUE)*100
  )

Control_Sim_Behav_diff <- 
   ddply(
    Control_Sim[!is.na(Control_Sim$Accuracy),], 
    .(subject_id, Accuracy, type),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100
  )

Control_Sim_Behav$null_History = Control_Sim_Behav$History - 50

if (compute_simulation_control_group_acf) {
source("./Functions/f_compute_sim_group_acf.R", local = knitr::knit_global())

acf_to_test = c('diff_acf_Stimulus', 'diff_acf_History')
max_trial = 99
Control_Sim_Summary_acf <- f_compute_sim_group_acf(Control_Sim, acf_to_test, max_trial, type_list)
} else {
#Control_Sim_Summary_acf <-  read.csv("./Results/Control_Sim_Summary_acf_param_variable_n_human_zeta.csv")
Control_Sim_Summary_acf <-  read.csv("./Results/Control_Sim_Summary_acf_param_variable_n_human_zeta_corr.csv")
}
Control_Sim_Summary_acf$Trial <- Control_Sim_Summary_acf$Trial - 1


Control_ID_Sim_Summary_acf <- ddply(
    Control_Sim, 
    .(subject_id,type),
    summarise,
    lag_significant_Stimulus = min(which(diff(which(c(0, exceed_acf_Stimulus,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_History = min(which(diff(which(c(0, exceed_acf_History,0) < n_permutations/2)) != 1)) - 1
  )
Control_ID_Sim_Summary_acf$lag_significant_Stimulus[Control_ID_Sim_Summary_acf$lag_significant_Stimulus == Inf] = NA
Control_ID_Sim_Summary_acf$lag_significant_History[Control_ID_Sim_Summary_acf$lag_significant_History == Inf] = NA

if (compute_power_spectra_simulation) {
source("./Functions/f_compute_power_spectra_sim_control.R", local = knitr::knit_global())

sliders = c("Accuracy_slider", "History_slider")  
Control_Sim_Power_Spectra <- f_compute_power_spectra_sim_control(Control_Sim, sliders, type_list)  
} else {
  Control_Sim_Power_Spectra <- read.csv(paste(root, "Control_Sim_power_spectra_smoothed_param_variable_n_human_zeta_corr.csv", sep = ""))
}

Control_Sim_Power_Spectra$r_freq = round(Control_Sim_Power_Spectra$freq, digits = 4)

step_size = 0.01
bins <- seq(from = min(Control_Sim_Power_Spectra$freq), to = max(Control_Sim_Power_Spectra$freq), by = step_size)
names <- round(bins, digits = 2)
names = names[1:length(names) - 1]

Control_Sim_Power_Spectra$bin_freq <- cut(Control_Sim_Power_Spectra$freq, breaks = bins, labels = names)
Control_Sim_Power_Spectra$bin_freq <- as.numeric(Control_Sim_Power_Spectra$bin_freq) * step_size
Control_Sim_Power_Spectra$bin_freq[is.na(Control_Sim_Power_Spectra$bin_freq)] = max(Control_Sim_Power_Spectra$freq)

Control_Sim_Power_Spectra$Coherence <- Control_Sim_Power_Spectra$Coherence * 100



gathercol = colnames(Control_Sim_Power_Spectra[, c(2,3)])
Control_Sim_Power_Spectra_long  <-
gather(Control_Sim_Power_Spectra[, c(1,2,3, 6,7,8)],
"Variable",
"Power",
gathercol,
factor_key = TRUE)

Control_Sim_Power_Frequency <-  ddply(
      Control_Sim_Power_Spectra_long,
      .(r_freq, Variable, type),
      summarise,
      mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
      ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE)/sqrt(length(exclude_3SD(Power)))
 )

Control_Sim_Coherence_Phase_Frequency <- ddply(
      Control_Sim_Power_Spectra,
      .(bin_freq,type),
      summarise,
    
      mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
      ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE)/sqrt(length(exclude_3SD(Coherence))),
      
      mode_phase = getmode(exclude_3SD((abs(Phase)))),
      mean_phase = getmode(exclude_3SD((abs(Phase)))),
      ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE)/sqrt(length(exclude_3SD(Phase)))
 )

##
## mode of coherence and phase
##
Control_Sim_Summary_Power_Spectra <- 
   ddply(
 Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$freq > 0.01 & Control_Sim_Power_Spectra$freq < 0.1,],
       .(subject_id, type),
       summarise,
       mean_coherence = mean(exclude_3SD((
 Coherence))),
       mode_phase = getmode(exclude_3SD((
 abs(Phase))))
   )

##
## Simulated modes
##
Control_Sim_Mode_gather <-  ddply(
  Control_Sim,
  .(subject_id),
  summarise,
  
  directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
  scaled_directed_mode = scale(directed_mode),
  strength_mode = abs(scaled_directed_mode)
)

Control_Sim$directed_mode = Control_Sim_Mode_gather$directed_mode
Control_Sim$scaled_directed_mode = Control_Sim_Mode_gather$scaled_directed_mode
Control_Sim$strength_mode = Control_Sim_Mode_gather$strength_mode

Control_Sim_ID_mode <-  ddply(
  Control_Sim,
  .(subject_id, type),
  summarise,
  strength_mode = mean(strength_mode, na.rm = TRUE),
  directed_mode = mean(directed_mode, na.rm = TRUE)
)


##
## Simulated confidence
##

Control_Sim$Confidence <- abs(Control_Sim$mu) 
Control_Sim$scaled_Confidence <- scale(Control_Sim$Confidence)
Control_Sim$clear_Confidence <- Control_Sim$Confidence
Control_Sim[Control_Sim$clear_Confidence > median(Control_Sim$clear_Confidence, na.rm = TRUE) + 3*median(Control_Sim$clear_Confidence, na.rm = TRUE),]$clear_Confidence <- NA

Control_Sim_Post_Perceptual_Modes = ddply(
 Control_Sim[!is.na(Control_Sim$History) & !is.na(Control_Sim$Accuracy) & !is.na(Control_Sim$clear_Confidence),], 
  .(directed_mode, type),
  summarise,
  average_Confidence = mean(clear_Confidence, na.rm = TRUE),
  se_Confidence = sd(clear_Confidence, na.rm = TRUE)/sqrt(length(clear_Confidence)),
  n = length(mu),
  n_percent = (sum(!is.na(mu))/nrow(Control_Sim))*100
)

##
## Confidence vs mode: Prepare plot and stats
##
Control_Sim_Confidence_Behav <- 
  ddply(
    Control_Sim[!is.na(Control_Sim$History) & !is.na(Control_Sim$Accuracy) & !is.na(Control_Sim$clear_Confidence),], 
    .(subject_id, type),
    summarise,
    diff_Confidence_History = mean(clear_Confidence[History == 1], na.rm = TRUE) -  mean(clear_Confidence[History == 0], na.rm = TRUE),
    diff_Confidence_Stimulus = mean(clear_Confidence[Accuracy == 1], na.rm = TRUE) -  mean(clear_Confidence[Accuracy == 0], na.rm = TRUE)
    )


gathercol = colnames(Control_Sim_Confidence_Behav[, c(3,4)])
Control_Sim_Confidence_Behav_long  <-
  gather(Control_Sim_Confidence_Behav,
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)

Control_Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_History", "History", Control_Sim_Confidence_Behav_long$Variable)
Control_Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_Stimulus", "Stimulus", Control_Sim_Confidence_Behav_long$Variable)

# Sim_diff_Confidence <- lmer(diff ~ Variable + (1|subject_id), data = Sim_Confidence_Behav_long)
# Sim_STAT.diff_Confidence <- summary(Sim_diff_Confidence)

## exclude low performance mice
Control_Summary_Sim_Confidence_Behav <- 
  ddply(
    Control_Sim_Confidence_Behav_long, 
    .(Variable),
    summarise,
    Mean = mean(diff, na.rm = TRUE),
    Error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))

# Sim_lmer_Confidence_Accuracy_History <- lmer(clear_Confidence ~ Accuracy + History + (1|subject_id), data = Sim)
# Sim_STAT.lmer_Confidence_Accuracy_History <- summary(Sim_lmer_Confidence_Accuracy_History)



#
## Plots
##
for (type_idx in c(1:length(type_list))){

gathercol <- colnames(Control_Sim_Behav[,c(3,4)])

Control_Sim_Behav_long  <-
  gather(Control_Sim_Behav[, c(1,2,3,4)],
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Control_Sim_Behav_long$Frequency <-
  gsub("Accuracy", "Stimulus", Control_Sim_Behav_long$Frequency)

Control_sim_p_distribution_Behav <- ggplot(Control_Sim_Behav_long[Control_Sim_Behav_long$type == type_list[type_idx],], 
                                  aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none") 

##
## Probability of History-congruence depending on perceptual accuracy
##

gathercol <- colnames(Control_Sim_Behav_diff[4])

Control_Sim_Behav_diff_long  <-
  gather(Control_Sim_Behav_diff,
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Control_Sim_Behav_diff_long$Accuracy <- as.character(Control_Sim_Behav_diff_long$Accuracy) 
Control_Sim_Behav_diff_long$Accuracy <-
  gsub("0", "error", Control_Sim_Behav_diff_long$Accuracy)
Control_Sim_Behav_diff_long$Accuracy <-
  gsub("1", "correct", Control_Sim_Behav_diff_long$Accuracy)

Control_sim_p_distribution_Behav_diff <- ggplot(Control_Sim_Behav_diff_long[Control_Sim_Behav_diff_long$type == type_list[type_idx],], aes(x = Frequency, color = -Accuracy, fill = Accuracy)) + 
  # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "bottom")


##
## Acf Plot
##
Plot_Acf_Data =  Control_Sim_Summary_acf[(Control_Sim_Summary_acf$Variable == "diff_acf_Stimulus" | Control_Sim_Summary_acf$Variable == "diff_acf_History") ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

# Plot_Acf_Data$type <- gsub("glaze_no_osc", "B. No oscillation", Plot_Acf_Data$type)
# Plot_Acf_Data$type <- gsub("glaze_only_l_osc", "D. No oscillation of the prior", Plot_Acf_Data$type)
# Plot_Acf_Data$type <- gsub("glaze_only_p_osc", "C. No oscillation of the likelihood", Plot_Acf_Data$type)
# Plot_Acf_Data$type <- gsub("glaze_osc_no_H", "A. No accumulation of evidence", Plot_Acf_Data$type)


Control_sim_p_acf <- ggplot() +
  geom_point(
  data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx],],
  aes(
  x = Trial,
  y = Mean,
  fill = Variable,
  color = Variable
  ),
  alpha = 0.8,
  size = 1,
  fill = "white",
  position = position_dodge(width = 0.5)
  ) + geom_point(data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx] & Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.01,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  geom_errorbar(
  data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx],],
  aes(
  x = Trial,
  ymin = Mean - Error,
  ymax = Mean + Error,
  fill = Variable,
  color = Variable
  ),
  position = position_dodge(width = 0.5),
  width = 0.5,
  alpha = 0.5,
  size = 0.5
  ) + geom_line(
  data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx],],
  aes(
  x = Trial,
  y = Mean,
  fill = Variable,
  color = Variable
  ),
  position = position_dodge(width = 0.5),
  linetype = "dotted"
  ) +
  
  geom_hline(
  yintercept = 0,
  linetype = "dashed",
  color = "black",
  size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(
  x = "Lag (Trials)",
  y =
  paste("Autocorrelation coefficient"),
  subtitle = "B",
  color = "Congruence to",
  shape = "p value"
  ) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.35, 0.9), legend.box = "horizontal") + ylim(-0.01, 0.03) + scale_x_continuous(
  breaks = seq(1, 25, by = 4),
  labels = seq(1, 25, by = 4),
  limits = c(-1, 25)
  ) 


gathercol <- colnames(Control_ID_Sim_Summary_acf[,c(3,4)])

Control_ID_Sim_Summary_acf_long  <-
  gather(Control_ID_Sim_Summary_acf,
         "Variable",
         "Lag",
         gathercol,
         factor_key = TRUE)

Control_sim_p_exceed_acf <- ggplot(Control_ID_Sim_Summary_acf_long[Control_ID_Sim_Summary_acf_long$type == type_list[type_idx],], aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
  scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())

Control_Sim_p_power_frequency <-
  ggplot(data = Control_Sim_Power_Frequency[Control_Sim_Power_Frequency$type == type_list[type_idx] & Control_Sim_Power_Frequency$r_freq > 0.01 & Control_Sim_Power_Frequency$r_freq < 0.09,],
         mapping = aes(x = (r_freq), y = (mean_power), ymin = (mean_power - ci_power), ymax = (mean_power - ci_power), color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.5, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
                                                level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.2) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency",  y = "Spectral Density", subtitle = "D") 

Control_Sim_p_density_phase <-
  ggplot(Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$type == type_list[type_idx] & Control_Sim_Power_Spectra$r_freq > 0.01 & Control_Sim_Power_Spectra$r_freq < 0.09,], aes(
    x = abs(Phase)
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + 
  labs(x = "Phase",  y = "Density", subtitle = "E") + theme_classic(base_size = 6) + 
  theme(legend.position = "none") +
  geom_vline(xintercept = pi, linetype = "dashed",
             color = "black",
             size = 0.25) #+ xlim(-0.2, 3.4)

Control_Sim_p_density_coherence <-
  ggplot(Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$type == type_list[type_idx] & Control_Sim_Power_Spectra$bin_freq < 0.09,], aes(
    x = Coherence
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + 
  labs(x = "Squared coherence (%)",  y = "Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position = "none") + 
  geom_vline(xintercept = mean(Control_Sim_Power_Spectra[Control_Sim_Power_Frequency$r_freq > 0.01 & Control_Sim_Power_Frequency$r_freq < 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",color = "black", size = 0.25) + xlim(-1, 26)

Control_sim_p_Confidence_mode <- ggplot(data = Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type  == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5,], aes(
  x = directed_mode,
  y = average_Confidence,
  ymin = average_Confidence - se_Confidence,
  ymax = average_Confidence + se_Confidence)) +
  geom_point(
    alpha = 0.5,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y = "Simulated posterior certainty", subtitle = "H")  + 
  scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  
  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + 
  stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = min(Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type  == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5 & Control_Sim_Post_Perceptual_Modes$average_Confidence == min(Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type  == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + ylim(0.15, 0.45) 

Control_sim_p_Confidence <- ggplot() +
  geom_point(data = Control_Sim_Confidence_Behav_long[Control_Sim_Confidence_Behav_long$type == type_list[type_idx],],
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Control_Summary_Sim_Confidence_Behav[Control_Summary_Sim_Confidence_Behav$type == type_list[type_idx],],
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff posterior certainty (zscore)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.1, 0.25) 

##
## store plots
##
assign(paste("Control_sim_p_distribution_Behav", type_list[type_idx], sep = "_"), Control_sim_p_distribution_Behav)  
assign(paste("Control_sim_p_acf", type_list[type_idx], sep = "_"), Control_sim_p_acf )  
assign(paste("Control_sim_p_exceed_acf", type_list[type_idx], sep = "_"), Control_sim_p_exceed_acf)  
assign(paste("Control_Sim_p_power_frequency", type_list[type_idx], sep = "_"), Control_Sim_p_power_frequency)  
assign(paste("Control_Sim_p_density_phase", type_list[type_idx], sep = "_"), Control_Sim_p_density_phase)  
assign(paste("Control_Sim_p_density_coherence", type_list[type_idx], sep = "_"), Control_Sim_p_density_coherence)  
assign(paste("Control_sim_p_Confidence_mode", type_list[type_idx], sep = "_"), Control_sim_p_Confidence_mode)  
assign(paste("Control_sim_p_distribution_Behav_diff", type_list[type_idx], sep = "_"), Control_sim_p_distribution_Behav_diff)  
assign(paste("Control_sim_p_Confidence", type_list[type_idx], sep = "_"), Control_sim_p_Confidence)  

##
## do and store stats
##

## History-Effect
Control_Sim_STAT.Global_History_Accuracy = t.test(Control_Sim_Behav[Control_Sim_Behav$type == type_list[type_idx],]$null_History)

compare_participants = which(Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 1,]$subject_id %in% Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 0,]$subject_id)

Control_Sim_STAT.diff_History_Accuracy = t.test(Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 0 & Control_Sim_Behav_diff$subject_id %in% compare_participants,]$History, Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 1 & Control_Sim_Behav_diff$subject_id %in% compare_participants,]$History, paired = TRUE)

assign(paste("Control_Sim_STAT.Global_History_Accuracy", type_list[type_idx], sep = "_"), Control_Sim_STAT.Global_History_Accuracy) 
assign(paste("Control_Sim_STAT.diff_History_Accuracy", type_list[type_idx], sep = "_"), Control_Sim_STAT.diff_History_Accuracy) 

## exceedance of autocorrelations
Control_Sim_STAT.exc_autocorrelationt_Stimulus = t.test(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[type_idx],]$lag_significant_Stimulus, 
       ID_Sim_Summary_acf$lag_significant_Stimulus, paired = TRUE)
Control_Sim_STAT.exc_autocorrelationt_History = t.test(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[type_idx],]$lag_significant_History, 
       ID_Sim_Summary_acf$lag_significant_History, paired = TRUE)

assign(paste("Control_Sim_STAT.exc_autocorrelationt_Stimulus", type_list[type_idx], sep = "_"), Control_Sim_STAT.exc_autocorrelationt_Stimulus) 
assign(paste("Control_Sim_STAT.exc_autocorrelationt_History", type_list[type_idx], sep = "_"), Control_Sim_STAT.exc_autocorrelationt_History) 

## STATS Power vs Frequency (1/f noise)
Control_Sim_lmer_power_freq_Stimulus <- lmer(log(Power_Stimulus) ~ log(freq) + (1|subject_id), data = Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$freq > 0.01 & Control_Sim_Power_Spectra$freq < 0.1 & is.finite(log(Control_Sim_Power_Spectra$Power_Stimulus)) & Control_Sim_Power_Spectra$type == type_list[type_idx],])
Control_Sim_STAT.lmer_power_freq_Stimulus <- summary(Control_Sim_lmer_power_freq_Stimulus)

Control_Sim_lmer_power_freq_History <- lmer(log(Power_History) ~ log(freq) + (1|subject_id), data = Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$freq > 0.01 & Control_Sim_Power_Spectra$freq < 0.1 & is.finite(log(Control_Sim_Power_Spectra$Power_History)) & Control_Sim_Power_Spectra$type == type_list[type_idx],])
Control_Sim_STAT.lmer_power_freq_History <- summary(Control_Sim_lmer_power_freq_History)

assign(paste("Control_Sim_STAT.lmer_power_freq_Stimulus", type_list[type_idx], sep = "_"), Control_Sim_STAT.lmer_power_freq_Stimulus) 
assign(paste("Control_Sim_STAT.lmer_power_freq_History", type_list[type_idx], sep = "_"), Control_Sim_STAT.lmer_power_freq_History) 

## Difference in coherence
Control_Sim_STAT.coherence = t.test(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[type_idx],]$mean_coherence, 
       Sim_Summary_Power_Spectra$mean_coherence, paired = TRUE)
assign(paste("Control_Sim_STAT.coherence", type_list[type_idx], sep = "_"), Control_Sim_STAT.coherence) 

## Effects of History and Accuracy on Confidence
Control_Sim_lmer_Confidence_Accuracy_History <- lmer(clear_Confidence ~ Accuracy + History + (1|subject_id), data = Control_Sim[Control_Sim$type == type_list[type_idx],])
Control_Sim_STAT.lmer_Confidence_Accuracy_History <- summary(Control_Sim_lmer_Confidence_Accuracy_History)

assign(paste("Control_Sim_STAT.lmer_Confidence_Accuracy_History", type_list[type_idx], sep = "_"), Control_Sim_STAT.lmer_Confidence_Accuracy_History)

## History vs Accuracy
Control_Sim_slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|subject_id), data = Control_Sim[Control_Sim$type == type_list[type_idx],])
Control_Sim_STAT.slider_History_vs_Accuracy <- summary(Control_Sim_slider_History_vs_Accuracy)

assign(paste("Control_Sim_STAT.slider_History_vs_Accuracy", type_list[type_idx], sep = "_"), Control_Sim_STAT.slider_History_vs_Accuracy)

## Confidence vs mode
Control_Sim_Confidence_vs_mode <- lmer(clear_Confidence ~ poly(directed_mode, 2) + (1|subject_id), data = Control_Sim[Control_Sim$type == type_list[type_idx],])
Control_Sim_STAT.Confidence_vs_mode <- summary(Control_Sim_Confidence_vs_mode)
assign(paste("Control_Sim_STAT.Confidence_vs_mode", type_list[type_idx], sep = "_"),Control_Sim_STAT.Confidence_vs_mode)

# lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
# p <- grid.arrange(
# Control_sim_p_distribution_Behav, Control_sim_p_acf, Control_sim_p_exceed_acf,
# Control_Sim_p_power_frequency, Control_Sim_p_density_phase, Control_Sim_p_density_coherence,
# Control_sim_p_Confidence_mode, Control_sim_p_distribution_Behav_diff, Control_sim_p_Confidence,
# layout_matrix = lay,
# heights = c(0.5, 0.5, 0.4,0.4),
# widths = c(0.3,0.2, 0.225,0.3))
}
               
    
  
  if (save_summary_data) {
save(
Control_Sim_Behav, Control_ID_Sim_Summary_acf, Control_Sim_Summary_Power_Spectra, type_list, file = "./Summary_Data/Control_Sim_global.Rdata")

      save( 
Control_sim_p_acf_glaze_osc_no_H,                               
Control_sim_p_Confidence_glaze_osc_no_H,                         
Control_sim_p_Confidence_mode_glaze_osc_no_H,                    
Control_Sim_p_density_coherence_glaze_osc_no_H,                  
Control_Sim_p_density_phase_glaze_osc_no_H,                      
Control_sim_p_distribution_Behav_diff_glaze_osc_no_H,            
Control_sim_p_distribution_Behav_glaze_osc_no_H,                 
Control_sim_p_exceed_acf_glaze_osc_no_H,                         
Control_Sim_p_power_frequency_glaze_osc_no_H,                    
Control_Sim_STAT.coherence_glaze_osc_no_H,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H,              
Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H,   
Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H,
Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H,  
file = "./Summary_Data/Control_Sim_osc_no_H.Rdata")
      
            save( 
Control_sim_p_acf_glaze_no_osc,                               
Control_sim_p_Confidence_glaze_no_osc,                         
Control_sim_p_Confidence_mode_glaze_no_osc,                    
Control_Sim_p_density_coherence_glaze_no_osc,                  
Control_Sim_p_density_phase_glaze_no_osc,                      
Control_sim_p_distribution_Behav_diff_glaze_no_osc,            
Control_sim_p_distribution_Behav_glaze_no_osc,                 
Control_sim_p_exceed_acf_glaze_no_osc,                         
Control_Sim_p_power_frequency_glaze_no_osc,                    
Control_Sim_STAT.coherence_glaze_no_osc,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc,              
Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc,   
Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc,
Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc,  
file = "./Summary_Data/Control_Sim_no_osc.Rdata")
            
                        save( 
Control_sim_p_acf_glaze_only_l_osc,                               
Control_sim_p_Confidence_glaze_only_l_osc,                         
Control_sim_p_Confidence_mode_glaze_only_l_osc,                    
Control_Sim_p_density_coherence_glaze_only_l_osc,                  
Control_Sim_p_density_phase_glaze_only_l_osc,                      
Control_sim_p_distribution_Behav_diff_glaze_only_l_osc,            
Control_sim_p_distribution_Behav_glaze_only_l_osc,                 
Control_sim_p_exceed_acf_glaze_only_l_osc,                         
Control_Sim_p_power_frequency_glaze_only_l_osc,                    
Control_Sim_STAT.coherence_glaze_only_l_osc,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc,              
Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc,   
Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc,
Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc,  
file = "./Summary_Data/Control_Sim_only_l_osc.Rdata")
                        
save( 
Control_sim_p_acf_glaze_only_p_osc,                               
Control_sim_p_Confidence_glaze_only_p_osc,                         
Control_sim_p_Confidence_mode_glaze_only_p_osc,                    
Control_Sim_p_density_coherence_glaze_only_p_osc,                  
Control_Sim_p_density_phase_glaze_only_p_osc,                      
Control_sim_p_distribution_Behav_diff_glaze_only_p_osc,            
Control_sim_p_distribution_Behav_glaze_only_p_osc,                 
Control_sim_p_exceed_acf_glaze_only_p_osc,                         
Control_Sim_p_power_frequency_glaze_only_p_osc,                    
Control_Sim_STAT.coherence_glaze_only_p_osc,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc,              
Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc,   
Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc,
Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc,  
file = "./Summary_Data/Control_Sim_only_p_osc.Rdata")
  }
  
} else {
load("./Summary_Data/Control_Sim_global.Rdata")
load("./Summary_Data/Control_Sim_no_osc.Rdata")
load("./Summary_Data/Control_Sim_osc_no_H.Rdata")
load("./Summary_Data/Control_Sim_only_l_osc.Rdata")
load("./Summary_Data/Control_Sim_only_p_osc.Rdata")
}


```

```{r Supplemental_Figure_S7, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_osc_no_H, Control_sim_p_acf_glaze_osc_no_H, Control_sim_p_exceed_acf_glaze_osc_no_H,
Control_Sim_p_power_frequency_glaze_osc_no_H, Control_Sim_p_density_phase_glaze_osc_no_H, Control_Sim_p_density_coherence_glaze_osc_no_H,
Control_sim_p_Confidence_mode_glaze_osc_no_H, Control_sim_p_distribution_Behav_diff_glaze_osc_no_H, Control_sim_p_Confidence_glaze_osc_no_H,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S7. Reduced Control Model 1: No accumulation of information across trials.** When simulating data for the *no-accumulation model*, we removed the accumulation of information across trials by setting the Hazard rate $H$ to 0.5. Simulated data thus depended only on the participant-wise estimates for the amplitudes $a_{LLR/\psi}$, frequency $f$, phase $p$ and inverse decision temperature $\zeta$.    

A. Similar to the full model (Figure 6), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$History))`% of trials (in blue). In contrast to the full model, the no-accumulation model showed a significant bias against perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H$p.value`$; upper panel). 
In contrast to the full model, there was no difference in the frequency of history-congruent choices between correct and error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H$p.value`$; lower panel).

B. In the no-accumulation model, we found no significant autocorrelation of history-congruence beyond the first trial, whereas the autocorrelation of stimulus-congruence was preserved.

C. In the no-accumulation model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data increased with respect to stimulus-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H$p.value`$) 
and decreased with respect to history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H$p.value`$) relative to the full model.

D. In the no-accumulation model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,5]`$).

E. In the no-accumulation model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). In contrast to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were not significantly anti-correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,5]`$).

F. In the no-accumulation model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.coherence_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_osc_no_H$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[4],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[4],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[4],]$mean_coherence)`%.

G. Similar to the full model, confidence simulated from the no-accumulation model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,5]`$).
In contrast to the full model (Figure 6), history-congruent choices were not characterized by enhanced confidence ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,5]`$). 

H. In the no-accumulation model, the positive quadratic relationship between the mode of perceptual processing and confidence was markedly reduced in comparison to the full model ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Supplemental Figure S8

```{r Supplemental_Figure_S8, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_no_osc, Control_sim_p_acf_glaze_no_osc, Control_sim_p_exceed_acf_glaze_no_osc,
Control_Sim_p_power_frequency_glaze_no_osc, Control_Sim_p_density_phase_glaze_no_osc, Control_Sim_p_density_coherence_glaze_no_osc,
Control_sim_p_Confidence_mode_glaze_no_osc, Control_sim_p_distribution_Behav_diff_glaze_no_osc, Control_sim_p_Confidence_glaze_no_osc,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S8. Reduced Control Model 2: No oscillations.** When simulating data for the *no-oscillation model*, we removed the oscillation from the likelihood and prior terms by setting the amplitudes $a_{LLR}$ and $a_{\psi}$ to zero. Simulated data thus depended only on the participant-wise estimates for hazard rate $H$ and inverse decision temperature $\zeta$.    

A. Similar to the full model (Figure 6), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$History))`% of trials (in blue). As in the full model, the no-oscillation model showed a significant bias toward perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc$p.value`$; upper panel). 
Similarly, history-congruent choices were more frequent at error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc$p.value`$; lower panel).

B. In the no-oscillation model, we did not find significant autocorrelations for stimulus-congruence. Likewise, we did not observe any autocorrelation of history-congruence beyond the first three consecutive trials.

C. In the no-oscillation model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data decreased with respect to both stimulus-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc$p.value`$) 
and history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc$p.value`$) relative to the full model.

D. In the no-oscillation model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,5]`$).

E. In the no-oscillation model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). In contrast to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were positively correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,5]`$).

F. In the no-oscillation model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dottet line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_no_osc$parameter`) = `r Control_Sim_STAT.coherence_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_no_osc$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[1],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[1],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[1],]$mean_coherence)`%.

G. Similar to the full model, confidence simulated from the no-oscillation model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,5]`$)
and history-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,5]`$). 

H. In the no-oscillation model, the positive quadratic relationship between the mode of perceptual processing and confidence was markedly reduced in comparison to the full model ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Supplemental Figure S9

```{r Supplemental_Figure_S9, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_only_l_osc, Control_sim_p_acf_glaze_only_l_osc, Control_sim_p_exceed_acf_glaze_only_l_osc,
Control_Sim_p_power_frequency_glaze_only_l_osc, Control_Sim_p_density_phase_glaze_only_l_osc, Control_Sim_p_density_coherence_glaze_only_l_osc,
Control_sim_p_Confidence_mode_glaze_only_l_osc, Control_sim_p_distribution_Behav_diff_glaze_only_l_osc, Control_sim_p_Confidence_glaze_only_l_osc,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S9. Reduced Control Model 3: Only oscillation of the likelihood.** When simulating data for the *likelihood-oscillation-only model*, we removed the oscillation from the prior term by setting the amplitude $a_{\psi}$ to zero. Simulated data thus depended only on the participant-wise estimates for hazard rate $H$, amplitude $a_{LLR}$, frequency $f$, phase $p$ and inverse decision temperature $\zeta$.    

A. Similar to the full model (Figure 6), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$History))`% of trials (in blue). As in the full model, the likelihood-oscillation-only model showed a significant bias toward perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc$p.value`$; upper panel). 
Similarly, history-congruent choices were more frequent at error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc$p.value`$; lower panel).

B. In the likelihood-oscillation-only model, we observed that the autocorrelation coefficients for history-congruence were reduced below the autocorrelation coefficients of stimulus-congruence. This is an approximately five-fold reduction relative to the empirical results observed in humans (Figure 2B), where the autocorrelation of history-congruence was above the autocorrelation of stimulus-congruence. Moreover, in the reduced model shown here, the number of consecutive trials that showed significant autocorrelation of history-congruence was reduced to 11.

C. In the likelihood-oscillation-only model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data did not differ with respect to stimulus-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc$p.value`$), 
but decreased with respect to history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc$p.value`$) relative to the full model.

D. In the likelihood-oscillation-only model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,5]`$).

E. In the likelihood-oscillation-only model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). In contrast to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were positively correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,5]`$).

F. In the likelihood-oscillation-only model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dottet line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.coherence_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_only_l_osc$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[3],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[3],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[3],]$mean_coherence)`%.

G. Similar to the full model, confidence simulated from the likelihood-oscillation-only model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,5]`$)
and history-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,5]`$). 

H. In the likelihood-oscillation-only model, the positive quadratic relationship between the mode of perceptual processing and confidence was markedly reduced in comparison to the full model ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.


\newpage
## Supplemental Figure S10

```{r Supplemental_Figure_S10, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_only_p_osc, Control_sim_p_acf_glaze_only_p_osc, Control_sim_p_exceed_acf_glaze_only_p_osc,
Control_Sim_p_power_frequency_glaze_only_p_osc, Control_Sim_p_density_phase_glaze_only_p_osc, Control_Sim_p_density_coherence_glaze_only_p_osc,
Control_sim_p_Confidence_mode_glaze_only_p_osc, Control_sim_p_distribution_Behav_diff_glaze_only_p_osc, Control_sim_p_Confidence_glaze_only_p_osc,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S10. Reduced Control Model 4: Only oscillation of the prior.** When simulating data for the *prior-oscillation-only model*, we removed the oscillation from the prior term by setting the amplitude $a_{LLR}$ to zero. Simulated data thus depended only on the participant-wise estimates for hazard rate $H$, amplitude $a_{\psi}$, frequency $f$, phase $p$ and inverse decision temperature $\zeta$.    

A. Similar to the full model (Figure 6), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$History))`% of trials (in blue). As in the full model, the prior-oscillation-only showed a significant bias toward perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc$p.value`$; upper panel). 
Similarly, history-congruent choices were more frequent at error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc$p.value`$; lower panel).

B. In the prior-oscillation-only model, we did not observe any significant positive autocorrelation of stimulus-congruence , whereas the autocorrelation of history-congruence was preserved.

C. In the prior-oscillation-only model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data did was decreased with respect to stimulus-congruence relative to the full model (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc$p.value`$), 
but did not differ from the full model with respect to history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc$p.value`$).

D. In the prior-oscillation-only model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as **a scale-invariant process with a 1/f power law**, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,5]`$).

E. In the prior-oscillation-only model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). Similar to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were anti-correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,5]`$).

F. In the prior-oscillation-only model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dottet line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.coherence_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_only_p_osc$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[2],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[2],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[2],]$mean_coherence)`%.

G. Similar to the full model, confidence simulated from the prior-oscillation-only model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,5]`$)
and history-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,5]`$). 

H. In contrast to the full model, the prior-oscillation-only model did not yield a positive quadratic relationship between the mode of perceptual processing and confidence ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

<!-- \newpage -->
<!-- ## Supplemental Figure S10 -->

<!-- **Supplemental Figure S10. Reset-Rebounce.** Here, we show group-level autocorrelations for a reset-rebound-model[@Hermoso-Mendizabal2020] which assumes that errors cause perception to switch between two regimes: After an error, internal predictions become irrelevant for perceptual decision-making (*reset*: H = 0.5) until the agent makes a correct decision. After that, the agent restarts to accumulate sensory information across successive trials (*rebounce*: H $\neq$ 0.5), until the next errors occurs. Simulation based on this model did not reproduce any significant autocorrelation of stimulus- or history-congruence beyond the second or first trial, respectively.   -->

\newpage
## Supplemental Table T1

```{r Supplemental_Table, cache = TRUE}

library(kableExtra)
Tables <- Experiments[Study_Behav[!is.na(Study_Behav$study_id),]$study_id, c(3,4,5)]
row.names(Tables) <- NULL
kbl(Tables, format = "latex", longtable = T, booktabs = T) %>%
kable_styling(font_size = 7, latex_options = c("repeat_header"))
```

\newpage
# Response to Reviewers

## Reviewer 1:

**This was an interesting and thought-provoking submission. I note that it is a revision: I am therefore supposing that the authors have already responded to one round of reviewer comments and that you are potentially interested in publishing this work. In brief, I think there are many elements of this report that warrant publication; however, there are some parts that are less compelling and could be deferred to a subsequent paper. The paper is far too long and would benefit greatly from being streamlined. Furthermore, some of the modelling is overengineered and is difficult to follow. I have tried to suggest how the authors might improve the presentation of their work in my comments to authors.**

**I enjoyed reading this long but thought-provoking report of fluctuations in the sensitivity to sensory evidence in perceptual decision-making tasks. There were some parts of this report that were compelling and interesting. Other parts were less convincing and difficult to understand. Overall, this paper is far too long. An analogy that might help here is that a dinner guest is very entertaining for the first hour or so - and then overstays their welcome; until you start wishing they would leave. Another analogy, which came to mind, was that the modelling—and its interpretation—was a bit autistic (i.e., lots of fascinating if questionable detail with a lack of central coherence).**

**I think that both issues could be resolved by shortening the paper and removing (or, at least, greatly simplifying) the final simulation studies of metacognition. I try to unpack this suggestion in the following.**

We would like to thank Prof. Friston for the very insightful and helpful comments on our manuscript. We fully agree that our ideas about the computational function of between-mode fluctuations and the associated simulations may be presented in a more accessible form in a standalone paper. As we outlined in more detail below, we have have streamlined our findings and rewrote the paper to reduce it's length by shortening the sections on computational modeling. We have also followed Prof. Friston's suggestion to interpret the effects of mode on RT and confidence in the context of predictive processing definitions of attention, namely the allocation of precision between prior and likelihood. 

**Major points:**

**As I understand it, you have used publicly available data on perceptual decision-making to demonstrate slow fluctuations in the tendency to predicate perceptual decisions on the stimuli and on the history of recent decisions. You find scale-free fluctuations in this tendency — that are anti-correlated — and interpret this as fluctuations in the precision afforded sensory evidence, relative to prior beliefs. This interpretation is based upon a model of serial dependencies (parameterised with a hazard function).**

**The stimulus and history (i.e., likelihood and prior) sensitivities are anti-correlated and both show scale free behaviour. This is reproduced in men and mice. You then proceed to model this with periodic fluctuations in the precisions or weights applied to the likelihood and prior that are in anti-phase - and then estimate the parameters of the ensuing model. Finally, you then simulate the learning of the hazard parameter — and something called metacognition - to show that periodic fluctuations improve estimates of metacognition (based upon a Rescorla-Wagner model of learning). You motivate this by suggesting that the fluctuations in sensitivity are somehow necessary to elude circular inference and provide better estimates of precision.**

**Note that I am reading the parameters omega_LLR and omega_ψ as the precision of the likelihood and prior, where the precision of the likelihood is called sensory precision. This contrasts with your use of sensory precision, which seems to be attributed to a metacognitive construct M.**

**As noted above, all of this is fascinating but there are too many moving parts that do not fit together comfortably. I will list a few examples:**

### Comment 1: Done

**If, empirically, the fluctuations in sensitivity are scale-free with a 1/f power law, why did you elect to model fluctuations in precision as a periodic function with one unique timescale (i.e., f).?**

The reason for choosing a unique timescale $f$ was to enable our model to depict the the dominant timescale at which prior and likelihood precision are shifted against each other, giving rise to what we believe constitutes between-mode fluctuations. We think that identifying this timescale is important for planning future experiments targeted at between-mode fluctuations and their manipulation by causal interventions (e.g., pharmacology or TMS). In line with the shape of the autocorrelation curves, the value for $f$ lies at approximately 0.11 1/$N_{trials}$ in both humans and mice. The value of $f$ approximately matches the transition probabilities between *engaged* and *diseganged* states in work assessing flucuations in perceptual decision-making using Hidden Markov models (stay probabilities ranged between 0.94 and 0.98, see Ashwood et al., Nature Neuroscience 2022). Simulating from our model (Figure 6) replicates the 1/f feature of the empirical data. Please note that the individual trial is the smallest unit of *measurement* for these fluctuations, such that our analysis is limited y definition to frequencies below 1 (1/$N_{trials}$). 

We now provide a rationale for choosing one value for $f$ - identifying the dominant timescale for fluctuations in mode - in the results section:

- **To allow for *bimodal inference*, i..e, alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$, i.e., two anti-phase sine functions (defined by amplitude $a$, frequency $f$ and phase $p$) that determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)**

- **(...) The parameter $f$ captures the dominant time scale at which likelihood and prior precision were shifted against each other and was estimated at 0.11 1/$N_{trials}$ in both humans and mice.**

- Table 2 (see response to Comment X.X by Reviewer 1) contains an additional definition for all model parameters, including $f$. 

### Comment 2: Done

**At present, the estimates of meta-cognition (M) play the role of accumulated estimates of (sensory or prior) precision. Why are these not used in your model of perceptual decisions in Equation 2.**

We would like to thank Prof. Friston for this comment. In our model, the parameter $\alpha$ controls the encoding precision by governing the transformation from sensory stimuli to the log likelihood ratio (LLR) via the following equations (the LLR ends up closer closer to zero when $\alpha$ is low):  

\begin{equation}
u_t = \frac{1}{1 + exp(-\alpha * s_t)}
\end{equation}

\begin{equation}
LLR_t = log(\frac{u_t}{1-u_t})
\end{equation}

Our model simulations on the adaptive benefits of bimodal inference rest on the assumption that $\alpha$ may change unpredictably. The construct $M$ is a belief about $\alpha$ that may be useful for, e.g., communicating the precision of sensory encoding to other cognitive domains or agents. To our mind, $\alpha$ is a feature of low-level sensory encoding that cannot be modulated by top-down beliefs such as $M$. This is why we did not include $M$ in equation (2). 

### Comment 3: Done

**Why do you assume that non-specific increases in attention and arousal will increase reaction times? If one has very precise prior beliefs (and is not attending to stimuli), would you not expect a decrease in reaction time?**

Thanks a lot for pointing this out (see also the comment below and comment X.X.X by Reviewer 3). Both high prior and high likelihood precision lead to higher absolute values of the posterior log ratio (reflecting certainty encoded by the decision variable), and thus faster response times (RTs). This is reflected empirically by RTs in humans (Figure 2) and to a lesser degree in mice (Figure 4): RTs tended to be shorter for stronger biases toward both external and internal mode. Our model, which incorporates (i), the accumulation of information across trials, and (ii), fluctuations in the likelihood precision relative to the prior precision, recapitulates this feature of the data, which is lost or greatly attenuated when eliminating process (i) and/or (ii) (see model comparison and simulations below). Our data thus confirm the hypothesis that both high prior and likelihood precision lead to faster RTs. 

In the previous version of the manuscript, we had included the relation between mode and RTs and confidence primarily as a defensive analysis against the proposition that what we call between-mode fluctuations are not a perceptual phenomenon, but occur downstream of perception. One may imagine that fluctuations in perceptual performance are not influenced by periods of relative increases in prior precision (which decrease performance in fully randomized designs), but by periods when participants may not attend to the task at all, i.e., neither to sensory information nor to prior precision. We think that analysis of response times and confidence can give some insight into whether such alternative mechanisms may be at play, as we would assume longer response times and lower confidence if participants failed to attend to the task at all (e.g., due to low arousal).

We realize that, due to the potential non-linearity in their relation to arousal (see also comment X.X.X by Reviewer 3), RTs and confidence cannot provide a definitive map of where fluctuations in mode are situated in relation to arousal. Such a delineation may potentially be provided by tracking of pupil size, response behavior or by neural data (e.g., noise correlations of fluctuations in LFP). These data are not available for the studies in the Confidence Database, but were recently published for the IBL database. While we believe that this is beyond the scope of this manuscript, we will assess the relation of pupil diameter, motor behavior (turning of the response wheel) and LFPs to between-mode fluctuations in a future publication using the IBL dataset.

In the light of the considerations above and our response to comment X.X.X by Reviewer 3, we now refer to attention in the predictive processing sense. We now use the term "task engagement" instead of "on-task attention" to refer to situation in which participants may not attend to the task at all, e.g. due to low arousal or fatigue, and discuss these as alternative explanations for between-mode fluctuations. We have made three sets of changes to our manuscript: 

- First, we present our results on the relation of mode to RT and Confidence in a more descriptive way, and do not use it as a strong defensive analysis against: **The above results point to the existence of systematic fluctuations in the *decision variable*[@Kepecs2008] that determines perceptual choices, leading to enhanced sensitivity to external stimulus information during external mode and increased biases toward preceding choices during internal mode. As such, fluctuations in mode should influence downstream aspects of behavior and cognition that operate on the perceptual decision variable[@Kepecs2008]. To test this hypothesis with respect to motor behavior and metacognition, we asked how between-mode fluctuations relate to response times (RTs) and confidence reports.**

- **With respect to RTs, we observed faster responses for stimulus-congruent as opposed to stimulus-incongruent choices ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$; Figure 2G). Intriguingly, whilst controlling for the effect of stimulus-congruence, we found that history-congruent (as opposed to history-incongruent) choices were also characterized by faster responses ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$; Figure 2G).** 

- **When analyzing the speed of response against the mode of sensory processing (Figure 2H), we found that RTs were shorter during externally-oriented perception ($\beta_1$ = $`r STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r STAT.RT_vs_mode$coefficients[2,2]`$, T($`r STAT.RT_vs_mode$coefficients[2,3]`$) = $`r STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r STAT.RT_vs_mode$coefficients[2,5]`$). Crucially, as indicated by a quadratic relationship between the mode of sensory processing and RTs ($\beta_2$ = $`r STAT.RT_vs_mode$coefficients[3,1]`$ ± $`r STAT.RT_vs_mode$coefficients[3,2]`$, T($`r STAT.RT_vs_mode$coefficients[3,3]`$) = $`r STAT.RT_vs_mode$coefficients[3,4]`$, p = $`r STAT.RT_vs_mode$coefficients[3,5]`$), participants became faster at indicating their perceptual decision when biases toward both internal and external mode grew stronger.**

- **In analogy to the speed of response, confidence was higher for stimulus-congruent as opposed to stimulus-incongruent choices ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$; Figure 2I). Yet whilst controlling for the effect of stimulus-congruence, we found that history-congruence also increased confidence ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$; Figure 2I).**

- **When depicted against the mode of sensory processing (Figure 2J), subjective confidence was indeed enhanced when perception was more externally-oriented ($\beta_1$ = $`r STAT.Confidence_vs_mode$coefficients[2,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[2,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[2,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[2,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[2,5]`$). Importantly, however, participants were more confident in their perceptual decision for stronger biases toward both internal and external mode ($\beta_2$ = $`r STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[3,5]`$). In analogy to RTs, subjective confidence thus showed a quadratic relationship to the mode of sensory processing (Figure 2J).**

- **Consequently, our findings predict that human participants lack full metacognitive insight into how strongly external signals and internal predictions contribute to perceptual decision-making. Stronger biases toward perceptual history thus lead to two seemingly contradictory effects, more frequent errors (Supplemental Figure 1C) and increasing subjective confidence (Figure 2I-J). This observation generates an intriguing prediction regarding the association of between-mode fluctuations and perceptual metacognition: Metacognitive efficiency should be lower in individuals who spend more time in internal mode, since their confidence reports are less predictive of whether the corresponding perceptual decision is correct. We computed each participant's M-ratio[@Fleming2014] (meta-d'/d' = `r mean(Mode_Meta$meta_dprime_ratio, na.rm = TRUE)` ± `r sd(Mode_Meta$meta_dprime_ratio, na.rm = TRUE)/sqrt(length(Mode_Meta$meta_dprime_ratio))`) to probe this hypothesis independently of inter-individual differences in perceptual performance. Indeed, we found that biases toward internal information (i.e., as defined by the average probability of history-congruence) were stronger in participants with lower metacognitive efficiency ($\beta$ = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,1]`$ ± $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,2]`$, T($`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,3]`$) = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,4]`$, p = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,5]`$).**

- **In sum, the above results indicate that reporting behavior and metacognition do not map linearly onto the mode of sensory processing, suggesting that slow fluctuations in the respective impact of external and internal information are most likely to affect perception at an early level of sensory analysis[@St.John-Saaltink2016; @Cicchini2021]. Such low-level processing may thus integrate perceptual history with external inputs into a decision variable[@Kepecs2008] that influences not only perceptual choices, but also the speed and confidence at which they are made.** 

- **In what follows, we probe alternative explanations for between-mode fluctuations, test for the existence of modes in mice, and propose a predictive processing model that explains fluctuations in mode ongoing shifts in the precision afforded to external sensory information relative to internal predictions driven by perceptual history.**

- Second, we have re-written our discussion of the quadratic relationship of mode to RTs and Confidence, focusing on predictive coding models attention: **How does attention relate between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. As outlined above, between-mode fluctuations can be understood as ongoing shifts in the precision afforded to likelihood (*external mode*) and prior (*internal mode*), respectively. When the precision afforded to prior or likelihood increases, posterior precision increases, which leads to faster RTs and higher confidence. When defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may thus provide a plausible explanation for the quadratic relationship of mode to RTs and confidence (Figure 2H and J; Figure 4I, Figure 6I)**.

- Third, we have added a more general discussion of our findings in the light of fluctuations in task engagement: **Outside of the predictive processing field, attention is often understood in the context of engagement with the task[@Matthew2010], which may vary according the availability of cognitive resources that are modulated by factors such as tonic arousal, fatigue or familiarity with the task. Importantly, our results indicate that internal mode processing can be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, internal mode was not associated with a general increase in lapse (Supplemental Figures SX), arguing against the proposition that internal mode is associated with random or stereotypical responses that may reflect low task engagement.** 

- **In interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects task performance[@Yerkes1908]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. Of note, high phasic arousal has been shown to suppress biases in decision-making in humans and mice across domains[@deGee2014; @deGee2017; @deGee2020], including biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. Future work should therefore more direct measures of arousal (such as pupil size[@deGee2014; @Urai2017; @deGee2020; @deGee2022; @Beerendonk2023], motor behavior[@deGee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.**

### Comment 4: Done

**In the predictive processing literature, attention is thought to correspond to fluctuations in sensory and prior precision. Why did you then consider attention as some additional or unrelated confound?**

This point is closely related to the comment above. We realize that, in the predictive coding field, attention is equated with the precision of probability distributions that contribute to the perceptual decision, such that an observer can attend strongly to sensory information (high likelihood precision) or to internal predictions derived from the sequence of preceding percepts (high prior precision). Therefore, when following the above predictive coding definition, fluctuations in attention can be equated with fluctuations in mode. 

However, we feel that outside of the predictive coding field, attention is not always conceived in that way, such that low attention is often considered as low engagement with the task, i.e., relating to low likelihood and low prior precision in the predictive coding sense. Is it against this notion that we have included the analysis of attention as a separate control analysis (with the caveats outlined in our response to the comment above). 

We now provide a more nuanced interpretation of our findings of RTs and confidence in relation to attention, with a specific focus on predictive coding and precision. We hope that our responses to the comment above resolves the points raised in this comment. 

### Comment 5: Done

**What licences the assumption that "agents depend upon internal confidence signals" in the absence of feedback?** 

In the absence of feedback, observers can only rely on internal estimates of performance to guide updates to their model of the reliability of their sensory apparatus (inferences about $M$). Previous work (e.g. Guggenmos et al., Elife 2106, https://doi.org/10.7554/eLife.13388) has shown that confidence signals can provide signals that drive perceptual learning in the absence of feedback. This has motivated our model simulation on the adaptive benefits of bimodal inference for metacognition, where the learning signal $\epsilon_M$ drives inferences about $M$:

\begin{equation}
\epsilon_M = (1-|y_t - P(y_t = 1)|) - M_t
\end{equation} 

### Comment 6: Done

**And what licences the assumption that internal confidence feedback corresponds to "the absolute of the posterior log ratio" (did you mean the log of the posterior ratio)?**

We mean the absolute of the log of the posterior ratio. Following first order models (see e.g., Fleming & Daw, Self-evaluation of decision-making: A general Bayesian framework for metacognitive computation, Psychol. Rev. 2017, https://doi.org/10.1037/rev0000045), the perceptual decision and the confidence report rely on the posterior. The distance of the log of the posterior ratio $L_t$ from zero becomes a measure of decision-certainty or confidence.

### Comment 7: Done

**I got a bit lost here when you say that "the precision of sensory coding M a function of u_t. This is largely because I couldn't find a definition of u_t.**

We apologize for this lack of clarity. In the model simulations on the adaptive benefits of bimodal inference, we generated stimuli $s_t$ from a Bernoulli-distribution with p = q = 0.5. The value of $u_t$ was then defined via equation (22), following our modeling of the human data:

\begin{equation}
u_t = \frac{1}{1+exp(-\alpha*(s_t-0.5))}
\end{equation}

### Comment 8: Done

**What licences an application of Rescorla-Wagner to learning the parameters (as in Equation 11) and, learning sensory precision as described by M_T (Equation 13). Are you moving from a Bayesian framework to a reinforcement learning framework?**

We would like to thank the reviewer for pointing out this inconsistency. We have chosen the Rescorla-Wagner learning rule for simplicity: In our model, the speed of learning about $H$ and $M$ varies according to the current mode of perceptual processing and a constant learning rate:  

\begin{equation}
H'_t = H'_{t-1} + \beta_H *\omega_{LLR} * \epsilon_H
\end{equation}  

\begin{equation}
M'_t = M'_{t-1} + \beta_M *\omega_{LLR} * \epsilon_M
\end{equation}  

Allowing the learning rate itself to vary as a function of preceding experiences would add an additional level of complexity that we sought to omit in this analysis. However, we fully agree that choosing a Bayesian framework (e.g., a three-level HGF) would indeed be more consistent.

### Comment 9: Done

**I am sure you have answers to these questions - but with each new question the reader is left more and more skeptical that there is a coherent story behind your analyses. It would have been more convincing had you just committed to a Bayesian filter and made your points using one update scheme, under ideal Bayesian observer assumptions.**

**Unlike your piecemeal scheme, things like the hierarchical Gaussian filter estimates the sensory and prior decisions explicitly and these estimates underwrite posterior inference. In your scheme, the sensory precision M appears to have no influence on perceptual inference (which is why, presumably you call it metacognition). The problem with this is that your motivation for systematic fluctuations in precision is weakened. This is because improved metacognition does not improve perception — it only improves the perception of perception.**

**In light of the above, can I suggest that you remove Section 5.8 and use your model in the preceding section to endorse your hypothesis along the following lines:**

**"In summary, we hypothesized that subjects have certain hyperpriors that are apt for accommodating fluctuations in the predictability of their environment; i.e., people believe that their world is inherently volatile. This means that to be Bayes optimal it is necessary to periodically re-evaluate posterior beliefs about model parameters. One way to do this is to periodically suspend the precision of prior beliefs and increase the precision afforded to sensory evidence that updates (Bayesian) beliefs about model parameters. The empirical evidence above suggests that the timescale of this periodic scheduling of evidence accumulation is scale invariant. This means there exists a timescale of periodic fluctuations in precision over every window or length of perceptual decision-making. In what follows, we model perceptual decisions under a generative model (based upon a hazard function to model historical or serial dependencies) with, a periodic fluctuation in the precision of sensory evidence relative to prior beliefs at a particular timescale. Remarkably—using Bayesian model comparison—we find that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. Furthermore, we were able to quantify the dominant timescale of periodic fluctuations; appropriate for these kinds of paradigm."**

**Note, again, I am reading your omega_LLR and omega_ψ as precisions and that the periodic modulation is the hyperprior that you are characterizing—and have discovered.**

We would like to thank Prof. Friston for these very helpful and precise suggestions. In brief, we now provide a quantitative assessment of model space based on AIC (i) and have followed the suggestion of omitting section 5.8 (ii).

- (i) In addition to the qualitative assessment of our models in the initial version of our manuscript, we have conducted a formal model comparison. Following the model comparisons in other sections of the manuscript, we based the model comparison on AIC values. We furthermore show that the winning *bimodal inference model* predicts the out-of-training variables (RT and confidence) and use simulations from posterior model parameters to show that, in contrast to reduced models, the bimodal inference model neither over- nor underfits the empirical data. We have added a description of our model comparison to the Method section:

**(...) We validated the bimodal inference model in a three-step procedure: formal model comparison to reduced models based on AIC, prediction of within-training (stimulus- and history-congruence) as well as out-of-training variables (RT and confidence), and qualitative reproduction of the empirical data from model simulation based on estimated parameters.** 

**Model comparison. We assessed the following model space based on AIC:**

- **The full *bimodal inference model* (model 1) incorporates the influence of sensory information according to the parameter $\alpha$ (likelihood); the integration of evidence across trials according to the free parameter $H$ (prior); anti-phase oscillations in between likelihood and prior precision according to $\omega_{LLR}$ and $omega_{\psi}$ according to parameters $a_{LLR}$ (amplitude likelihood fluctuation), $a_{\psi}$ (amplitude prior fluctuation), $f$ (frequency) and $p$ (phase). The bimodal inference model is the most complex, as it allows prior and likelihood precision to fluctuate at different amplitudes.** 

- **The *likelihood-oscillation-only model* (model 2) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to free parameter $H$ (prior); oscillations in likelihood precision according to $\omega_{LLR}$ with free parameters $a_{LLR}$ (amplitude likelihood fluctuation), $f$ (frequency) and $p$ (phase).**

- **The *prior-oscillation-only model* (model 3) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to free parameter $H$ (prior); oscillations in the prior precision according to $\omega_{\psi}$ with free parameters $a_{\psi}$ (amplitude prior fluctuation), $f$ (frequency) and $p$ (phase). Please note that all models 1-3 lead to shifts in the relative precision of likelihood and prior.** 

- **The *normative-evidence-accumulation* (model 4) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to free parameter $H$ (prior); no oscillations. Model 4 corresponds to the model proposed by Glaze et al. and captures normative evidence accumulation in unpredictable environments using a Bayesian update scheme[@Glaze2015]. Importantly, model 4 thus represents the null hypothesis that fluctuations in mode emerge from a normative Bayesian model without the ad-hoc addition of oscillations as in model 1-3.**

- **The *no-evidence-accumulation* (model 5) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); no integration of evidence across trials; no oscillation. This model has no effective (flat) priors. It represents the null hypothesis that observers do not use prior information derived from serial dependency.** 

**Prediction of within-training and out-of-training variables. To validate our model, we correlated individual posterior parameter estimates with the respective conventional variables. As a sanity check, we tested (i), whether the estimated hazard rate $H$ correlated negatively with the frequency of history-congruent choices and, (ii), whether the estimated $\alpha$ correlated positively with the frequency of stimulus-congruent choices. In addition, we tested whether the posterior decision certainty (i..e. the absolute of the posterior log ratio) correlated negatively with RTs and positively with confidence. This allowed us to assess whether our model could explain aspects of the data it was not fitted to (i.e., RTs and confidence).** 

**Simulations. Finally, we used simulations (see below) to show that all model components, including the anti-phase oscillations governed by $a_{\psi}$, $a_{LLR}$, $f$ and $p$, were necessary for our model to reproduce the characteristics of the empirical data. This allowed a qualitative assessment of over- or under-fitting in the bimodal inference model and all reduced models 2-5. We used the posterior model parameters observed for humans ($H$, $\alpha$, $a_{\psi}$, $a_{LLR}$ and $f$) to define individual parameters for simulation in `r nrow(Optim_Behav)` simulated participants (i.e., equivalent to the number of human participants). For each participant, the number of simulated trials was drawn at random between 300 to 700. Inputs $s$ were drawn at random for each trial, such that the sequence of inputs to the simulation did not contain any systematic seriality. Noisy observations $u$ were generated by applying the posterior parameter $\alpha$ to inputs $s$, thus generating stimulus-congruent choices in `r mean(Sim_Behav$Accuracy, na.rm = TRUE)` ± `r sd(Sim_Behav$Accuracy, na.rm = TRUE)/nrow(Sim_Behav)`% of trials. Choices were simulated based on the trial-wise choice probabilities $y_{p}$ obtained from our model. Simulated data were analyzed in analogy to the human and murine data. As a substitute of subjective confidence, we computed the absolute of the trial-wise posterior log ratio $|L|$ (i.e., the posterior decision certainty).**

```{r model_comp, cache = TRUE}
Optim_eval = read.csv("./Results/Optim_eval_human_interim.csv")
Optim_eval$species = "humans"
Optim_eval$unique_id = as.character(Optim_eval$subject_id * rand(1))

Optim_eval_mice = read.csv("./Results/Optim_eval_mouse_interim.csv")
Optim_eval_mice$species = "mice"
Optim_eval_mice$unique_id = as.character(Optim_eval_mice$subject_id * Optim_eval_mice$session_id * rand(1))

Optim_eval = rbind(Optim_eval[,c("value", "type", "free_param", "trial_n", "species", "convcode", "unique_id")], Optim_eval_mice[,c("value", "type", "free_param", "trial_n", "species", "convcode", "unique_id")])
Optim_eval$type = gsub("_mouse", "", Optim_eval$type)

Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_no_integration", "5: no-evidence-accumulation", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_no_amp", "4: normative-evidence-accumulation", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_Prior_amp", "3: prior-oscillation-only model", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_LLR_amp", "2: likelihood-oscillation-only model", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_one_amp", "Model X: Only one amplitude", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1" , "1: Bimodal inference", Optim_eval$type)


Sum_AIC_Models <-   ddply(Optim_eval[Optim_eval$convcode == 0,],
                   .(species, type),
                   summarise,
                   median_error = median(value, na.rm = TRUE),
                   sum_error = sum(value, na.rm = TRUE),
                   k = mean(free_param),
                   AIC = 2*k + 2 * sum(value, na.rm = TRUE) 
                ) 


Sum_Subj_Model_AIC <-   ddply(Optim_eval[Optim_eval$convcode == 0,],
                   .(unique_id, type, species),
                   summarise,
                   median_error = median(value, na.rm = TRUE),
                   sum_error = sum(value, na.rm = TRUE),
                   k = mean(free_param),
                   AIC = 2*k + 2 * sum(value, na.rm = TRUE) 
                ) 
# p_distribution_Sum <- ggplot(Optim_eval[Optim_eval$convcode == 0 & Optim_eval$type != "Model X: Only one amplitude",], 
#                              aes(x = value,  y=..density..,  color = type, fill = type)) + 
#   #geom_density(color = "white", alpha=.25, bw = 0.025, size = 0.5) + 
#   geom_histogram(position = "dodge", alpha = 0.5, center = 0) +
#   theme_classic(base_size = 6) +
#     scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +  
#     theme(legend.position = "top") + facet_wrap(~species, ncol = 1) 
# p_distribution_Sum

print(Sum_AIC_Models)
```

The formal model comparison yielded clear evidence for a superiority of the bimodal inference model, in particular over the normative Bayesian model of evidence accumulation. The model successfully predicted both within-training variables (as a sanity-check) and out-of-training variables. Simulations from posterior model parameters closely followed the empirical data (Figure X), which was not the case for reduced models. We summarize these findings in the Results section and have added a Supplemental Figure SX to show the distribution of observer-level AICs at the session-level (see below):

- **We used a maximum likelihood procedure to fit the bimodal inference model to the behavioral data from the Confidence database[@Rahnev2020] and IBL database[@Aguillon-Rodriguez2020]. We validated our model in three steps: First, we compared the bimodal inference model against a set of four control models to show that bimodal inference does not emerge spontaneously in normative Bayesian models of evidence accumulation, but requires the ad-hoc addition of anti-phase oscillations in prior and likelihood precision. To this end, we successively removed the anti-phase oscillations and the integration of information across trials from the bimodal inference model (model 1) and performed a model comparison based on AIC.** 

- **Model 2 ($AIC_2$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC' in mice) and Model 3 ($AIC_3$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC' in mice) incorporated only oscillations of either likelihood or prior precision. Model 4 ($AIC_4$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC' in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model 5 ($AIC_4$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC' in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information.** 

- **The bimodal inference model achieved the lowest AIC across the full model space ($AIC_1$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC' in mice) and was clearly superior to the normative Bayesian model of evidence accumulation ($\delta_{AIC}$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC' in mice; Supplemental Figure SX).**

- **As a second validation of the bimodal inference model, we tested how well the posterior model predicted within-training and out-of-training variables. The bimodal inference model characterizes each subject by a sensitivity parameter $\alpha$ that captures how strongly perception is driven by the available sensory information, and a hazard rate parameter $H$ that controls how heavily perception is biased by perceptual history. As a sanity check for model fit, we tested whether the frequency of stimulus- and history-congruent trials in the Confidence database[@Rahnev2020] and IBL database[@Aguillon-Rodriguez2020] correlate with the estimated parameters $\alpha$ and $H$, respectively. As expected, the estimated sensitivity toward stimulus information $\alpha$ was positively correlated with the frequency of stimulus-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r STAT.Optim_Accuracy_Precision$coefficients[2,5]`$; mice: $\beta$ = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r M_STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,5]`$). Likewise, $H$ was negatively correlated with the frequency of history-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r STAT.Optim_History_Hazard$coefficients[2,5]`$; mice:  $\beta$ = $`r M_STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r M_STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r M_STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r M_STAT.Optim_History_Hazard$coefficients[2,5]`$).** 

- **Our behavioral analyses have shown that humans and mice showed significant effects of perceptual history that impaired performance in randomized psychophysical experiments[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] (Figure 2A and 3A). We therefore expected that humans and mice underestimated the true hazard rate $\hat{H}$ of the experimental environments (Confidence database[@Rahnev2020]: $\hat{H}_{Humans}$ = `r mean((100 - Behav$Stimulus_History)/100, na.rm = TRUE)` ± `r sd((100 - Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(Behav)`); IBL database[@Aguillon-Rodriguez2020]: $\hat{H}_{Mice}$ = `r mean((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)` ± `r sd((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(M_Behav)`). Indeed, when fitting the bimodal inference model outlined above to the trial-wise perceptual choices (see Methods), we found that the estimated (i.e., subjective) hazard rate $H$ was lower than $\hat{H}$ for both humans (H = `r mean(Optim_Behav$Hazard, na.rm = TRUE)` ± `r sd(Optim_Behav$Hazard, na.rm = TRUE)/nrow(Optim_Behav)`, $\beta$ = $`r STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r STAT.Optim_Underestimation$coefficients[1,5]`$) and mice (H = `r mean(Optim_M_Behav$Hazard, na.rm = TRUE)` ± `r sd(Optim_M_Behav$Hazard, na.rm = TRUE)/nrow(Optim_M_Behav)`, $\beta$ = $`r M_STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r M_STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r M_STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r M_STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r M_STAT.Optim_Underestimation$coefficients[1,5]`$).**

- **To further probe the validity of the bimodal inference model, we tested whether posterior model quantities could explain aspects of the behavioral data that the model was not fitted to. We predicted that the posterior decision variable $L_t$ not only encodes perceptual choices (i.e., the variable used for model estimation), but should also predict the speed of response and subjective confidence[@Kepecs2008; @Braun2018]. Indeed, the estimated trial-wise posterior decision certainty $|L_t|$ correlated negatively with RTs in humans ($\beta$ = $`r STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_RT_Certainty$coefficients[2,5]`$) and TDs mice ($\beta$ = $`r M_STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r M_STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r M_STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r M_STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r M_STAT.Optim_RT_Certainty$coefficients[2,5]`$). Likewise, subjective confidence reports were positively correlated with the estimated posterior decision certainty in humans ($\beta$ = $`r STAT.Optim_Confidence_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_Confidence_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_Confidence_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_Confidence_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_Confidence_Certainty$coefficients[2,5]`$).** 

- **The dynamic accumulation of information inherent to our model entails that biases toward perceptual history are stronger when the posterior decision certainty at the preceding trial is high[@Glaze2015; @Braun2018; @Bergen2019]. Due to the link between posterior decision certainty and confidence, we reasoned that confident perceptual choices should be more likely to induce history-congruent perception at the subsequent trial[@Braun2018; @Bergen2019]. Indeed, logistic regression indicated that history-congruence was predicted by the posterior decision certainty $|L_{t-1}|$  (humans: $\beta$ = $`r STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_mu_minus_1$coefficients[2,4]`$; mice: $\beta$ = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,4]`$) and subjective confidence (humans: $\beta$ = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,4]`$) at the preceding trial.**

- **As a third validation of the bimodal inference model, we used the posterior model parameters to simulate synthetic perceptual choices and repeated the behavioral analyses conducted for the empirical data. Simulations from the bimodal inference model closely replicated our results empirical results outlined above: Simulated perceptual decisions resulted from a competition of perceptual history with incoming sensory signals (Figure 6A). Stimulus- and history-congruence were significantly auto-correlated (Figure 6B-C), fluctuating in anti-phase as a scale-invariant process with a 1/f power law (Figure 6D-F). Simulated posterior certainty[@Kepecs2008; @Urai2017; @Braun2018] (i.e., the absolute of the posterior log ratio $|L_t|$) showed a quadratic relationship to the mode of sensory processing (Figure 6H), mirroring the relation of RTs and confidence reports to external and internal biases in perception (Figure 2G-H and Figure 4G-H). Crucially, the overlap between empirical and simulated data broke down when we removed the anti-phase oscillations or the accumulation of evidence over time from the bimodal inference model (see Supplemental Figure S7-10).**

- **In sum, computational modeling thus suggested that between-mode fluctuations are best explained by two interlinked processes (Figure 1E): (i), the dynamic accumulation of information across successive trials mandated by normative Bayesian model of evidence accumulation and, (ii), ongoing anti-phase oscillations in the impact of external and internal information.**

```{r Supplemental_Figure_SX, cache = TRUE}
p_distribution_Sum_Subj_Model_AIC <-
  ggplot(Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type != "Model X: Only one amplitude", ],
         aes(
           x = round_any(AIC, 25),
           y = ..density..,
           color = type,
           fill = type
         )) +
  geom_histogram(position = "dodge", alpha = 0.5) +
  theme_classic(base_size = 6) + xlim(-10, max(Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type != "Model X: Only one amplitude", ]$AIC)) +
  labs(x = "AIC (subject-level)", y = "Density", color = "Model", fill = "Model", subtitle = "B") +
  scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +
  theme(legend.position = "none") + facet_wrap( ~ species, ncol = 2)

p_distribution_Sum_Model_AIC <-
  ggplot(Sum_AIC_Models[Sum_AIC_Models$type != "Model X: Only one amplitude", ],
         aes(
           y = type,
           x = AIC,
           color = type,
           fill = type
         )) +
  geom_col(alpha = 0.5, width = 0.1) +
  theme_classic(base_size = 6) + 
  scale_y_discrete(labels=c("M 1", "M 2", "M 3", "M 4", "M 4"))+
  labs(x = "AIC (group-level)", y = "Model", color = "Model", fill = "Model", subtitle = "A") +
  scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +
  theme(legend.position = "top") + facet_wrap( ~ species, ncol = 2)

lay <- rbind(c(1), c(2))
grid.arrange(
p_distribution_Sum_Model_AIC,
p_distribution_Sum_Subj_Model_AIC,
layout_matrix = lay,
heights = c(0.75,1),
widths = c(1))
```

**Supplemental Figure SX. Comparison of the bimodal inference model against reduced control models.** **A. Group-level AIC.** The bimodal inference model (Model 1) achieved the lowest AIC across the full model space ($AIC_1$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC' in mice). Model 2 ($AIC_2$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC' in mice) and Model 3 ($AIC_3$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC' in mice) incorporated only oscillations of either likelihood or prior precision. Model 4 ($AIC_4$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC' in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model 5 ($AIC_4$ = 'r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC' and 'r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC' in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information. **B. Subject-level AIC.** Here, we show the distribution of AIC values at the subject-level. AICs for the bimodal inference tended to be smaller than AICs for the comparator models.

(ii) In the light of our response to Comments 1 - 9 above, we agree that a complete and extensive investigation of the relation between bimodal inference, learning about changes in the environment and the relation to metacognition may be beyond the scope of the current manuscript: Both Reviewer 1 and 3 (see below) have shared that the manuscript is too long and should be streamlined. Yet evaluating the full model space (e.g., comparing update rules for inferences about $H$ and $M$, testing for an influence of beliefs about $M$ on learning about $H$ etc.) would make the manuscript even longer. We are therefore happy to follow Prof. Fristons suggestions to omit section 5.8. We have changed the manuscript in the following ways:

- When introducing $\omega_{LLR}$, we identify it as the precision afforded to the likelihood, referring to the Bayesian framework, and refer to fluctuations in mode as a hyperprior: **To allow for *bimodal inference*, i..e, alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$, i.e., two anti-phase sine functions (defined by amplitude $a$, frequency $f$ and phase $p$) that determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)**

- We have deleted the section 5.8 and added a summary of our modeling approach to the discussion, closely following the text recommended by Prof. Friston: (...) **Yet relying too strongly on serial dependencies may come at a cost: When accumulating over time, internal predictions may eventually override external information, leading to circular and false inferences about the state of the environment. Akin to the wake-sleep-algorithm in machine learning[@Bengio2015], bimodal inference may help to determine whether errors result from external input or from internally-stored predictions: During internal mode, sensory processing is more strongly constrained by predictive processes that auto-encode the agent's environment. Conversely, during external mode, the network is driven predominantly by sensory inputs[@Honey2017]. Between-mode fluctuations may thus generate an unambiguous error signal that aligns internal predictions with the current state of the environment in iterative test-update-cycles[@Bengio2015]. On a broader scale, between-mode fluctuations may thus regulate the balance between feedforward versus feedback contributions to perception and thereby play a adaptive role in metacognition and reality monitoring[@Dijkstra2021].**

**From the perspective of the Bayesian brain hypothesis, we hypothesized that observers have certain hyperpriors that are apt for accommodating fluctuations in the predictability of their environment, i.e., people believe that their world is inherently volatile. To be Bayes optimal, it is therefore necessary to periodically re-evaluate posterior beliefs about the parameters that define an internal generative model of the external sensory environment. One way to do this is to periodically suspend the increase the precision afforded to sensory evidence relative to prior precisions in order to update Bayesian beliefs about model parameters.**

**The empirical evidence above suggests that the timescale of this periodic scheduling of evidence accumulation is scale invariant. This means there exists a timescale of periodic fluctuations in precision over every window or length of perceptual decision-making. Our bimodal inference approach models perceptual decisions under a generative model (based upon a hazard function to model serial dependencies between subsequent trials) with periodic fluctuations in the precision of sensory evidence relative to prior beliefs at a particular timescale. Remarkably, a systematic model comparison based on AIC indicated that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. The ad-hoc addition of oscillations to a normative Bayesian model of evidence accumulation[@Glaze2015] allowed us to quantify the dominant timescale of periodic fluctuations mode at approximately 0.11 1/$N_{trials}$ in humans and mice that is appropriate for these kinds of paradigms.**

### Comment 11: Done

**This begs the question as to whether you want to pursue the 1/f story. You refer to this as "noise". However, there is no noise in this setup. I think what you meant was that the fluctuations are scale free, because they evinced a power law. I am sure that there are scale free aspects of these kinds of hyperpriors; however, in the context of your paradigm I wonder whether you should just ignore the scale free aspect and focus on your estimated temporal scale implicit in f. This means you don't have to hand wave about self-organized criticality in the discussion and focus upon your hypothesis.**

We would like to thank the reviewer for this suggestion. We have agree that the discussion of self-organized criticality is far from the data. We have omitted this section from the discussion. With respect to $f$, we have adapted the manuscript to make clear that it captures the dominant timescale of fluctuations in mode:

-  Results: **(...) This implements a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)**

-  Results: **(...) The parameter $f$ captured the dominant time scale at which likelihood and prior precision were shifted against each other and was estimated at 0.11 1/$N_{trials}$ in both humans and mice.**

- Discussion: **Remarkably, a systematic model comparison based on AIC indicated that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. The ad-hoc addition of oscillations to a normative Bayesian model of evidence accumulation[@Glaze2015] allowed us to quantify the dominant timescale of periodic fluctuations mode at approximately 0.11 1/$N_{trials}$ in humans and mice that is appropriate for these kinds of paradigms.**

### Comment 12: Done

**A final move—to make the paper more focused and digestible—would be to put a lot of your defensive analyses (e.g. about general arousal et cetera) in supplementary material. You have to be careful not to exhaust the reader by putting up a lot of auxiliary material before the important messages in your report.**

- We have followed this suggestion and move the following sections to the supplement: section 5.3 (Internal and external modes of processing facilitate response behavior and enhance confidence in human perceptual decision-making), section 5.4 (Fluctuations between internal and external mode modulate perceptual performance beyond the effect of general response biases), section 5.5 (Internal mode is characterized by lower thresholds as well as by history-dependent changes in biases and lapses). 

**Minor points**

### Comment 13: Done

**I cannot resist suggesting that you change your title to "Bimodal Inference in Mice and Men"**

We thank you for this suggestion and have changed the title accordingly. 

### Comment 14: Done

**Please replace "infra-slow fluctuations" with "slow fluctuations". slow has some colloquial meaning in fMRI studies but not in any scale free context.**

Done.

### Comment 15: Done

**Please replace "simulated data" with "simulations" in the abstract. Finally, please replace "robust learning and metacognition in volatile environments" with "enable optimal inference and learning in volatile environments."**

Done. Since we have followed the suggestion to delete section 5.8, we have rephrased the last paragraph of the abstract into "We propose that between-mode fluctuations may benefit perception by generating unambiguous error signals that enable optimal inference and learning in volatile environments".

### Comment 16: Done

**Line 50, please replace "about the degree of noise inherent in encoding of sensory information" with "the precision of sensory information relative to prior (Bayesian) beliefs."**

Done. 

### Comment 17: Done

**Line 125: please replace "a source of error" with "a source of bias"**

Done.

### Comment 18: Done

**Line 141: please replace "one 1/f noise" with a scale invariant process with a 1/f power law" (here and throughout) this is not "noise" it is a particular kind of fluctuation.**

Done.

### Comment 19

**Line 178, when you say that the fluctuations may arise due to "changes in level of tonic arousal or on-task attention", I think you need to qualify this. In predictive processing, on-task attention is exactly the modulation of sensory precision, relative to prior precision that you are characterising here. Tonic arousal may be another thing may or may not confound your current results.**

Thank you very much for pointing this out. We have adapted the discussion to make the distinction between attention in the predictive processing sense and the broader issue of task engagement (reflecting fluctuations in arousal, fatigue etc.) clearer (see also comments above): 

**How does attention relate between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. As outlined above, between-mode fluctuations can be understood as ongoing shifts in the precision afforded to likelihood (*external mode*) and prior (*internal mode*), respectively. When the precision afforded to prior or likelihood increases, posterior precision increases, which leads to faster RTs and higher confidence. When defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may thus provide a plausible explanation for the quadratic relationship between mode and RTs and confidence (Figure 2H and J; Figure 4I, Figure 6I)**.

**Outside of the predictive processing field, attention is often understood in the context of engagement with the task[@Matthew2010], which may vary according the availability of cognitive resources that are modulated by factors such as tonic arousal, fatigue or familiarity with the task. Importantly, our results indicate that internal mode processing can be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, internal mode was not associated with a general increase in lapse (Supplemental Figures SX), arguing against the proposition that internal mode is associated with random or stereotypical responses that may reflect low task engagement.** 

**In interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects task performance[@Yerkes1908]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. Of note, high phasic arousal has been shown to suppress biases in decision-making in humans and mice across domains[@deGee2014; @deGee2017; @deGee2020], including biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. Future work should therefore more direct measures of arousal (such as pupil size[@deGee2014; @Urai2017; @deGee2020; @deGee2022; @Beerendonk2023], motor behavior[@deGee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.**

### Comment 20: Done

**When introducing Equation 2, please make it clear that the omega terms stand in for the precisions afforded to the likelihood (omega_LLR) and prior (omega_ψ) that constitute the log posterior.**

We have modified the introduction of equation 2 as follows: Following Bayes' theorem, we reasoned that binary perceptual decisions depend on the posterior log ratio $L$ of the two alternative states of the environment that participants learn about via noisy sensory information[@Glaze2015]. We computed the posterior by combining the sensory evidence available at time-point $t$ (i.e., the log likelihood ratio $LLR$) with the prior probability $\psi$**, weighted by the respective precision terms $\omega_{LLR}$ and $\omega_{\psi}$**:

\begin{equation}
L_t = LLR_t * \omega_{LLR} + \psi_t(L_{t-1}, H) * \omega_{\psi}
\end{equation}

**You can then motivate Equation 6 and 7 as implementing the hyperprior in which the sensory and prior precisions fluctuate at a particular time scale**.

We would like to thank the reviewer for this suggestion, which we have added to the introduction of equations (6) and (7): **To allow for *bimodal inference*, i..e, alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$, i.e., two anti-phase sine functions (defined by amplitude $a$, frequency $f$ and phase $p$) that determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)**

\begin{equation}
\omega_{LLR} = a_{LLR} * sin(f * t + p) + 1
\end{equation}

\begin{equation}
\omega_{\psi} = a_{\psi} * sin(f * t + p + \pi) + 1
\end{equation}

### Comment 21: Done

**You can also point out that the implicit anti-phase fluctuations are mandated by Bayes optimal formulations in which it is only the relative values of the prior and sensory precision that matter. Bayesian filters these precisions constitute the Kalman gain. You can find a derivation of why this in treatments of the hierarchical Gaussian filter is by Mathys et al.**

We would like to thank the reviewer for this suggestion. We added this information to the description of our model in the Results section (see comment above).

### Comment 22: Done

**In your first model simulations, I would make it clear in the main text which parameters you are optimizing's; namely (H, alpha, a_likelihood, a_prior f). Perhaps a little table with a brief description of the meaning of these hyper parameters would be useful?**

- We now identify the optimized parameters at the outset of the modeling section: **(...) We fitted the bimodal inference model outlined above to behavioral data Confidence database[@Rahnev2020] and IBL database[@Aguillon-Rodriguez2020] using a maximum likelihood procedure, optimizing the parameters $\alpha$ and $H$ as well as the amplitudes $amp_{LLR/psi}$, frequency $f$ and phase $p$ that determine $\omega_{\LLR}$ and $\omega_{\psi}$ (see Methods for details and Supplemental Table T2 for a summary of the parameters of the bimodal inference model).**

- **The bimodal inference model characterizes each subject by a sensitivity parameter $\alpha$ that captures how strongly perception is driven by the available sensory information, and a hazard rate parameter $H$ that controls how heavily perception is biased by perceptual history. As a sanity check for model fit, we tested whether the frequency of stimulus- and history-congruent trials in the Confidence database[@Rahnev2020] and IBL database[@Aguillon-Rodriguez2020] correlate with the estimated parameters $\alpha$ and $H$, respectively (...).** 

- We furthermore included a table summarizing the model parameters: 

```{r Supplemental_Table_T2, cache = TRUE}

library(kableExtra)
Parameters <- data.frame(
  Parameters = c("$\\alpha$", "$H$", "$amp_{LLR}$", "$amp_{\\psi}$", "$f$", "$p$"), 
  Interpretation = c("Sensitivity to sensory information", 
                     "Expected probability of a switch in the cause of sensory information (Hazard)", 
                     "Amplitude of fluctuations in likelihood precision ($omega_{LLR}$)",
                     "Amplitude of fluctuations in prior precision ($omega_{\\psi}$)", 
                     "Frequency of $omega_{LLR}$ and $omega_{\\psi}$",
                     "Phase ($p$ for $omega_{LLR}$; $p + \\pi$  for $omega_{\\psi}$)"))
row.names(Parameters) <- NULL
kbl(Parameters, format = "latex", longtable = T, booktabs = T) %>%
kable_styling(font_size = 7, latex_options = c("repeat_header"))
```

### Comment 23: Done

**Please remove Section 5.8. If you do not, you need to explain why — on line 586 - setting a = 0 is appropriate when a = 0, the log posterior in Equation 2 is zero because the precisions (omegas) are zero (by Equations 6 and 7).**

We have removed the section 5.8. When setting the amplitude parameters to zero, $\omega_{LLR}$ and $\omega_{\psi}$ are constant at 1, creating a unimodal control model that corresponds to the normative Bayesian evidence accumulation model proposed by Glaze et al[@Glaze2015].  

## Reviewer 2

**The authors elucidate whether periodicities in the sensitivity to external information represent an epiphenomenon of limited processing capacity or, alternatively, result from a structured and adaptive mechanism of perceptual inference. Analyzing large datasets of perceptual decision-making in humans and mice, they investigated whether the accuracy of visual perception is constant over time or whether it fluctuates. The authors found significant autocorrelations on the group level and on the level of individual participants, indicating that a stimulus-congruent response in a given trial increased the probability of stimulus-congruent responses in the future. Furthermore, the authors addressed whether observers cycle through periods of enhanced and reduced sensitivity to external information or whether observers rely on internal information in certain phases. This was quantified by whether a response at a given trial was correlated with responses in previous trials. The authors used computational modeling to infer the origin of the different modes (internal vs. external).**

**Evaluation**

**This is a very interesting and well-written manuscript, dealing with an important question. The findings are novel and provide an innovative account of interpreting visual perception. I am not an expert in modeling, so I will restrict my comments to the theoretical framework and the experimental approach. I have a few minor questions that I would like the authors to answer or clarify.**

We would like to thank the reviewer for the evaluating our manuscript. We have added the discussion of potential effects at the motor-level to our discussion. 

**Minor questions**

### Comment 1: Done

**History congruent perception was defined on the basis of response repetitions. Are we really sure that responses are repeated due to some variant of a perceptual decision process (internal or external) or may arise on the motor-level - independent of a perceptual source? For instance, a response primed by residual activation in the motor system may represent a local effect independent from a general response bias. If indeed, a response repetition is initiated by whatever reasons (non-perceptual), wouldn't this imply that the repeated response is per se more related to previous than to current visual information and would hence signal a reduced sensitivity to current external information? The authors are discussing the option of stereotypically repeated responses in the context of alertness. However, a tendency to repeat responses may arise due to other reasons. For instance, may the motor priming effects mentioned possibly explain faster RTs along with a stronger bias when in internal-mode.**

Thanks a lot for pointing this out. In this manuscript, we attempt to characterize the phenomenon of bimodal inference at the level of behavioral data. The confidence database consists only of behavioral data. At the time of publishing this paper as a preprint, the IBL database had also released only behavioral data. We realize that is very difficult to discern effects that occur at the level of perception from effects that occur at the level of behavior. Not all studies in the confidence database have used a counter-balanced mapping between the perceptual decision and the associated motor-response. In the IBL data, the mapping between the perceptual decision and the associated motor-response is fixed (turning a response wheel left or right depending on the perceived location of a grating). Confidence reports also only provide a very information on motor- vs. perceptual effects. One may speculate that, if a response was driven by residual activity in the motor system, it may be more likely to be a lapse and be accompanied by reduced confidence. We found that confidence was, on average, elevated for history-congruent choices. 

That being said, behavioral analyses alone are insufficient to rule out the contribution of motor-related effects to seriality in choices. This control analysis would require analyses of additional types of data, such as video tracking of the motor response or even neural data collected in brain areas directly related to motor behavior. While this analysis is beyond the scope of the present manuscript, we plan to carry out these analyses using the recent data publication of the IBL, that contains, among others, video tracking of the motor response (turning of the response wheel) and neuropixel recording across the whole brain, including premotor and motor cortex. 

- We have added these considerations to the discussion of potential confounds:

**Residual activation of the motor system may provide another contribution to serial biases in perceptual choices[@Mawase2018]. Such motor-driven priming may lead to errors in randomized psychophysical designs, contributing to the phenomenon that resemble the internally-biased processing that we defined at the level of perception[@Pomper2022]. Moreover, residual activation of the motor system may lead to faster responses, and thus constitutes an alternative explanation for the quadratic relationship of mode with RTs[@Mawase2018]. At the same time, we observed elevated confidence for stronger biases toward internal mode. This may contradict the proposition that residual activation of the motor system is the primary driver of serial choice biases, since strong motor-driven priming should lead to frequent lapses that are typically associated reduced confidence[@Kepecs2012]. Likewise, perceptual history effects have repeatedly been replicated in experiments with counter-balanced stimulus-response mappings[@Braun2018: Feigin2021].  No-response paradigms, in which perceptual decision are inferred from eye-movements alone, could help to better differentiate perceptual from motor-related effects. Likewise, video-tracking of response behavior and neural recording from motor- and premotor, which has recently been released for the IBL database[IBL2023], may provide further insight into the relation of motor behavior to the perceptual phenomenon of between-mode fluctuations.** 

## Reviewer 3 

**In this paper the authors propose that during perceptual decisions, humans and mice exhibit regular oscillatory fluctuations  between an "external"  (that places more weight on the perceptual evidence) and an "internal" (that places more weight on historical experiences) mode. In particular, the authors propose a computational scheme in which the influences of history and current stimulus on choice oscillate in anti phase, effectively implementing "bimodal inference". The computational advantages of these scheme as well as its relation to the underlying neurophysiology are discussed.**

**Overall, the authors make a very interesting proposal about what drives slow fluctuations in perceptual performance during randomised two-alternative choice tasks. This proposal relates changes in accuracy with changes in serial choice biases, which is a timely and synthesising contribution. Furthermore, this proposal is backed by analyses over several human datasets  and a large dataset in mice.** 

**Despite its strong empirical contribution, the paper seems limited by the fact that alternative computational hypotheses are not adequately considered (or at least considered in a systematic way). At the same time, and although the paper is well written, some parts are overly technical.**

**Major comments:**

### Comment 1: TODO

**The authors collapse across various datasets in which different tasks were employed. However, some details on the nature of these different tasks and a discussion on the rationale of collapsing behavioural metrics across them is missing. The authors mention that all tasks involved binary perceptual decisions. In some parts of the manuscript the term "false alarms" is mentioned, indicating a detection protocol. Other terms in the methods section (e.g., "set size") might need further clarification. Importantly, it is not clear how reaction times were calculated in the various tasks and whether some experiments involved free response paradigms while others interrogation/ cued paradigms (in which case RTs can be defined as the latency between the response cue and the response).**

- go through all the papers that we investigated in the confidence database and check for these points (what paradigm etc.): type of 2AFC task, response mapping, response time, feedback.

### Comment 2: Done

**The key premise that when participants do not rely on the external stimulus they rely more on the previous trial needs to be more clearly (and statistically) contrasted against a null hypothesis. For instance, an null hypothesis could be that when participants place a lower weight on the stimulus they simply choose randomly. It is important to specify a null hypothesis such that the key premise does not appear self-evident or circular.**

```{r null_hypotheses, cache = TRUE, warning = FALSE}
##
## humans
##

### dynamic probabilities:
#STAT.slider_History_Preferred_vs_Accuracy$coefficients


#t.test((abs(STAT.slider_Preferred_vs_Accuracy$residuals)) - abs(STAT.slider_History_Preferred_vs_Accuracy$residuals)) ## replace with Accuracy ~ 

delta_AIC_correlation_humans = compute_AIC(
  LL = STAT.slider_History_Preferred_vs_Accuracy$logLik[1],
  K =  2,
  n = length(STAT.slider_History_Preferred_vs_Accuracy$residuals),
  correction = 0
) - compute_AIC(
  LL = STAT.slider_Preferred_vs_Accuracy$logLik[1],
  K =  1,
  n = length(STAT.slider_History_vs_Preferred$residuals),
  correction = 0
) 

### log reg
#STAT.full_logit_Sum$coefficients

#t.test(abs(STAT.reduced_logit_Sum$residuals) - abs(STAT.full_logit_Sum$residuals))

delta_AIC_log_reg_humans = compute_AIC(
  LL = STAT.full_logit_Sum$logLik[1],
  K =  3,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
) - compute_AIC(
  LL = STAT.reduced_logit_Sum$logLik[1],
  K =  2,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
)

##
## mice
##

### slider
### dynamic probabilities:
#M_STAT.slider_History_Preferred_vs_Accuracy$coefficients

## !! replace with Accuracy ~ Preferred model
t.test((abs(M_STAT.slider_History_vs_Preferred$residuals)) - abs(M_STAT.slider_Preferred_vs_Accuracy$residuals)) ## replace with Accuracy ~ 

compute_AIC(
  LL = M_STAT.slider_History_Preferred_vs_Accuracy$logLik[1],
  K =  2,
  n = length(STAT.slider_History_Preferred_vs_Accuracy$residuals),
  correction = 0
) - compute_AIC(
  LL = STAT.slider_History_vs_Accuracy$logLik[1],
  K =  1,
  n = length(STAT.slider_History_vs_Accuracy$residuals),
  correction = 0
) 


### log reg
# M_STAT.full_logit_Sum$coefficients

# t.test(abs(M_STAT.reduced_logit_Sum$residuals) - abs(M_STAT.full_logit_Sum$residuals))

delta_AIC_log_reg_mice = compute_AIC(
  LL = M_STAT.full_logit_Sum$logLik[1],
  K =  3,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
) - compute_AIC(
  LL = M_STAT.reduced_logit_Sum$logLik[1],
  K =  2,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
)
```

We would like to thank the reviewer for highlighting this important point. Following this suggestion, we have explicitly tested our main hypothesis ($H1$: periods of reduced stimulus-congruence are periods of enhanced reliance on history-congruence) against the following null hypotheses:

- $H0_1$: Periods of reduced stimulus-congruence are periods of enhanced random choices
- $H0_2$: Periods of reduced stimulus-congruence are periods of enhanced general bias

We present three sets of statistical analyses to test H1 against $H0_{1/2}$:

- First, we used logistic regression to predict individual choices. Under H1, one would expect a significant effect of perceptual history in a logistic regression model that predicts individual choices from the external stimulus, perceptual history and general response bias. At the model level, one would expect higher AIC in a model without perceptual history as a predictor of individual choices, indicating that perceptual history influences choices beyond noise ($H0_1$) and general response bias ($H0_2$). Both would not be the case under $H0_{1/2}$. In both humans and mice, we found a significant effect of perceptual history on choices while controlling for bias. When eliminating perceptual history as a predictor of individual choices, we found higher AIC (providing model-level evidence against $H0_1$ and $H0_2$). We complemented this analysis by computing AIC in individual observers (Supplemental Figure S4), and again found lower higher AIC in models from which perceptual history was eliminated. The following analyses have been added to the main manuscript. 

**Humans:**

**Fluctuations between internal and external mode cannot be explained by changes in the probability of stereotypical or random choices**

- Results: **The hypothesis that ongoing changes in the sensitivity to external information are driven by internal predictions induced via perceptual history needs to be contrasted against two alternative hypotheses: When making errors, observers may act respond stereotypically, i.e., exhibit stronger general biases toward one of the two potential outcomes, or simply choose randomly. Logistic regression confirmed that perceptual history made a significant contribution to perception ($\beta$ = $`r STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum$coefficients[3,4]`$) over and above the ongoing stream of external sensory information ($\beta$ = $`r STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward ($\beta$ = $`r STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r STAT.full_logit_Sum$coefficients[1,4]`$). When eliminating perceptual history as a predictor of individual choices, AIC increased by $\delta_{AIC}$ = `r -delta_AIC_log_reg_humans`, providing model-level evidence against both choice randomness and general response bias as the sole modulators of perceptual performance (see Supplemental Figure S4A-B for parameter- and model-level inference at the level of individual observers).**

**Mice:**

- **In line with humans, mice were biased toward perceptual history in `r mean(M_Behav$History, na.rm = TRUE)`% ± `r sd(M_Behav$History, na.rm = TRUE)/sqrt(length(M_Behav$History))`% of trials (T(`r M_STAT.Global_History_Accuracy$parameter`) = `r M_STAT.Global_History_Accuracy$statistic`, p = $`r M_STAT.Global_History_Accuracy$p.value`$; Figure 4A and Supplemental Figure S1D). Perceptual history effects remained significant ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[3,4]`$) when controlling for external sensory information ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward one of the two potential outcomes ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[1,4]`$). When eliminating perceptual history as a predictor of individual choices, AIC increased by  $\delta_{AIC}$ = `r -delta_AIC_log_reg_mice`, arguing against the notion that choice randomness and general response bias are the only determinants of perceptual performance (see Supplemental Figure S4C-D for parameter- and model-level inference within individual mice).**

- Second, we analyzed dynamic changes in history- and stimulus-congruence (i.e., smoothed probabilities for stimulus-congruence, history-congruence and general response bias in sliding 10 trial time-windows). Under H1, one would expect a significant negative correlation between the dynamic probability of stimulus- and history-congruence. At the model level, one would expect higher AIC in a model without history-congruence as a predictor of stimulus-congruence, indicating that changes in the probability of history-congruence influence stimulus-congruence beyond noise ($H0_1$) and general response bias ($H0_2$). Both would not be the case under $H0_{1/2}$. In both humans and mice, we found a significant negative correlation between history-congruence and stimulus-congruence while controlling for general response bias. When eliminating the dynamic probability of history-congruence as a predictor of stimulus-congruence, we found higher AIC (providing model-level evidence against $H0_1$ and $H0_2$). The section 5.4 or our original manuscript (*Fluctuations between internal and external mode modulate perceptual performance beyond the effect of general response biases*) complements these control analyses and has been moved to the supplement to stream-line the manuscript (following the Comment X.X by Reviewer 1 and Comment X.X by Reviewer 3). We have modified the main manuscript in the following way:

**Humans:**

- Results: **Finally, we ensured that fluctuations in stimulus- and history-congruence are linked to each other, while controlling for fluctuations in the strength of general response biases. When perceptual choices were less biased toward external information, participants relied more strongly on internal information acquired from perceptual history (and vice versa, $\beta$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$), controlling for fluctuations in the strength of general response biases ($\beta$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$).**

- Results: **(...) Likewise, eliminating the dynamic fluctuations in history-congruence as a predictor of fluctuations in stimulus-congruence yielded an increase in AIC by \delta_{AIC} + `r delta_AIC_correlation_humans`. This provided model-level evidence against the null hypotheses that fluctuations in stimulus-congruence are driven exclusively by choice randomness or general response bias (see Supplemental Section X.X for an in-depth assessment of general response bias).** 

**Mice:**

- **As in humans, fluctuations in the strength of history-congruent biases had a significant effect on stimulus-congruence ($\beta_1$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) beyond the effect of ongoing changes in general response biases ($\beta_2$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$). Eliminating the dynamic fluctuations in history-congruence as a predictor of fluctuations in stimulus-congruence resulted in an increase in AIC by \delta_{AIC} + `r delta_AIC_correlation_mice`. This confirmed that, in both humans and mice, perceptual performance is modulated by systematic fluctuations between externally- and internally-oriented modes of sensory processing that exist beyond choice randomness and general response bias.**

- Third, we analyzed full amd history-conditioned psychometric curves in external and internal mode as well as across modes. Under H1, one would expect (i) a history-dependent increase in biases and lapses (effects of perceptual history), and (ii), a history-independent increase in threshold (reduced sensitivity to external information). Conversely, if what we identified as internal mode processing were in fact driven by random choices ($H0_1$), one would expect (i), a history-independent increase in lapse (choice randomness), (ii), no change in bias (no effect of perceptual history), and (iii), reduced thresholds (reduced sensitivity to external information. In both humans and mice, we observed the pattern predicted by H1. In response to the comments by Reviewer 1 and 3, we have significantly streamlined the manuscript and moved our assessment of psychometric functions to the supplement. We now provide a summary of our results in the main manuscript to make our reasoning with respect to $H0_1$ and $H1$ more explicit:

**Humans:**

**Finally, to rule out the possibility that the reduced sensitivity to external information that we identified as internal mode processing is in fact driven by enhanced choice randomness as opposed to perceptual history, we estimated full and history-dependent psychometric curves for internal, external, and across modes (Supplemental Section X.X and Supplemental Figure SX). If, as hypothesized, internal mode processing reflects an enhanced impact of perceptual history, one would expect a history-dependent increase in biases and lapses as well as a history-independent increase in threshold. Conversely, if internal mode processing were driven by random choices, one would expect a history-independent increase in biases and threshold, and no change in bias. In line with our hypothesis, we found that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold. This confirms that internal mode processing is indeed driven by an enhanced impact of perceptual history (Supplemental Section X.X and Supplemental Figure SX).**   

**Mice:**

**When fitting full and history-conditioned psychometric curves to the data from the IBL database, we observed that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section X.X and Supplemental Figure SX). This provided further evidence for the hypothesis that internal mode processing is driven by an enhanced impact of perceptual history, as opposed to increased choice randomness.**   

### Comment 3: Add model comparison

**From a mechanistic (sequential sampling) perspective, several previous papers have examined whether choice history biases influence the starting point or the drift rate of the evidence accumulation process. Under the former formulation, reliance on the evidence vs. reliance on the previous choice will be naturally anti-correlated (the less weight you place on the evidence the more impactful the choice history will be, assuming that the last choice is represented as a starting point bias). This seems to be mapping onto the computational model the authors describe, in which there is a weight on the prior, a weight on the likelihood and the assumption that these weights fluctuate in anti-phase. It is not obvious that this anti-phase relationship needs to be imposed ad-hoc. Or whether it would emerge naturally (using a mechanistic or Bayesian framework). More generally, the authors assert that without an external mechanism prior biases would be impossible to overcome, and this would misfit the data. However, it would be important to a) actually show that the results cannot be explained by a single mechanism in which the anti-phase relationship is emergent rather than ad-hoc, b) relate the current framework with previous mechanistic considerations of serial choice biases.**

We would like to thank the reviewer for pointing this out. We agree that both normative Bayesian and mechanistic drift diffusion are bound to lead to anti-correlated effects of sensory information and perceptual history at individual trials. This, however, does not necessarily entail slow fluctuations in the impact of sensory information and perceptual history that evolves over many consective trials. We now provide a systematic model comparison and discuss our model in relation to drift diffusion models and descriptive models that assume slow changes in the latent parameters underlying perceptual decision-making. We also discuss the ad-hoc nature of the bimodal inference model in the subsection *Limitations and open questions*. 

- Results: model comparison

- Discussion: **In Bayesian models of perceptual decision-making, the relative precision of prior and likelihood determines their integration into the posterior that determines the content of perception. At the level of individual trials, the perceptual impact of internal predictions generated from perceptual history (prior precision) and external sensory information (likelihood precision) is thus necessarily anti-correlated. The same holds for mechanistic models of drift diffusion, which understand choice history biases as driven by changes in the starting point[@Glaze2015] or the drift rate of evidence accumulation[@Urai2019]. Under the former formulation, perceptual history is bound to have a stronger influence on perception when the less weight is given to incoming sensory evidence, assuming that the last choice is represented as a starting point bias. The effects of choice history in normative Bayesian and mechanistic drift diffusion models can be mapped onto one another via the Bayesian formulation of drift diffusion[@Bitzer2014], where the inverse of likelihood precision determines the amount of noise in the accumulation of new evidence and prior precision determines the absolute shift in its starting point[@Bitzer2014].**

- **While it is thus clear that the impact of perceptual history and sensory evidence are anti-correlated *at each individual trial*, we here introduce anti-phase oscillations as an ad-hoc modification to model slow fluctuations in prior and likelihood precision that evolve *over many consecutive trials* and are not mandated by normative Bayesian or mechanistic drift diffusion models. The bimodal inference model provides a reasonable explanation of the linked autocorrelations in stimulus- and history-congruence, as evidenced by formal model comparison, successful prediction of RTs and confidence as out-of-training variables, and a qualitative reproduction of our empirical data from posterior model parameter as evidence against over- or under-fitting. Of note, similar non-stationarities have been observed in descriptive models that assume continuous[@Roy2021] or discrete[@Ashwood2022] changes in the latent states that modulate perceptual decision-making at slow timescales.**

- Limitations and open questions: **The addition of slow anti-phase oscillations to the integration of prior and likelihood represents in the bimodal inference model represents an ad-hoc modification to a normative Bayesian model of evidence accumulation[@Glaze2015]. It is an important task for future research to test whether such slow fluctuations can emerge spontaneously in hierarchical models of Bayesian inference, and whether they can be causally manipulated by experimental variables. We speculate that between-mode fluctuations may partially separate the perceptual contribution of internal predictions and external sensory data in time, creating unambiguous learning signals that benefit inference about the precision of prior and likelihood, respectively. This proposition should be tested empirically by relating the phenomenon of bimodal inference to performance in, e.g., reversal learning, probabilistic reasoning, or metacogntive tasks.** 

### Comment 4: Done

**The authors need to unpack their definition of history biases since in previous work biases due to the response or the identity of the stimulus at the previous trial are treated differently. Here, the authors focus on response biases but it is not clear whether they could examine also stimulus-driven history biases (in paradigms where stimulus-response is remapped on each trial).**

```{r choice_history, cache = TRUE}
if (!load_summary_data) {
  
## humans

## General effect of stimulus-history
Behav$null_Stimulus_History = Behav$Stimulus_History - 50
mean_Stimulus_History = mean(Behav$Stimulus_History, na.rm = TRUE)
error_Stimulus_History = sd(Behav$Stimulus_History, na.rm = TRUE)/sqrt(length(Behav$Stimulus_History))
  
Global_Stimulus_History_Accuracy <- lmer(null_Stimulus_History ~ Accuracy + (1 | study_id), data = Behav)
STAT.Global_Stimulus_History_Accuracy = summary(Global_Stimulus_History_Accuracy)

## Difference between choice- and stimulus-history
Difference_Stimulus_Choice_History = lmer(null_Stimulus_History-null_History ~ 1 + (1 | study_id), data = Behav)
STAT.Difference_Stimulus_Choice_History = summary(Difference_Stimulus_Choice_History)

if (compute_logreg) {
## Comparison of the explanatory power of stimulus- and choice history
PwData$Stimulus_minus_1 = c(NA, PwData$Stimulus[1:length(PwData$Stimulus)-1])
PwData$Stimulus_minus_1[PwData$Trial == 1] = NA
full_logit_Sum_Stimulus_History <-
    glmer(Response ~ Stimulus + Stimulus_minus_1 + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1 & PwData$Stimulus_minus_1 != PwData$Response_minus_1,],
        family = binomial)

full_logit_Sum_Choice_History <-
    glmer(Response ~ Stimulus + Response_minus_1 + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1 & PwData$Stimulus_minus_1 != PwData$Response_minus_1,],
        family = binomial)
} else {
  full_logit_Sum_Stimulus_History <- readRDS(file = "./Results/full_logit_Sum_Stimulus_History.rds")
  full_logit_Sum_Choice_History <- readRDS(file = "./Results/full_logit_Sum_Choice_History.rds")
}
STAT.full_logit_Sum_Stimulus_History <- summary(full_logit_Sum_Stimulus_History) 
STAT.full_logit_Sum_Choice_History <- summary(full_logit_Sum_Choice_History)

##
## mice
##
## General effect of stimulus-history
M_Behav$null_Stimulus_History = M_Behav$Stimulus_History - 50
M_mean_Stimulus_History = mean(M_Behav$Stimulus_History, na.rm = TRUE)
M_error_Stimulus_History = sd(M_Behav$Stimulus_History, na.rm = TRUE)/sqrt(length(M_Behav$Stimulus_History))
M_STAT.Global_Stimulus_History_Accuracy = t.test(M_Behav$null_Stimulus_History)

## Difference between choice- and stimulus-history
M_STAT.Difference_Stimulus_Choice_History = t.test(M_Behav$null_Stimulus_History - M_Behav$null_History)

if (compute_mouse_logreg) {
    M_full_logit_Sum_Stimulus_History <-
      glmer(
        Response ~ Stimulus + Stimulus_minus_1 + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1 & MwData$Stimulus_minus_1 != MwData$Response_minus_1, ],
        family = binomial
      )
    
    M_full_logit_Sum_Choice_History <-
      glmer(
        Response ~ Stimulus + Response_minus_1 + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1 & MwData$Stimulus_minus_1 != MwData$Response_minus_1, ],
        family = binomial
      )
    
  } else {
    M_full_logit_Sum_Stimulus_History <-
      readRDS(file = "./Results/M_full_logit_Sum_Stimulus_History.rds")
     M_full_logit_Sum_Choice_History <-
      readRDS(file = "./Results/M_full_logit_Sum_Choice_History.rds")
   
  }
  M_STAT.full_logit_Sum_Stimulus_History <- summary(M_full_logit_Sum_Stimulus_History)
  M_STAT.full_logit_Sum_Choice_History <- summary(M_full_logit_Sum_Choice_History)

  if (save_summary_data) {
    save(mean_Stimulus_History,
         error_Stimulus_History,
         STAT.Global_Stimulus_History_Accuracy, 
         STAT.Difference_Stimulus_Choice_History,
         M_STAT.full_logit_Sum_Stimulus_History,
         M_STAT.full_logit_Sum_Choice_History,
         M_mean_Stimulus_History,
         M_error_Stimulus_History,
         M_STAT.Global_Stimulus_History_Accuracy, 
         M_STAT.Difference_Stimulus_Choice_History,
         M_STAT.full_logit_Sum_Stimulus_History,
         M_STAT.full_logit_Sum_Choice_History,
         file = "./Summary_Data/choice_history.Rdata")
  }
  
} else {
  load("./Summary_Data/choice_history.Rdata")
}
```

We thank the reviewer for raising this imporant point. We defined the history-biases reported in our main manuscript by comparing the *response about the perceived* stimulus category (A vs. B) at the current and at the preceding trial (choice history). An alternative would have been to define history biases by comparing the choice at the current trial to the stimulus category *presented* at the preceding trial(stimulus history). As we show below, perceptual responses tended to be biased not only toward choice history, but also (but to a lesser degree) to stimulus history. This is expected, as perception was *stimulus-congruent* on approximately 75% of trials, causing the effects of the preceding response and the preceding stimulus to be highly correlated. We therefore compared the effects on choice history and stimulus history induced by trials were perception was *stimulus-incongruent*, since those trials lead to opposite predictions regarding the perceptual choice at the subsequent trial. 

As expected, perceptual choices were attracted toward perceptual choices at preceding stimulus-incongruent trials (i.e., a positive effect of choice history). By contrast, perceptual choices tended to be repelled away from the stimulus presented the the preceding trial. This repulsion of choices away from stimuli presented at stimulus-incongruent trials confirms that the choices at stimulus-incongruent trials were the primary driver of serial effects in perception in both humans and mice.

We now refer to our analysis on stimulus history as an additional confound, which we present in detail in the Supplement of our manuscript, in the subsection X.X of the Results:

- Results: **Moreover, we ensured that that the serial effects reported here were driven by the choices reported as opposed to the stimuli presented at the preceding trial (Supplemental Secion X.X).**

- Supplement: **In our main manuscript, we investigated the effects of perceptual history, which we defined as the impact of the choice at the preceding trial on the choice at the current trial (henceforth *choice history*). Importantly, an alternative approach to this could be to look at *stimulus history*, which is defined as the impact of the stimulus presented at the preceding trial on the choice at the present trial. Here, we compare the effects of choice-history reported in the main manuscript to the effects of stimulus-history in humans and in mice.** 

- **We observed a significant bias toward stimulus history (humans: `r mean_Stimulus_History`% ± `r  error_Stimulus_History`% of trials, $\beta$ = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,1]`$ ± $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,2]`$, T($`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,3]`$) = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,4]`$, p = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,5]`$; mice: `r M_mean_Stimulus_History`% ± `r  M_error_Stimulus_History`% of trials, $\beta$ = $`r M_STAT.Global_Stimulus_History_Accuracy$coefficients[1,1]`$ ± $`r M_STAT.Global_Stimulus_History_Accuracy$coefficients[1,2]`$, T($`r M_STAT.Global_Stimulus_History_Accuracy$coefficients[1,3]`$) = $`r M_STAT.Global_Stimulus_History_Accuracy$coefficients[1,4]`$, p = $`r M_STAT.Global_Stimulus_History_Accuracy$coefficients[1,5]`$). The bias toward stimulus history was smaller than the bias toward choice history (humans: $\beta$ = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,1]`$ ± $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,2]`$, T($`r STAT.Difference_Stimulus_Choice_History$coefficients[1,3]`$) = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,4]`$, p = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,5]`$; mice: T(`r M_STAT.Difference_Stimulus_Choice_History$parameter`) = `r M_STAT.Difference_Stimulus_Choice_History$statistic`, p = $`r M_STAT.Difference_Stimulus_Choice_History$p.value`$ ).**

- **The attraction of choices toward stimuli presented at preceding trial is expected, as perception was *stimulus-congruent* on approximately 75% of trials, causing choices and stimuli to be highly correlated. We therefore compared the effects on choice history and stimulus history induced by trials were perception was *stimulus-incongruent* (i.e., *error* trials), since those trials lead to opposite predictions regarding the perceptual choice at the subsequent trial.** 

- **As expected form the findings presented in the main manuscript, perceptual choices were attracted toward perceptual choices at preceding stimulus-incongruent trials (i.e., a positive effect of choice history; humans: $\beta$ = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum_Choice_History$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,4]`$: mice: $\beta$ = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,4]`$). By contrast, perceptual choices tended to be repelled away from the stimulus presented the preceding trial (i.e., a negative effect of stimulus history; humans: $\beta$ = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,4]`$: mice: $\beta$ = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,4]`$). This repulsion of choices away from stimuli presented at stimulus-incongruent trials confirmed that the choices at stimulus-incongruent trials were the primary driver of serial effects in perception in both humans and mice.**

- **In sum, the above results suggest that, in both humans and mice, serial dependencies were better explained by the effects of choice history as opposed to the effects of stimulus history. This aligns with a result recently published for the IBL database, where mice were shown to follow an action-kernel as opposed to an stimulus-kernel model when integrating information across trials[@Findling2023].** 

### Comment 5: Done

**Previous work, which the authors acknowledges in their Discussion (6.5), distinguishes repetitive history biases from alternating biases. For instance, in Braun, Urai & Donner (2018, JoN) participants are split into repetitive or alternating. Shouldn't the authors define the history bias in a similar fashion? The authors point out that attracting and repelling biases operate simultaneously across different timescales. However, this is not warranted given Braun et. al and other similar papers. It is not clear how this more nuanced definition of history bias would alter the conclusions.**

We would like to thank the reviewer for raising this important point. Our empirical results show that, on average, history biases tend to be repetitive (Figure 2A, Figure 4A, and the biases in the psychometric functions in the Supplemental Figure SX and SX). However, this does not rule out the possibility that there are periods of alternating biases. In fact, our central finding of autocorrelation in history-congruence does not distinguish between alternating and repetitive history biases. In the plot below, we show autocorrelation curves for fluctuations history-congruence for both alternation (hazard = 0.8) and repetition (hazard = 0.2). Both were simulated for 10 blocks of a random duration between 15 and 30 trials, interleaved with 10 blocks with no history biases (hazard = 0.5). This simulation illustrates that the autocorrelation of alternating and repeating biases is identical for symmetric pairs of hazard rates. The autocorrelation of history-congruence and the associated internal mode processing is therefore not tied to repeating biases, but accommodates alternating biases as well (which seem to be, on average and in our data, less frequent). We therefore did not separate alternating from repeating biases in our analysis of mode. 

We apologize for not having recognized this aspect in the previous version of the discussion.  We have rewritten the paragraph on alternating and repeating biases in the discussion, referring to the plot below, which we have added to the Supplemental Materials. 

**Our empirical results indicate that, in the data analyzed here, history biases tend to be repetitive (Figure 2A, Figure 4A, and the biases in the psychometric functions in the Supplemental Figure SX and SX). However, as we show in Supplemental Figure SX, fluctuations in both alternating and repeating history biases generate overlapping autocorrelation curves. Our analysis of between-mode fluctuations is therefore not tied exclusively to repeating biases, but accommodates alternating biases as well, such that both may lead to internally-biased processing and reduced sensitivity to external sensory information. To elucidate how repeating and alternating history biases both fluctuate in their impact on perceptual decision-making will be an important task for future research, since this would help to understand whether repetition and alternation are linked in terms of their computational function and neural implementation[@Fritsche2020].**

**Supplemental Figure SX**
```{r Supplemental_Figure_SX, cache = TRUE}
##
## Visualize alternating and repetitive biases 
##
if (run_visualize_alt_sim){
source("./Functions/visualize_rep_alt_sim_correction.R",
local = knitr::knit_global())

blocks = rep(c(0.9, 0.5), 10)
n_participants = 1000

Rep_Sim = data.frame()
for (subj_idx in c(1:n_participants)) {
n_trials_per_block = round(runif(length(blocks), min = 15, max = 30))
add_Rep_Sim <- visualize_rep_alt_sim_correction(blocks, n_trials_per_block)
add_Rep_Sim$subj_idx = subj_idx

Rep_Sim = rbind(Rep_Sim, add_Rep_Sim)
}
Rep_Sim$corrected <- Rep_Sim$acf - Rep_Sim$random
Rep_Sim$type = "Attraction"


blocks = rep(c(0.1, 0.5), 10)
n_participants = 1000
Alt_Sim = data.frame()
for (subj_idx in c(1:n_participants)) {
n_trials_per_block = round(runif(length(blocks), min = 15, max = 30))
add_Alt_Sim <- visualize_rep_alt_sim_correction(blocks, n_trials_per_block)
add_Alt_Sim$subj_idx = subj_idx

Alt_Sim = rbind(Alt_Sim, add_Alt_Sim)
}
Alt_Sim$corrected <- Alt_Sim$acf - Alt_Sim$random
Alt_Sim$type = "Repulsion"

Rep_Alt_Sim = rbind(Rep_Sim, Alt_Sim)


Sum_Rep_Alt_Sim <-  ddply(
Rep_Alt_Sim,
.(trial, type),
summarise,
mean = mean(corrected, na.rm = TRUE),
error = sd(corrected, na.rm = TRUE) / sqrt(length(corrected))
)

Sum_Rep_Alt_Sim <- Sum_Rep_Alt_Sim[Sum_Rep_Alt_Sim$trial != 1,]
Sum_Rep_Alt_Sim$trial = Sum_Rep_Alt_Sim$trial-1
} else {Sum_Bias_Sim <- read.csv("./Results/Sum_Rep_Alt_Sim.csv")}

p_Sum_Rep_Alt_Sim <- ggplot() +
geom_errorbar(
data = Sum_Rep_Alt_Sim,
aes(
x = trial,
ymin = mean - error,
ymax = mean + error,
color = type
),
width = 0.5,
alpha = 0.5,
size = 0.5,
) +
geom_line(
data = Sum_Rep_Alt_Sim,
aes(x = trial,
y = mean,
color = type),
linetype = "dotted"
) +
geom_hline(
yintercept = 0,
linetype = "dashed",
color = "black",
size = 0.25
) +
theme_classic(base_size = 6) + labs(
x = "Lag (Trials)",
y =
paste("Autocorrelation coefficient: History congruence"),
subtitle = NULL
) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(
breaks = seq(1, 25, by = 4),
labels = seq(1, 25, by = 4),
limits = c(0, 25)
) #+ ylim(-0.01, 0.035) 
p_Sum_Rep_Alt_Sim
```

**Supplemental Figure SX. Autocorrelation of history-congruence of alternating and repeating biases.** Here, we simulate the autocorrelation of history-congruence in `r n_participants` synthetic participants. In the attraction regime (blue), history-congruence fluctuated between 50% and 80% (blue) in interleaved blocks (10 blocks per condition with a random duration between 15 and 30 trials). In the repulsion regime (red), history-congruence fluctuated between 50% and 20%. The resulting autocorrelation curves for history-congruence overlap, indicating that our analysis is able to accomodate both attractive and repulsive biases. 

### Comment 6: Restructuring of defensive analyses and moves to the Supplement

**The arousal hypothesis seems to be ruled out too easily, merely in the presence of a non-monotonic "state" vs. RT pattern. Arousal can have an inverted U-shaped effect on behavioural performance and recent paper has demonstrated a non-monotonic effect of tonic arousal (baseline pupil) on RTs and accuracy (https://www.biorxiv.org/content/10.1101/2023.07.28.550956.abstract). More generally, the RT and confidence analyses need to be complemented, perhaps by computational modelling using sequential sampling models, as these behavioral metrics have multiple mechanistic mappings (e.g., a fast RT might correspond to high SNR or an impulsive decisions driven by a starting point bias).**

We would like to thank the reviewer for this important point. Considering this comment and the comment X.X made by Reviewer 1, we realize that the quadratic relationship between mode and RTs/confidence do not represent a convincing defensive analysis against the potential contributions of arousal to the phenomenon that we have identified as between-mode fluctuations. Rather, we interpret the fluctuations of RTs/confidence with mode as indicative of a scenario in which betwee-mode fluctuations modulate a decision-variable that determines not only the perceptual choices, but also the speed and confidence at which they are made. 

Therefore, as a first response to this comment, we have re-phrased our assessment of RT and confidence in the following way:

- **The above results point to the existence of systematic fluctuations in the *decision variable*[@Kepecs2008] that determines perceptual choices, leading to enhanced sensitivity to external stimulus information during external mode and increased biases toward preceding choices during internal mode. As such, fluctuations in mode should influence downstream aspects of behavior and cognition that operate on the perceptual decision variable[@Kepecs2008]. To test this hypothesis with respect to motor behavior and metacognition, we asked how between-mode fluctuations relate to response times (RTs) and confidence reports. (...). **

- **(...) In sum, the above results indicate that reporting behavior and metacognition do not map linearly onto the mode of sensory processing, suggesting that slow fluctuations in the respective impact of external and internal information are most likely to affect perception at an early level of sensory analysis[@St.John-Saaltink2016; @Cicchini2021]. Such low-level processing may thus integrate perceptual history with external inputs into a decision variable[@Kepecs2008] that influences not only perceptual choices, but also the speed and confidence at which they are made.** 

- **In what follows, we probe alternative explanations for between-mode fluctuations, test for the existence of modes in mice, and propose a predictive processing model that explains fluctuations in mode ongoing shifts in the precision afforded to external sensory information relative to internal predictions driven by perceptual history.**

Second, we have re-structured the section of defensive analyses, where we contrast the phenomenon that we identify as fluctuating modes in perception with (i) sterotypical or random responses as evidence of low task engagement, (ii) insufficient training on the task, (iii), a post-perceptual strategy applied in situations of low confidence, and (iv), the effects of stimulus history. To streamline the manuscript, we report those analyses in detail in the supplement. 

- **TEXT**

Following the comment X.X by Reviewer 1, we now use the predictive coding definition of attention to explain the quadratic relationship of mode to RTs/confidence[@Feldman2010]. Based on the Bayesian formulation of drift diffusion[@Bitzer2014], we propose that the effects of likelihood on prior precision on the decision variable and, consequently, on confidence and RTs can be transferred to mechanistic framework of drift diffusion. Specifically, we relate likelihood precision to noise in the accumulation process, and prior precision to the amount of shift in the starting point (see also our response to comment X.X of Reviewer 3). As a third response to this comment, we have re-written our discussion of the quadratic relationship of mode to RTs and Confidence, focusing on predictive coding models attention, which we relate to mechanistic drift diffusion models: 

**How does attention relate between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. As outlined above, between-mode fluctuations can be understood as ongoing shifts in the precision afforded to likelihood (*external mode*) and prior (*internal mode*), respectively. When the precision afforded to prior or likelihood increases, posterior precision increases, which leads to faster RTs and higher confidence. When defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may thus provide a plausible explanation for the quadratic relationship of mode to RTs and confidence (Figure 2H and J; Figure 4I, Figure 6I). The effects of attention in the predictive processing sense can be directly related to mechanistic drift diffusion models[@Bitzer2014], where both larger shifts in starting point (related to icnreased prior precision in internal mode) and lower noise in the accumulation of evidence (related to increased likelihood precision in external mode) may explain faster and more confident responses.**

Given the correspondence between normative Bayesian and mechanistic drift diffusion model, we believe that we would not gain additional insights into the role of arousal and additional potential confounds or causes of between-mode fluctuations by fitting drift diffusion as an alternative class of behavioral models to our data. To understand the relation of arousal to the bimodal inference, we think that it is necessary to look at data beyond behavior, such as pupillometry, video tracking of response behavior, or neural data. While this is beyond the scope of the current manuscript, we plan to do these analyses in a follow-up paper, using recently published data from the IBL, which now contains eye tracking, video tracking of response behavior, and neuropixel recordings across the whole mouse brain[@IBL2023]. As a fourth response to this comment, we have therefore re-written our discussion of attention and arousal:

- **Outside of the predictive processing field, attention is often understood in the context of engagement with the task[@Matthew2010], which may vary according the availability of cognitive resources that are modulated by factors such as tonic arousal, fatigue or familiarity with the task. Importantly, our results indicate that internal mode processing can be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, internal mode was not associated with a general increase in lapse (Supplemental Figures SX), arguing against the proposition that internal mode is associated with random or stereotypical responses that may reflect low task engagement.** 

- **In interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects task performance[@Yerkes1908]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. Of note, high phasic arousal has been shown to suppress biases in decision-making in humans and mice across domains[@deGee2014; @deGee2017; @deGee2020], including biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. Future work should therefore more direct measures of arousal (such as pupil size[@deGee2014; @Urai2017; @deGee2020; @deGee2022; @Beerendonk2023], motor behavior[@deGee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.** 

### Comment 7: TODO when working on main text

**In several analysis the authors present an effect and then show that this effecs persists when key variables/ design aspects are also taken into account (see an example at around line 70). It makes more sense to present only one single analysis in which these key variables are controlled for. Results cannot be interpreted if they are spurious factors driving them so it is not clear why some of the results are presented in two versions ("uncontrolled" and "controlled" analyses).**

We apologize for this. We have updated our manuscript accordingly and have omitted, whenever possible, reports of uncontrolled analyses. 

### Comment 8: Done

The central empirical finding is potentially important but is currently shadowed by more speculative sections/ discussions. For instance, the section on the adaptive merits of the computational model is relatively weaker compared to the empirical results. In particular, the model is simulated without feedback (whereas most experiments employ trial by trial feedback) and does not outperform the baseline model in accuracy but in other secondary metrics.

We aggree with the Reviewer (see also a similar suggestion by Reviewer 1 in Comment X.X.). We have removed the section 5.8 from our manuscript and will develop a model on the potential function of between-mode fluctuations in a separate publication. 

**Minor comments:**

### Comment 9: Done
**The amount of statistical analysis and results is often overwhelming. The authors could streamline the presentation better such that the main result is brought to the foreground. Currently the manuscript resembles a technical report.**

We apologize to this. From the Results, we have moved a number of sections to the Supplemental Materials to stream-line the manuscript (i.e., our analysis of general response biases [section 5.4], the analysis of psychometric functions [section 5.5], and the respective paragraph on the IBL database). From the Discussion, we have removed the section on self-organized criticality (following the suggestion of Reviewer 1).

### Comment 10: Done
**Some typos or omissions may alter the meaning in various places. Indicatively, in lines 273, 439, 649.**
Thanks a lot, we have corrected these typos. 

\newpage
# References