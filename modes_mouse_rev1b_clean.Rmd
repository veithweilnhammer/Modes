---
bibliography: References/library.bib
csl: References/nature.csl
header-includes:
- \usepackage{setspace}\doublespacing
- \usepackage[fontsize=12pt]{scrextend}
- \usepackage[left]{lineno}
- \usepackage[labelformat=empty]{caption}
- \usepackage{longtable}
- \usepackage{booktabs}
always_allow_html: yes
output:
  word_document: default
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: cerulean
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
---

```{r setup, include=FALSE}

##
## Latex Addition
##
# smallest_value = .Machine$double.xmin
# replace p = \(0\) with p < \(\ensuremath{2.2\times 10^{-308}}\)

##
## Table T2
##
# \begingroup\fontsize{7}{9}\selectfont
# 
# \begin{longtable}[t]{ll}
# \toprule
# Parameters & Interpretation\\
# \midrule
# \endfirsthead
# \multicolumn{2}{@{}l}{\textit{(continued)}}\\
# \toprule
# Parameters & Interpretation\\
# \midrule
# \endhead
# 
# \endfoot
# \bottomrule
# \endlastfoot
# \(\alpha\) & Sensitivity to sensory information\\
# H & Expected probability of a switch in the cause of sensory information (Hazard)\\
# \(a_{LLR}\) & Amplitude of fluctuations in likelihood precision \(\omega_{LLR}\)\\
# \(a_{\psi}\) & Amplitude of fluctuations in prior precision \(\omega_{\psi}\)\\
# f & Frequency of \(\omega_{LLR}\) and \(\omega_{\psi}\)\\
# p & Phase (p for \(\omega_{LLR}\); p + \(\pi\)  for \(\omega_{\psi}\))\\
# \(\zeta\) & Inverse decision temperature\\*
# \end{longtable}
# \endgroup{}

knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
options(scipen = -1, digits = 2)

##
## Global settings: what to do in R markdown (compute primary statistics vs. load data from disc)
##

##
## ROOT
##
#root = "/home/veithweilnhammer/Desktop/Modes Data/"
#root = "E:/Modes Data/"
root = "C:/Users/Veith Weilnhammer/Downloads/"

##
## Load/save fully preprocessed data
##
load_summary_data = TRUE
save_summary_data = FALSE

##
## Human Data
##
collect_data = FALSE
n_permutations = 100

compute_slider = FALSE
sliding_window = 10

additional_autocorrelations = FALSE

load_data = FALSE

compute_logreg = FALSE
compute_Tw_LogReg = FALSE
compute_sine_wave_fit = FALSE
compute_power_spectra = FALSE
compute_group_acf = FALSE
compute_training_history = FALSE
compute_metacognitive_sensitivity = FALSE

extract_additional_Slider_Data = FALSE
compute_sliding_log = FALSE


##
## Mouse data
##

preprocess_mouse_data = FALSE
compute_slider_mouse_data = FALSE
filter_mouse_data = FALSE
compute_pretraining_data_mouse = FALSE

load_mouse_data = FALSE
apply_mouse_exclusion_criteria = FALSE

compute_mouse_group_acf = FALSE
compute_mouse_power_spectra = FALSE
compute_mouse_logreg = FALSE
compute_slider_History_Accuracy_lmer = FALSE
compute_mouse_Tw_LogReg = FALSE
mouse_compute_RT_Accuracy_History = FALSE
compute_mouse_training_history = FALSE
extract_mouse_additional_Slider_Data = FALSE

##
## Optimization
##
run_optim_human = FALSE
evaluate_human = FALSE

run_optim_mouse = FALSE
evaluate_mouse = FALSE

## generate output from fitted data
generate_output_human = FALSE
run_logreg_Confidence_mu_minus_1 = FALSE 

generate_output_mouse = FALSE
run_mouse_logreg_mu_minus_1 = FALSE

##
## Simulation
##
# visualize bias 
run_visualize_bias_sim = FALSE
run_visualize_alt_sim = FALSE

# Adaptive simulation
run_adaptive_simulation = FALSE
 
# Posterior Parameters
run_simulation = FALSE
compute_power_spectra_simulation = FALSE
run_control_simulation = FALSE
compute_simulation_control_group_acf = FALSE

visualize_circular_inference = FALSE 

#### General Markdown Settings
library(pander)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)

library(knitcitations)
cleanbib()
cite_options(citation_format = "pandoc", check.entries = FALSE)
library(bibtex)
```

```{r libraries}
library(lme4)
library(afex)
library(ppcor)
library(optimx)
library(sjPlot)
library(sjlabelled)
library(sjmisc)
library(effects)
library(quickpsy)
library(pracma)

library(ggplot2)
library(ggridges)
library(ggExtra)
library(gridExtra)
library(ggpubr)
#library(kableExtra)

library(tidyr)
library(plyr)

library(e1071)
library(readxl)

# red: "#E41A1C"
# blue: "#377EB8"
# green: "#66C2A5"
# orange:  "#FC8D62"
  
# library("RColorBrewer")
#   display.brewer.pal(n = 8, name = 'Set2')
#   brewer.pal(n = 8, name = 'Set2')
```

```{r functions}
source("./Functions/helper_functions.R", local = knitr::knit_global())
```

```{r load_and_prepare_data}
##
## loads data from the Confidence database and computes autocorrelations
##
Studies <-
  data.frame(read_excel("Database_Information.xlsx", skip = 0))
Experiments <- Studies

if (collect_data) {
  source("./Functions/f_collect_data.R", local = knitr::knit_global())
  PwData <- f_collect_data(Experiments)
}
```

```{r additional_autocorrelations, cache = TRUE}
##
## add additional autocorrelations (External Stimuli and binarized Difficulty)
##

if (additional_autocorrelations) {
  source("./Functions/f_add_autocorrelations.R", local = knitr::knit_global())
  PwData <- f_add_autocorrelations(PwData)
  
  source("./Functions/f_correct_autocorrelations.R", local = knitr::knit_global())
  PwData <- f_correct_autocorrelations(PwData)
}
```

```{r compute_slider}
source("./Functions/f_compute_slider.R", local = knitr::knit_global())

##
## Compute Dynamic probability of stimulus- and history-congruence
##

if (compute_slider) {

for (id in unique(PwData$subject_id)) {
index = PwData$subject_id == id
print(id)

PwData$History_slider[index] <- f_compute_slider(PwData$History[index], sliding_window)
PwData$Accuracy_slider[index] <- f_compute_slider(PwData$Accuracy[index], sliding_window)
PwData$RT_slider[index] <- f_compute_slider(PwData$RT[index], sliding_window)
PwData$Confidence_slider[index] <- f_compute_slider(PwData$Confidence[index], sliding_window)
}
}
```

```{r load_preprocessed_data, cache = TRUE}
##
## load preprocessed trial-wise human data
##
if (load_data) {
  PwData <-
    read.csv(
      paste(
        root,
        "PwData_full_new_preproc_plus_slider_normalized_hardeasy_external_stimulus_history.csv",
        #"PwData_corr.csv",
        sep = ""
      )
    )
  
  ## only evaluate perceptual studies with binary perceptual responses
  PwData <-
    PwData[PwData$study_type == "Perception" &
             PwData$response_type == "bin",]
  PwData[PwData == "NaN"] = NA
}

##
## compute summary measures on human data + apply exlcusion based on performance
##
if (!load_summary_data) {
  Behav <-
    ddply(
      PwData,
      .(subject_id, study_id),
      summarise,
      History = sum(History, na.rm = TRUE) / length(History) * 100,
      Min_History = min(History_slider, na.rm = TRUE) * 100,
      Max_History = max(History_slider, na.rm = TRUE) * 100,
      Accuracy = sum(Accuracy, na.rm = TRUE) / length(Accuracy) * 100,
      Min_Accuracy = min(Accuracy_slider, na.rm = TRUE) * 100,
      Max_Accuracy = max(Accuracy_slider, na.rm = TRUE) * 100,
      Stimulus_History = mean(Stimulus_History, na.rm = TRUE) * 100,
      Stimulus_Bias = abs(0.5 - sum(Stimulus == max(Stimulus), na.rm = TRUE) /
                            length(Stimulus)) * 100,
      Response_Bias = abs(0.5 - sum(Response == max(Response), na.rm = TRUE) /
                            length(Response)) * 100
    )
  
  Behav$Bias = Behav$Response_Bias - Behav$Stimulus_Bias + 50
  
  Behav$null_History = Behav$History - 50
  
  Behav_diff <-
    ddply(
      PwData[!is.na(PwData$Accuracy),],
      .(subject_id, study_id, Accuracy),
      summarise,
      History = sum(History, na.rm = TRUE) / length(History) * 100
    )
  
  ##
  ## exclude studies with stimulus-congruence below chance level (unclear response labels?)
  ##
  
  Study_Behav <-
    ddply(
      Behav,
      .(study_id),
      summarise,
      mean_Accuracy = mean(Accuracy, na.rm = TRUE),
      mean_History = mean(History, na.rm = TRUE),
      error_Accuracy = sd(Accuracy, na.rm = TRUE) / sqrt(length(Accuracy)),
      error_History = sd(History, na.rm = TRUE) / sqrt(length(History)),
      mean_id = mean(subject_id)
    )
  
  exclusion_list = Study_Behav[Study_Behav$mean_Accuracy < 50,]$study_id
  
  Behav[Behav$study_id %in% exclusion_list,] = NA
  Study_Behav[Study_Behav$study_id %in% exclusion_list,] = NA
  
  
  ##
  ## study-wise effect of perceptual history
  ##
  
  Study_Behav$estimate = NA
  Study_Behav$p = NA
  for (study_idx in unique(Study_Behav$study_id[!is.na(Study_Behav$study_id)])) {
    T_test_save = t.test(Behav[Behav$study_id == study_idx,]$History - 50)
    Study_Behav$estimate[Study_Behav$study_id == study_idx] <-
      T_test_save$estimate
    Study_Behav$p[Study_Behav$study_id == study_idx] <-
      T_test_save$p.value
    
  }
  
  Behav$study_id <- as.factor(Behav$study_id)
  
  ##
  ## exclude_studies with low accuracy
  ##
  
  '%ni%' <- Negate('%in%')
  PwData <- PwData[PwData$study_id %ni% exclusion_list,]
  '%ni%' <- Negate('%in%')
  PwData <- PwData[PwData$study_id %ni% exclusion_list,]
  PwData[PwData$Trial == 1,]$History = NA
  
  n_trials_human = nrow(PwData)
  n_studies_human = length(unique(PwData$study_id))
  n_participants_human = length(unique(PwData$subject_id))
  
  if (save_summary_data) {
    save(
      Behav,
      Behav_diff,
      Study_Behav,
      exclusion_list,
      n_trials_human,
      n_studies_human,
      n_participants_human,
      file = "./Summary_Data/preprocessed_data_human.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/preprocessed_data_human.Rdata")
}

if (load_data){  
  
  PwData <- PwData[PwData$study_id %in% Behav$study_id,]
  
  
  n_trials_human = nrow(PwData)
  n_studies_human = length(unique(PwData$study_id))
  n_participants_human = length(unique(PwData$subject_id))
  
  PwData[PwData$study_id %in% c(72, 77, 78, 105, 106, 117, 125, 135, 137, 138, 144),]$Difficulty = -PwData[PwData$study_id %in% c(72, 77, 78, 105, 106, 117, 125, 135, 137, 138, 144),]$Difficulty
  
  PwData[PwData$study_id == 14,]$Difficulty <- scale(abs(PwData[PwData$study_id == 14,]$Difficulty))
}
```

# Title Page

**Bimodal Inference in Humans and Mice** \
\

<br/><br/>
**Authors**: 

Veith Weilnhammer$^{1,2}$, Heiner Stuke$^{1,2}$, Kai Standvoss$^{1,3,5}$,  Philipp Sterzer$^{6}$\
\
<br/><br/>
**Affiliations**: 

$^{1}$ Department of Psychiatry, Charité-Universitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu Berlin, 10117 Berlin, Germany \
$^{2}$ Berlin Institute of Health, Charité-Universitätsmedizin Berlin and Max Delbrück Center, 10178 Berlin, Germany \
$^{3}$ Bernstein Center for Computational Neuroscience, Charité-Universitätsmedizin Berlin, 10117 Berlin, Germany\
$^{4}$ Berlin School of Mind and Brain, Humboldt-Universität zu Berlin, 10099 Berlin, Germany\
$^{5}$ Einstein Center for Neurosciences Berlin, 10117 Berlin, Germany\
$^{6}$ Department of Psychiatry (UPK), University of Basel, Switzerland\
\
<br/><br/>

**Corresponding Author**:

Veith Weilnhammer, Department of Psychiatry, Charité Campus Mitte, Charitéplatz 1, 10117 Berlin, phone: 0049 (0)30 450 517 317, email: veith.weilnhammer@gmail.com \

\newpage

\linenumbers

# Abstract

Perception is known to cycle through periods of enhanced and reduced sensitivity to external information. Here, we asked whether such slow fluctuations arise as a noise-related epiphenomenon of limited processing capacity or, alternatively, represent a structured mechanism of perceptual inference. Using two large-scale datasets, we found that humans and mice alternate between externally- and internally-oriented modes of sensory analysis. During external mode, perception aligns more closely with the external sensory information, whereas internal mode is characterized by enhanced biases toward perceptual history. Computational modeling indicated that dynamic changes in mode are enabled by two interlinked factors: (i), the integration of subsequent inputs over time and, (ii), slow anti-phase oscillations in the perceptual impact of external sensory information versus internal predictions that are provided by perceptual history. We propose that between-mode fluctuations generate unambiguous error signals that enable optimal inference in volatile environments.

# One sentence summary

Humans and mice fluctuate between external and internal modes of sensory processing.

\
<br/><br/>

\newpage
# Introduction

The capacity to respond to changes in the environment is a defining feature of life[@Schrodinger1944; @Ashby1947; @Friston2013]. Intriguingly, the ability of living things to process their surroundings fluctuates considerably over time[@Palva2011; @VanRullen2016]. In humans and mice, perception[@Verplanck1952; @Atkinson1963;  @Dehaene1993; @Gilden1995; @Gilden1995a; @Monto2008; @Ashwood2022], cognition[@Gilden2001] and memory[@Duncan2012] cycle through prolonged periods of enhanced and reduced sensitivity to external information, suggesting that the brain detaches from the world in recurring intervals that last from milliseconds to seconds and even minutes[@Palva2011]. Yet breaking from external information is risky, as swift responses to the environment are often crucial to survival.

What could be the reason for these fluctuations in perceptual performance[@Monto2008]? First, periodic fluctuations in the ability to parse external information[@ClareKelly2008; @Monto2008; @Hesselmann2008] may arise simply due to bandwidth limitations and noise. Second, it may be advantageous to actively reduce the costs of neural processing by seeking sensory information only in recurring intervals[@Schroeder2010a], otherwise relying on random or stereotypical responses to the external world. Third, spending time away from the ongoing stream of sensory inputs may also reflect a functional strategy that facilitates flexible behavior and learning[@Honey2017]: Intermittently relying more strongly on information acquired from past experiences may enable agents to build up stable internal predictions about the environment despite an ongoing stream of external sensory signals[@Weilnhammer2021a]. By the same token, recurring intervals of enhanced sensitivity to external information may help to detect changes in both the state of the environment and the amount of noise that is inherent in sensory encoding[@Weilnhammer2021a]. 

In this work, we sought to elucidate whether periodicities in the sensitivity to external information represent an epiphenomenon of limited processing capacity or, alternatively, result from a structured and adaptive mechanism of perceptual inference. To this end, we analyzed two large-scale datasets on perceptual decision-making in humans[@Rahnev2020] and mice[@IBL2021]. When less sensitive to external stimulus information, humans and mice did not behave more randomly, but showed stronger serial dependencies in their perceptual choices[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Akrami2018; @Braun2018; @Bergen2019; @Urai2019; @Hsu2020]. These serial dependencies may be understood as driven by internal predictions that reflect the autocorrelation of natural environments[@Dong1995] and bias perception toward preceding experiences[@Burr2014; @Braun2018; @Bergen2019]. Computational modeling indicated that ongoing changes in perceptual performance may be driven by systematic fluctuations between externally- and internally-oriented *modes* of sensory analysis. We suggest that such *bimodal inference* may help to build stable internal representations of the sensory environment despite an ongoing stream of sensory information.

# Results

## Human perception fluctuates between epochs of enhanced and reduced sensitivity to external information

```{r serial_dependence, cache = TRUE}

if (!load_summary_data) {
  ##
  ## STATS on History-congruent perception
  ##
  Global_History_Accuracy <-
    lmer(null_History ~ Accuracy + (1 |
                                      study_id), data = Behav)
  true_Global_History_Accuracy <-
    lmer(History ~ Accuracy + (1 | study_id), data = Behav)
  STAT.Global_History_Accuracy = summary(Global_History_Accuracy)
  
  diff_History_Accuracy <-
    lmer(History ~ Accuracy + (1 |
                                 study_id), data = Behav_diff)
  STAT.diff_History_Accuracy = summary(diff_History_Accuracy)
  
  if (save_summary_data) {
    save(
      Global_History_Accuracy,
      true_Global_History_Accuracy,
      STAT.Global_History_Accuracy,
      diff_History_Accuracy,
      STAT.diff_History_Accuracy,
      file = "./Summary_Data/serial_dependence.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/serial_dependence.Rdata")
}
```

```{r stats_autocorrelations, cache = TRUE}

##
## STATS based on autocorrelations
##

if (!load_summary_data) {
  # compute difference to random autocorrelation and remove outliers
  PwData$diff_acf_Stimulus = exclude_3SD(PwData$acf_Stimulus - PwData$random_acf_Stimulus)
  PwData$diff_acf_History = exclude_3SD(PwData$acf_History - PwData$random_acf_History)
  PwData$diff_acf_Difficulty = exclude_3SD(PwData$acf_Difficulty - PwData$random_acf_Difficulty)
  PwData$diff_acf_Condition = exclude_3SD(PwData$acf_Condition - PwData$random_acf_Condition)
  PwData$diff_acf_RT = exclude_3SD(PwData$acf_RT - PwData$random_acf_RT)
  PwData$diff_acf_Confidence = exclude_3SD(PwData$acf_Confidence - PwData$random_acf_Confidence)
  PwData$diff_acf_HardEasy = exclude_3SD(PwData$acf_HardEasy - PwData$random_acf_HardEasy)
  PwData$diff_acf_External = exclude_3SD(PwData$acf_External - PwData$random_acf_External)
  
  # compute group-level acf
  if (compute_group_acf) {
    source("./Functions/f_compute_group_acf.R",
           local = knitr::knit_global())
    
    acf_to_test = c('diff_acf_Stimulus',  'diff_acf_History')
    max_trial = 99
    
    Summary_acf <- f_compute_group_acf(PwData, acf_to_test, max_trial)
    
  } else {
    Summary_acf <-
      read.csv(
        #"./Results/Summary_acf_study_corrected_normalized_3SD_HardEasy_External2.csv" ## extended_version: hardeasy + external / MV1
        "./Results/Summary_acf_full_corr.csv" ## /RV1b
      )
  }
  Summary_acf$Trial <- Summary_acf$Trial - 1
  
  ##
  ## Summary over exceed statistic
  ##
  ID_Summary_acf <- ddply(
    PwData,
    .(study_id, subject_id),
    summarise,
    lag_significant_Stimulus = min(which(diff(
      which(c(0, exceed_acf_Stimulus, 0) < n_permutations / 2)
    ) != 1)) - 1,
    lag_significant_History = min(which(diff(
      which(c(0, exceed_acf_History, 0) < n_permutations / 2)
    ) != 1)) - 1,
    lag_significant_Difficulty = min(which(diff(
      which(c(0, exceed_acf_Difficulty, 0) < n_permutations / 2)
    ) != 1)) - 1,
    lag_significant_External = min(which(diff(
      which(c(0, exceed_acf_External, 0) < n_permutations / 2)
    ) != 1)) - 1,
    Confidence_Stimulus_con = mean(Confidence[Accuracy == 1], na.rm = TRUE) - mean(Confidence[Accuracy == 0], na.rm = TRUE),
    RT_Stimulus_con = mean(RT[Accuracy == 1], na.rm = TRUE) -  mean(RT[Accuracy == 0], na.rm = TRUE),
    Confidence_History_con = mean(Confidence[History == 1], na.rm = TRUE) - mean(Confidence[History == 0], na.rm = TRUE),
    RT_History_con = mean(RT[History == 1], na.rm = TRUE) - mean(RT[History == 0], na.rm = TRUE)
  )
  
  ID_Summary_acf$lag_significant_Stimulus[ID_Summary_acf$lag_significant_Stimulus == Inf] = NA
  ID_Summary_acf$lag_significant_History[ID_Summary_acf$lag_significant_History == Inf] = NA
  ID_Summary_acf$lag_significant_Difficulty[ID_Summary_acf$lag_significant_Difficulty == Inf] = NA
  ID_Summary_acf$lag_significant_External[ID_Summary_acf$lag_significant_External == Inf] = NA
  
  gathercol = colnames(PwData[, c(29, 30)])
  Exceed_long  <-
    gather(PwData[, c(1, 4, 5, 29, 30)],
           "Variable",
           "Probability",
           gathercol,
           factor_key = TRUE)
  
  Summary_exceed <-  ddply(
    Exceed_long[Exceed_long$Trial != 1, ],
    .(Trial, Variable),
    summarise,
    
    mean_probability = mean(exclude_3SD((100 - Probability)), na.rm = TRUE),
    ci_probability = qnorm(0.975) * sd(exclude_3SD(100 - Probability), na.rm = TRUE) /
      sqrt(length(exclude_3SD(100 - Probability)))
  )
  Summary_exceed$Trial <- Summary_exceed$Trial - 1
  
  ##
  ## STATS: Autocorrelations decay over time
  ##
  Stimulus_linear <-
    summary(lm(Mean ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus" &
                                                   Summary_acf$Trial < 21, ]))
  Stimulus_exponential <-
    summary(lm(log(Mean) ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus" &
                                                        Summary_acf$Trial < 21, ]))
  
  History_linear <-
    summary(lm(Mean ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_History" &
                                                   Summary_acf$Trial < 21, ]))
  History_exponential <-
    summary(lm(log(Mean) ~ Trial , data = Summary_acf[Summary_acf$Variable == "diff_acf_History" &
                                                        Summary_acf$Trial < 21, ]))
  
  PwData$min_diff_acf_Stimulus = PwData$diff_acf_Stimulus - min(PwData$diff_acf_Stimulus, na.rm = TRUE)
  PwData$min_diff_acf_History = PwData$diff_acf_History - min(PwData$diff_acf_History, na.rm = TRUE)
  
  lmer_acf_Stimulus <-
    lmer(log(min_diff_acf_Stimulus) ~ Trial + (1|
                                                 study_id / subject_id),
         data = PwData[PwData$Trial > 1 & PwData$Trial < 21, ])
  
  STAT.lmer_acf_Stimulus <- summary(lmer_acf_Stimulus)
  
  lmer_acf_History <-
    lmer(log(min_diff_acf_History) ~ Trial + (1  |
                                                study_id / subject_id),
         data = PwData[PwData$Trial > 1 &
                         PwData$Trial < 21 & PwData$min_diff_acf_History > 0, ])
  STAT.lmer_acf_History <- summary(lmer_acf_History)
  
  
  ##
  ## STATS: RT as a function of stimulus- and history-congruence
  ##
  lmer_RT_Accuracy_History <-
    lmer(RT ~ Accuracy + History + (1  |
                                      study_id / subject_id),
         data = PwData)
  
  STAT.lmer_RT_Accuracy_History <- summary(lmer_RT_Accuracy_History)
  
  lmer_Confidence_Accuracy_History <-
    lmer(Confidence ~ Accuracy + History + (1 |
                                              study_id / subject_id),
         data = PwData)
  
  STAT.lmer_Confidence_Accuracy_History <- summary(lmer_Confidence_Accuracy_History)
  
  
  if (save_summary_data) {
    save(
      Summary_acf,
      ID_Summary_acf,
      Summary_exceed,
      Stimulus_linear,
      Stimulus_exponential,
      History_linear,
      History_exponential,
      lmer_acf_Stimulus,
      STAT.lmer_acf_Stimulus,
      lmer_acf_History,
      STAT.lmer_acf_History,
      #lmer_RT_Accuracy_History,
      STAT.lmer_RT_Accuracy_History,
      #lmer_Confidence_Accuracy_History,
      STAT.lmer_Confidence_Accuracy_History,
      file = "./Summary_Data/stats_autocorrelations.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/stats_autocorrelations.Rdata")
}
```

```{r logistic_regression_autocorrelation, cache = TRUE}
##
## Reproduce autocorrelation with logistic regression (correcting for difficulty and stimulus repetition)
##
if (!load_summary_data) {
   n_back = 25
  if (compute_Tw_LogReg == TRUE) {
   
    source("./Functions/f_compute_trial_wise_logreg.R",
           local = knitr::knit_global())
    Tw_LogReg <- f_compute_trial_wise_logreg(PwData, n_back)
  } else {
    #Tw_LogReg <- read.csv("./Results/Tw_LogReg.csv")
    Tw_LogReg <- read.csv("./Results/Tw_LogReg_corr.csv")
  }
  
  gathercol = colnames(Tw_LogReg[, c(5, 7)])
  Tw_LogReg_long <-
    gather(Tw_LogReg[, c(2:5, 7)],
           "Variable",
           "beta",
           gathercol,
           factor_key = TRUE)
  
  Tw_LogReg_long$Variable <-
    gsub("History_weight", "History", Tw_LogReg_long$Variable)
  Tw_LogReg_long$Variable <-
    gsub("Stimulus_weight", "Stimulus", Tw_LogReg_long$Variable)
  
  Group_Tw_LogReg_long <-  ddply(
    Tw_LogReg_long,
    .(lag, Variable),
    summarise,
    Mean = mean(exclude_3SD(beta), na.rm = TRUE),
    Error = sd(exclude_3SD(beta), na.rm = TRUE) / sqrt(length(exclude_3SD(beta)))
  )
  
  
  ## trial-wise stats
  Group_Tw_LogReg_long$p_value = NA
  for (lag_idx in c(1:n_back)) {
    Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "History" &
                           Group_Tw_LogReg_long$lag == lag_idx,]$p_value <-
      summary(lmer(exclude_3SD(History_weight) ~ 1 + (1 |
                                                        study_id) , data = Tw_LogReg[Tw_LogReg$lag == lag_idx,]))$coefficients[5]
    
    Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "Stimulus" &
                           Group_Tw_LogReg_long$lag == lag_idx,]$p_value <-
      summary(lmer(exclude_3SD(Stimulus_weight) ~ 1 + (1 |
                                                         study_id) , data = Tw_LogReg[Tw_LogReg$lag == lag_idx,]))$coefficients[5]
  }
  

  if (save_summary_data) {
    save(Group_Tw_LogReg_long,
         Tw_LogReg_long,
         Tw_LogReg,
         n_back,
         Group_Tw_LogReg_long,
         file = "./Summary_Data/log_reg_autocorrelation.Rdata")
  }
  
} else {
  load("./Summary_Data/log_reg_autocorrelation.Rdata")
}
```

We began by selecting `r n_studies_human` studies from the Confidence Database[@Rahnev2020] that investigated how human participants (N = `r n_participants_human`) perform binary perceptual decisions (Figure 1A; see Methods for details on inclusion criteria). As a metric for perceptual performance (i.e., the sensitivity to external sensory information), we asked whether the participant's response and the presented stimulus matched (*stimulus-congruent* choices) or differed from each other (*stimulus-incongruent* choices; Figure 1B and C) in a total of $`r n_trials_human/100000`$ million trials.

In a first step, we asked whether the ability to accurately perceive sensory stimuli is constant over time or, alternatively, fluctuates in periods of enhanced and reduced sensitivity to external information. We found perception to be stimulus-congruent in `r mean(Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Accuracy, na.rm = TRUE)/sqrt(length(Behav$Accuracy))`% of trials (mean ± standard error of the mean; Figure 2A), which was highly consistent across the selected studies (Supplemental Figure S1A). In line with previous work[@Dehaene1993], we found that the probability of stimulus-congruence was not independent across successive trials: At the group level, stimulus-congruent perceptual choices were significantly autocorrelated for up to `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` trials (Figure 2B), controlling for task difficulty and the sequence of presented stimuli (Supplemental Figure 2A-B). 

At the level of individual participants, the autocorrelation of stimulus-congruence exceeded the respective autocorrelation of randomly permuted data within an interval of $`r mean(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)`$ ± $`r sd(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_Summary_acf)`$ trials (Figure 2C). In other words, if a participant's experience was congruent (or incongruent) with the external stimulus information at a given trial, her perception was more likely to remain stimulus-congruent (or -incongruent) for approximately `r round(mean(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE))` trials into the future. The autocorrelation of stimulus-congruence was corroborated by logistic regression models that successfully predicted the stimulus-congruence of perception at the index trial $t = 0$ from the stimulus-congruence at the preceding trials within a lag of `r min(which(Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "Stimulus",]$p_value > 0.05))` trials (Supplemental Figure S3). 

```{r power_spectra, cache = TRUE}
##
## Spectra analysis of behavior
##
if (!load_summary_data) {
  
  if (compute_power_spectra) {
    source("./Functions/f_compute_power_spectra.R",
           local = knitr::knit_global())
    
    sliders = c("Accuracy_slider", "History_slider")
    Power_Spectra <- f_compute_power_spectra(PwData, sliders)
  } else {
    Power_Spectra <-
      read.csv(paste(root, "power_spectra_smoothed.csv", sep = ""))
  }
  
  Power_Spectra$r_freq = round(Power_Spectra$freq, digits = 4)
  
  step_size = 0.01
  bins <-
    seq(
      from = min(Power_Spectra$freq),
      to = max(Power_Spectra$freq),
      by = step_size
    )
  
  names <- round(bins, digits = 2)
  names = names[1:length(names) - 1]
  
  Power_Spectra$bin_freq <-
    cut(Power_Spectra$freq, breaks = bins, labels = names)
  Power_Spectra$bin_freq <-
    as.numeric(Power_Spectra$bin_freq) * step_size
  Power_Spectra$bin_freq[is.na(Power_Spectra$bin_freq)] = max(Power_Spectra$freq)
  
  Power_Spectra$Coherence <- Power_Spectra$Coherence * 100
  
  gathercol = colnames(Power_Spectra[, c(2, 3)])
  Power_Spectra_long  <-
    gather(Power_Spectra[, c(1, 2, 3, 6, 7, 8, 9)],
           "Variable",
           "Power",
           gathercol,
           factor_key = TRUE)
  
  Power_Frequency <-  ddply(
    Power_Spectra_long,
    .(r_freq, Variable),
    summarise,
    
    mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
    ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE) / sqrt(length(exclude_3SD(Power)))
  )
  
  Coherence_Phase_Frequency <- ddply(
    Power_Spectra,
    .(bin_freq),
    summarise,
    
    mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
    ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE) /
      sqrt(length(exclude_3SD(Coherence))),
    
    mode_phase = getmode(exclude_3SD((abs(
      Phase
    )))),
    mean_phase = getmode(exclude_3SD((abs(
      Phase
    )))),
    ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE) /
      sqrt(length(exclude_3SD(Phase)))
  )
  
  
  ##
  ## mode of coherence and phase
  ##
  Summary_Power_Spectra <-
    ddply(
      Power_Spectra[Power_Spectra$freq > 0.01 &
                      Power_Spectra$freq < 0.1, ],
      .(study_id, subject_id),
      summarise,
      mean_coherence = mean(exclude_3SD((Coherence))),
      mode_phase = getmode(exclude_3SD((abs(
        Phase
      ))))
    )
  
  
  ##
  ## STATS Power vs Frequency (1/f noise)
  ##
  lmer_power_freq_Stimulus <-
    lmer(log(Power_Stimulus) ~ log(freq) + (1 |
                                              study_id / subject_id),
         data = Power_Spectra[Power_Spectra$freq > 0.01 &
                                Power_Spectra$freq < 0.1 &
                                is.finite(log(Power_Spectra$Power_Stimulus)), ])
  
  STAT.lmer_power_freq_Stimulus <- summary(lmer_power_freq_Stimulus)
  
  lmer_power_freq_History <-
    lmer(log(Power_History) ~ log(freq) + (1 |
                                             study_id / subject_id),
         data = Power_Spectra[Power_Spectra$freq > 0.01 &
                                Power_Spectra$freq < 0.1 &
                                is.finite(log(Power_Spectra$Power_History)), ])
  
  STAT.lmer_power_freq_History <- summary(lmer_power_freq_History)
  
  if (save_summary_data) {
    save(
      Power_Spectra,
      Power_Spectra_long,
      Power_Frequency,
      Coherence_Phase_Frequency,
      Summary_Power_Spectra,
      lmer_power_freq_Stimulus,
      lmer_power_freq_History,
      STAT.lmer_power_freq_Stimulus,
      STAT.lmer_power_freq_History,
      file = "./Summary_Data/power_spectra.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/power_spectra.Rdata")
}
```

These results confirm that the ability to process sensory signals is not constant over time but unfolds in multi-trial epochs of enhanced and reduced sensitivity to external information[@Dehaene1993]. As a consequence of this autocorrelation, the dynamic probability of stimulus-congruent perception (i.e., computed in sliding windows of ± 5 trials; Figure 1C) fluctuated considerably within participants (average minimum: `r mean(Behav$Min_Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Min_Accuracy, na.rm = TRUE)/sqrt(length(Behav$Min_Accuracy))`%, maximum: `r mean(Behav$Max_Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Max_Accuracy, na.rm = TRUE)/sqrt(length(Behav$Max_Accuracy))`%). 
In line with previous findings[@Gilden1995], such fluctuations in the sensitivity to external information had a power density that was inversely proportional to the frequency in the slow spectrum[@Monto2008] (power ~ 1/$f^\beta$, $\beta$ = $`r STAT.lmer_power_freq_Stimulus$coefficients[2,1]`$ ± $`r STAT.lmer_power_freq_Stimulus$coefficients[2,2]`$, T($`r STAT.lmer_power_freq_Stimulus$coefficients[2,3]`$) = $`r STAT.lmer_power_freq_Stimulus$coefficients[2,4]`$, p = $`r STAT.lmer_power_freq_Stimulus$coefficients[2,5]`$; Figure 2D). This feature, which is also known as a *1/f power law*[@Montroll1982; @Bak1987], represents a characteristic of scale-free fluctuations in complex dynamic systems such as the brain[@Chialvo2010] and the cognitive processes it entertains[@Gilden1995; @Gilden1995a; @Gilden2001; @Wagenmakers2004; @VanOrden2005]. 

## Humans fluctuate between external and internal modes of sensory processing

In a second step, we sought to explain why perception cycles through periods of enhanced and reduced sensitivity to external information[@Palva2011]. We reasoned that observers may intermittently rely more strongly on internal information, i.e., on predictions about the environment that are constructed from previous experiences[@Bergen2019; @Weilnhammer2021a].  

In perception, *serial dependencies* represent one of the most basic internal predictions that cause perceptual decisions to be systematically biased toward preceding choices[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Akrami2018; @Braun2018; @Bergen2019; @Urai2019; @Hsu2020]. Such effects of perceptual history mirror the continuity of the external world, in which the recent past often predicts the near future[@Dong1995; @Chopin2012; @Burr2014; @Braun2018; @Bergen2019]. Therefore, as a metric for the perceptual impact of internal information, we computed whether the participant's response at a given trial matched or differed from her response at the preceding trial (*history-congruent* and *history-incongruent perception*, respectively; Figure 1B and C).  

```{r logistic_regression_outcomes, cache = TRUE}

if (!load_summary_data) {
  
 if (compute_logreg) {
source("./Functions/f_compute_logreg.R", local = knitr::knit_global())
LogAggr <- f_compute_logreg(PwData)
} else {
  LogAggr <- read.csv("./Results/LogAggr.csv")
}

LogAggr$diff_AIC = exclude_3SD(LogAggr$reduced_AIC - LogAggr$full_AIC)
LogAggr$diff_AIC = (LogAggr$reduced_AIC - LogAggr$full_AIC)

LogAggr$study_id <- as.factor(LogAggr$study_id)

Study_LogAggr <- 
  ddply(LogAggr, 
    .(study_id),
    summarise,
    mean_diff_AIC = mean(diff_AIC, na.rm = TRUE),
    error_diff_AIC = sd(diff_AIC, na.rm = TRUE)/sqrt(length(diff_AIC))
    )
Study_LogAggr[Study_LogAggr == "NaN"] = NA

STAT.LogReg = summary(lmer(diff_AIC ~ 1 + (1|study_id), data = LogAggr))

if (compute_logreg) {
full_logit_Sum <-
    glmer(Response ~ Stimulus + Response_minus_1 + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1,],
        family = binomial)
reduced_logit_Sum <-
    glmer(Response ~ Stimulus + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1,],
        family = binomial)
} else {
  full_logit_Sum <- readRDS(file = "./Results/full_logit_Sum.rds")
  reduced_logit_Sum <- readRDS(file = "./Results/reduced_logit_Sum.rds")
}
STAT.full_logit_Sum <- summary(full_logit_Sum) 
STAT.reduced_logit_Sum <- summary(reduced_logit_Sum)  
  if (save_summary_data) {
    save(STAT.full_logit_Sum, 
         STAT.reduced_logit_Sum,
         STAT.LogReg, 
         Study_LogAggr,
         LogAggr,
         file = "./Summary_Data/log_reg_outcomes.Rdata")
  }
  
} else {
  load("./Summary_Data/log_reg_outcomes.Rdata")
}
```

First, we confirmed that perceptual history played a significant role in perception despite the ongoing stream of external information. With a global average of `r mean(Behav$History, na.rm = TRUE)`% ± `r sd(Behav$History, na.rm = TRUE)/sqrt(length(Behav$History))`% history-congruent trials, we found a small but highly significant perceptual bias towards preceding experiences ($\beta$ = $`r STAT.Global_History_Accuracy$coefficients[1,1]`$ ± $`r STAT.Global_History_Accuracy$coefficients[1,2]`$, T($`r STAT.Global_History_Accuracy$coefficients[1,3]`$) = $`r STAT.Global_History_Accuracy$coefficients[1,4]`$, p = $`r STAT.Global_History_Accuracy$coefficients[1,5]`$; Figure 2A) that was largely consistent across studies (Supplemental Figure 1B) and more pronounced in participants who were less sensitive to external sensory information (Supplemental Figure 1C). 
Importantly, history-congruence was not a corollary of the sequence of presented stimuli: History-congruent perceptual choices were more frequent at trials when perception was stimulus-incongruent (`r mean(Behav_diff[Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)`% ± `r sd(Behav_diff[Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)/sqrt(length(Behav_diff[Behav_diff$Accuracy == 0,]$History))`%) as opposed to stimulus-congruent (`r mean(Behav_diff[Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)`% ± `r sd(Behav_diff[Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)/sqrt(length(Behav_diff[Behav_diff$Accuracy == 1,]$History))`%, $\beta$ = $`r STAT.diff_History_Accuracy$coefficients[2,1]`$ ± $`r STAT.diff_History_Accuracy$coefficients[2,2]`$, T($`r STAT.diff_History_Accuracy$coefficients[2,3]`$) = $`r STAT.diff_History_Accuracy$coefficients[2,4]`$, p = $`r STAT.diff_History_Accuracy$coefficients[2,5]`$; Figure 2A, lower panel). Despite being adaptive in autocorrelated real-world environments[@Dong1995; @Burr2014; @Cicchini2018; @Weilnhammer2021a], perceptual history thus represented a source of bias in the randomized experimental designs studied here[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019]. These serial biases were effects of choice history, i.e., driven by the experiences reported at the preceding trial, and could not be attributed to stimulus history, i.e., to effects of the stimuli presented at the preceding trial (Supplemental Section 9.1).

Second, we asked whether perception cycles through multi-trial epochs during which perception is characterized by stronger or weaker biases toward preceding experiences. In close analogy to stimulus-congruence, we found history-congruence to be significantly autocorrelated for up to `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` trials (Figure 2B), while controlling for task difficulty and the sequence of presented stimuli (Supplemental Figure 2A-B). In individual participants, the autocorrelation of history-congruence was elevated above randomly permuted data for a lag of $`r mean(ID_Summary_acf$lag_significant_History, na.rm = TRUE)`$ ± $`r sd(ID_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_Summary_acf)`$ trials (Figure 2C), confirming that the autocorrelation of history-congruence was not only a group-level phenomenon. The autocorrelation of history-congruence was corroborated by logistic regression models that successfully predicted the history-congruence of perception at an index trial $t = 0$ from the history-congruence at the preceding trials within a lag of `r min(which(Group_Tw_LogReg_long[Group_Tw_LogReg_long$Variable == "History",]$p_value > 0.05))` trials (Supplemental Figure S3).  

Third, we asked whether the impact of internal information fluctuates as a scale-invariant process with a 1/f power law (i.e., the feature typically associated with fluctuations in the sensitivity to external information[@Gilden1995; @Gilden1995a; @Gilden2001; @Wagenmakers2004; @VanOrden2005]). The dynamic probability of history-congruent perception (i.e., computed in sliding windows of ± 5 trials; Figure 1C) varied considerably over time, ranging between a minimum of `r mean(Behav$Min_History, na.rm = TRUE)`% ± `r sd(Behav$Min_History, na.rm = TRUE)/sqrt(length(Behav$Min_History))`% and a maximum `r mean(Behav$Max_History, na.rm = TRUE)`% ± `r sd(Behav$Max_History, na.rm = TRUE)/sqrt(length(Behav$Max_History))`%. In analogy to stimulus-congruence, we found that history-congruence fluctuated as at power densities that were inversely proportional to the frequency in the slow spectrum[@Monto2008] (power ~ 1/$f^\beta$, $\beta$ = $`r STAT.lmer_power_freq_History$coefficients[2,1]`$ ± $`r STAT.lmer_power_freq_History$coefficients[2,2]`$, T($`r STAT.lmer_power_freq_History$coefficients[2,3]`$) = $`r STAT.lmer_power_freq_History$coefficients[2,4]`$, p = $`r STAT.lmer_power_freq_History$coefficients[2,5]`$; Figure 2D). 

```{r slider_History_Accuracy, cache = TRUE}
##
## Compute Association between History- and Accuracy_slider
##

if (!load_summary_data) {
  
slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|study_id/subject_id), data = PwData)
STAT.slider_History_vs_Accuracy <- summary(slider_History_vs_Accuracy)

slider_History_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ History_slider + Preferred_slider + (1|study_id/subject_id), data = Slider_Data)
STAT.slider_History_Preferred_vs_Accuracy <- summary(slider_History_Preferred_vs_Accuracy)
  
  if (save_summary_data) {
    save(slider_History_vs_Accuracy, 
         STAT.slider_History_vs_Accuracy,  
         file = "./Summary_Data/slider_History_Accuracy.Rdata")
  }
  
} else {
  load("./Summary_Data/slider_History_Accuracy.Rdata")
}
```

```{r bias_slider, cache = TRUE, warning = FALSE}

##
## relation of history-dependent biases and general response biases to fluctuations in stimulus-congruency
##

if (!load_summary_data) {

if (extract_additional_Slider_Data == TRUE) {
Slider_Data = PwData[, c(1, 4, 5, 7, 36, 37)]

Slider_Data$Preferred = NA
Slider_Data$Preferred_slider = NA
for (subj_idx in unique(Slider_Data$subject_id)) {
print(subj_idx)
responses = Slider_Data[Slider_Data$subject_id == subj_idx, ]$Response

outcomes = unique(responses)

n = rep(NA, length(outcomes))
for (outcome_idx in c(1,length(outcomes))) {
n[outcome_idx] = sum(responses == outcomes[outcome_idx])
}

Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred = rep(0, length(responses))
Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred[responses == outcomes[which(n == max(n))]] = 1
Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred_slider = f_compute_slider(Slider_Data[Slider_Data$subject_id == subj_idx, ]$Preferred, sliding_window)
}
} else {Slider_Data <- read.csv(paste(root, "Slider_Data.csv", sep = ""))}

slider_History_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ History_slider + Preferred_slider + (1|study_id/subject_id), data = Slider_Data)
STAT.slider_History_Preferred_vs_Accuracy <- summary(slider_History_Preferred_vs_Accuracy)

slider_History_vs_Preferred <- lmer(History_slider ~ Preferred_slider + (1|study_id/subject_id), data = Slider_Data)
STAT.slider_History_vs_Preferred <- summary(slider_History_vs_Preferred)

Global_History_Preferred_vs_Bias <- lmer(Accuracy ~ History + Bias + (1|study_id), data = Behav)
STAT.Global_History_Preferred_vs_Bias <- summary(Global_History_Preferred_vs_Bias)

    Variance_History_General <- 
    ddply(
    Slider_Data, 
    .(subject_id, study_id),
    summarise,
    var_History = sd(History_slider, na.rm = TRUE) - sd(Preferred_slider, na.rm = TRUE)
  )

Variance_History_vs_Preferred <- lmer(var_History ~ 1 + (1|study_id), data = Variance_History_General)
STAT.Variance_History_vs_Preferred <- summary(Variance_History_vs_Preferred)
  
  
  if (save_summary_data) {
    save(Variance_History_General,
         STAT.Variance_History_vs_Preferred,
         STAT.Global_History_Preferred_vs_Bias,
         STAT.slider_History_vs_Preferred,
         STAT.slider_History_Preferred_vs_Accuracy,
         file = "./Summary_Data/bias_slider.Rdata")
  }
  
} else {
  load("./Summary_Data/bias_slider.Rdata")
}
```

Finally, we ensured that fluctuations in stimulus- and history-congruence are linked to each other. When perceptual choices were less biased toward external information, participants relied more strongly on internal information acquired from perceptual history (and vice versa, $\beta$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$, controlling for fluctuations in general response biases; Supplemental Section 9.2). 
Thus, while sharing the 1/f power law characteristic, fluctuations in stimulus- and history-congruence were shifted against each other by approximately half a cycle and showed a squared coherence of $`r mean(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)`$ ± $`r sd(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(Summary_Power_Spectra$mean_coherence)`$% (Figure 2E and F; we report the average phase and coherence for frequencies below 0.1 $1/N_{trials}$; see Methods for details).

```{r summary_internal_external_mode, cache = TRUE}
##
## Create summary_variable for Modes
##

if (!load_summary_data) {
  
PwData <-  ddply(
      PwData,
      .(subject_id),
      mutate,
      
      directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
      scaled_directed_mode = scale(directed_mode),
      strength_mode = abs(scaled_directed_mode)
      )

ID_mode <-  ddply(
      PwData,
      .(subject_id),
      summarise,
      strength_mode = mean(strength_mode, na.rm = TRUE),
      directed_mode = mean(directed_mode, na.rm = TRUE)
)  
  
  if (save_summary_data) {
    save(ID_mode,  file = "./Summary_Data/Summary_mode.Rdata")
  }
  
} else {
  load("./Summary_Data/Summary_mode.Rdata")
}
```

In sum, our analyses indicate that perceptual decisions may result from a competition between external sensory signals with internal predictions provided by perceptual history. We show that the impact of these external and internal sources of information is not stable over time, but fluctuates systematically, emitting overlapping autocorrelation curves and antiphase 1/f profiles.

These links between stimulus- and history-congruence suggest that the fluctuations in the impact of external and internal information may be generated by a unifying mechanism that causes perception to alternate between two opposing *modes*[@Honey2017] (Figure 1D): During *external mode*, perception is more strongly driven by the available external stimulus information. Conversely, during *internal mode*, participants rely more heavily on internal predictions that are implicitly provided by preceding perceptual experiences. The fluctuations in the degree of bias toward external versus internal information created by such *bimodal inference* may thus provide a novel explanation for ongoing fluctuations in the sensitivity to external information[@Palva2011; @VanRullen2016; @Honey2017].

## Internal and external modes of processing facilitate response behavior and enhance confidence in human perceptual decision-making

```{r EI_RT_Confidence, cache = TRUE}

##
## Confidence and RT vs mode
##
if (!load_summary_data) {
  
PwData$clear_RT = PwData$RT
PwData$clear_RT = exclude_3SD(PwData$clear_RT)

PwData$clear_Confidence = PwData$Confidence
PwData$clear_Confidence = exclude_3SD(PwData$clear_Confidence)

Post_Perceptual_Modes = ddply(
      PwData,
      .(directed_mode),
      summarise,
      average_RT = mean(clear_RT, na.rm = TRUE),
      se_RT = sd(exclude_3SD(clear_RT), na.rm = TRUE)/sqrt(length(clear_RT)),
      average_Confidence = mean(clear_Confidence, na.rm = TRUE),
      se_Confidence = sd(exclude_3SD(clear_Confidence), na.rm = TRUE)/sqrt(length(clear_Confidence)),
      n = length(clear_Confidence),
      n_percent = (length(clear_Confidence)/nrow(PwData))*100
      )  

##
## Confidence/RT vs mode: STATS and plot preparation
##

RT_vs_mode <- lmer(clear_RT ~ poly(directed_mode, 2) + (1|study_id/subject_id), data = PwData[!is.na(PwData$directed_mode),])
STAT.RT_vs_mode <- summary(RT_vs_mode)

Confidence_vs_mode <- lmer(clear_Confidence ~ poly(directed_mode, 2) + (1|study_id/subject_id), data = PwData[!is.na(PwData$directed_mode),])
STAT.Confidence_vs_mode <- summary(Confidence_vs_mode)


RT_Confidence_Behav <- 
  ddply(
    PwData[!is.na(PwData$History) & !is.na(PwData$Accuracy),], 
    .(subject_id, study_id),
    summarise,
    diff_Confidence_History = mean(clear_Confidence[History == 1], na.rm = TRUE) -  mean(clear_Confidence[History == 0], na.rm = TRUE),
    diff_Confidence_Stimulus = mean(clear_Confidence[Accuracy == 1], na.rm = TRUE) -  mean(clear_Confidence[Accuracy == 0], na.rm = TRUE),
    diff_RT_History = mean(clear_RT[History == 1], na.rm = TRUE) -  mean(clear_RT[History == 0], na.rm = TRUE),
    diff_RT_Stimulus = mean(clear_RT[Accuracy == 1], na.rm = TRUE) -  mean(clear_RT[Accuracy == 0], na.rm = TRUE),
    confidence_bias = mean(clear_Confidence, na.rm = TRUE),
    mode_strength_bias = mean(strength_mode, na.rm =TRUE))

gathercol = colnames(RT_Confidence_Behav[, c(3,4)])
RT_Confidence_Behav_long_C  <-
  gather(RT_Confidence_Behav[,c(1,3,4)],
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)
RT_Confidence_Behav_long_C$Type = "Confidence"

gathercol = colnames(RT_Confidence_Behav[, c(5,6)])
RT_Confidence_Behav_long_R  <-
  gather(RT_Confidence_Behav[,c(1,5,6)],
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)
RT_Confidence_Behav_long_R$Type = "RT"

RT_Confidence_Behav_long <- rbind(RT_Confidence_Behav_long_R, RT_Confidence_Behav_long_C)

RT_Confidence_Behav_long$Variable <-
  gsub("diff_RT_History", "History", RT_Confidence_Behav_long$Variable)
RT_Confidence_Behav_long$Variable <-
  gsub("diff_RT_Stimulus", "Stimulus", RT_Confidence_Behav_long$Variable)
RT_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_History", "History", RT_Confidence_Behav_long$Variable)
RT_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_Stimulus", "Stimulus", RT_Confidence_Behav_long$Variable)

diff_RT <- lmer(diff ~ Variable + (1 |subject_id), data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "RT",])
STAT.diff_RT <- summary(diff_RT)

diff_Confidence <- lmer(diff ~ Variable + (1|subject_id), data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "Confidence",])
STAT.diff_Confidence <- summary(diff_Confidence)

Summary_RT_Confidence_Behav <- 
  ddply(
    RT_Confidence_Behav_long, 
    .(Variable, Type),
    summarise,
    Mean = mean(diff, na.rm = TRUE),
    Error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))
  
  if (save_summary_data) {
    save(Summary_RT_Confidence_Behav, 
         STAT.diff_RT,
         STAT.diff_Confidence, 
         RT_Confidence_Behav_long,
         RT_Confidence_Behav, 
         STAT.Confidence_vs_mode,
         STAT.RT_vs_mode,
         Post_Perceptual_Modes,
         file = "./Summary_Data/EI_Confidence_RT.Rdata")
  }
  
} else {
  load("./Summary_Data/EI_Confidence_RT.Rdata")
}

```

The above results point to systematic fluctuations in the *decision variable*[@Kepecs2008] that determines perceptual choices, causing enhanced sensitivity to external stimulus information during external mode and increased biases toward preceding choices during internal mode. As such, fluctuations in mode should influence downstream aspects of behavior and cognition that operate on the perceptual decision variable[@Kepecs2008]. To test this hypothesis with respect to motor behavior and metacognition, we asked how bimodal inference relates to response times (RTs) and confidence reports.

With respect to RTs, we observed faster responses for stimulus-congruent as opposed to stimulus-incongruent choices ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$; Figure 2G). Intriguingly, whilst controlling for the effect of stimulus-congruence, we found that history-congruent (as opposed to history-incongruent) choices were also characterized by faster responses ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$; Figure 2G).

When analyzing the speed of response against the mode of sensory processing (Figure 2H), we found that RTs were shorter during externally-oriented perception ($\beta_1$ = $`r STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r STAT.RT_vs_mode$coefficients[2,2]`$, T($`r STAT.RT_vs_mode$coefficients[2,3]`$) = $`r STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r STAT.RT_vs_mode$coefficients[2,5]`$). Crucially, as indicated by a quadratic relationship between the mode of sensory processing and RTs ($\beta_2$ = $`r STAT.RT_vs_mode$coefficients[3,1]`$ ± $`r STAT.RT_vs_mode$coefficients[3,2]`$, T($`r STAT.RT_vs_mode$coefficients[3,3]`$) = $`r STAT.RT_vs_mode$coefficients[3,4]`$, p = $`r STAT.RT_vs_mode$coefficients[3,5]`$), participants became faster at indicating their perceptual decision when biases toward both internal and external mode grew stronger.

In analogy to the speed of response, confidence was higher for stimulus-congruent as opposed to stimulus-incongruent choices ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$; Figure 2I). Yet whilst controlling for the effect of stimulus-congruence, we found that history-congruence also increased confidence ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$; Figure 2I).

When depicted against the mode of sensory processing (Figure 2J), subjective confidence was indeed enhanced when perception was more externally-oriented ($\beta_1$ = $`r STAT.Confidence_vs_mode$coefficients[2,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[2,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[2,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[2,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[2,5]`$). Importantly, however, participants were more confident in their perceptual decision for stronger biases toward both internal and external mode ($\beta_2$ = $`r STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[3,5]`$). In analogy to RTs, subjective confidence thus showed a quadratic relationship to the mode of sensory processing (Figure 2J).

```{r metacognitive_sensitivity, cache = TRUE}

if (!load_summary_data) {
  
  if (compute_metacognitive_sensitivity) {
  
  source("./Functions/f_compute_metacognition.R", local = knitr::knit_global())
  Metacognitive <- f_compute_metacognition(PwData)
  
} else {
  Metacognitive <- read.csv("./Results/Metacognitive_2.csv")
}

Metacognitive$ratio <-
Metacognitive$average_external / Metacognitive$average_internal

Mode_Meta = data.frame(
  meta_dprime = Metacognitive$meta_dprime,
  average_ei = Metacognitive$average_diff_ei,
  average_internal = Metacognitive$average_internal,
  lag_History = ID_Summary_acf$lag_significant_History,
  lag_Stimulus = ID_Summary_acf$lag_significant_Stimulus,
  dprime =  Metacognitive$dprime,
  strength_mode = ID_mode$strength_mode,
  directed_mode = ID_mode$directed_mode,
  study_id = Metacognitive$study_id,
  subject_id = Metacognitive$subject_id,
  Stimulus = Behav[!is.na(Behav$subject_id), ]$Accuracy,
  History = Behav[!is.na(Behav$subject_id), ]$History
)

Mode_Meta$clear_dprime <- Mode_Meta$dprime
Mode_Meta$clear_dprime[Mode_Meta$clear_dprime > median(Mode_Meta$clear_dprime, na.rm = TRUE) + 3* median(Mode_Meta$clear_dprime, na.rm = TRUE)] = NA
Mode_Meta$clear_dprime[Mode_Meta$clear_dprime < median(Mode_Meta$clear_dprime, na.rm = TRUE) - 3* median(Mode_Meta$clear_dprime, na.rm = TRUE)] = NA


Mode_Meta$clear_meta_dprime <- Mode_Meta$meta_dprime
Mode_Meta$clear_meta_dprime[Mode_Meta$clear_meta_dprime > median(Mode_Meta$clear_meta_dprime, na.rm = TRUE) + 3* median(Mode_Meta$clear_meta_dprime, na.rm = TRUE)] = NA
Mode_Meta$clear_meta_dprime[Mode_Meta$clear_meta_dprime < median(Mode_Meta$clear_meta_dprime, na.rm = TRUE) - 3* median(Mode_Meta$clear_meta_dprime, na.rm = TRUE)] = NA

Mode_Meta$meta_dprime_ratio = Mode_Meta$clear_meta_dprime/Mode_Meta$clear_dprime

Mode_Meta$clear_meta_dprime_ratio <- Mode_Meta$meta_dprime_ratio
Mode_Meta$clear_meta_dprime_ratio[Mode_Meta$clear_meta_dprime_ratio > median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE) + 3* median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE)] = NA
Mode_Meta$clear_meta_dprime_ratio[Mode_Meta$clear_meta_dprime_ratio < median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE) - 3* median(Mode_Meta$clear_meta_dprime_ratio, na.rm = TRUE)] = NA

Meta_dprime_ratio_vs_internal <- lmer(clear_meta_dprime_ratio ~ History + (1 |study_id), data = Mode_Meta)
STAT.Meta_dprime_ratio_vs_internal <- summary(Meta_dprime_ratio_vs_internal)

##
## metacognitive  bias
##

Confidence_bias_vs_strength_mode <- lmer(confidence_bias ~ mode_strength_bias + (1|study_id), data = RT_Confidence_Behav)
STAT.Confidence_bias_vs_strength_mode <- summary(Confidence_bias_vs_strength_mode)
  
  if (save_summary_data) {
    save(STAT.Meta_dprime_ratio_vs_internal,
         STAT.Confidence_bias_vs_strength_mode,
         Mode_Meta,
         Metacognitive,
         file = "./Summary_Data/metacognitive.Rdata")
  }
  
} else {
  load("./Summary_Data/metacognitive.Rdata")
}
```

Consequently, our findings predict that human participants lack full metacognitive insight into how strongly external signals and internal predictions contribute to perceptual decision-making. Stronger biases toward perceptual history thus lead to two seemingly contradictory effects, more frequent errors (Supplemental Figure 1C) and increasing subjective confidence (Figure 2I-J). This observation generates an intriguing prediction regarding the association of between-mode fluctuations and perceptual metacognition: Metacognitive efficiency should be lower in individuals who spend more time in internal mode, since their confidence reports are less predictive of whether the corresponding perceptual decision is correct. We computed each participant's M-ratio[@Fleming2014] (meta-d'/d' = `r mean(Mode_Meta$meta_dprime_ratio, na.rm = TRUE)` ± `r sd(Mode_Meta$meta_dprime_ratio, na.rm = TRUE)/sqrt(length(Mode_Meta$meta_dprime_ratio))`) to probe this hypothesis independently of inter-individual differences in perceptual performance. Indeed, we found that biases toward internal information (as defined by the average probability of history-congruence) were stronger in participants with lower metacognitive efficiency ($\beta$ = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,1]`$ ± $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,2]`$, T($`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,3]`$) = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,4]`$, p = $`r STAT.Meta_dprime_ratio_vs_internal$coefficients[2,5]`$).

In sum, the above results indicate that reporting behavior and metacognition do not map linearly onto the mode of sensory processing. Rather, they suggest that slow fluctuations in the respective impact of external and internal information are most likely to affect perception at an early level of sensory analysis[@St.John-Saaltink2016; @Cicchini2021]. Such low-level processing may thus integrate perceptual history with external inputs into a decision variable[@Kepecs2008] that influences not only perceptual choices, but also the speed and confidence at which they are made.

In what follows, we probe alternative explanations for between-mode fluctuations, test for the existence of modes in mice, and propose a predictive processing model that explains fluctuations in mode ongoing shifts in the precision afforded to external sensory information relative to internal predictions driven by perceptual history.

## Fluctuations between internal and external mode cannot be reduced to general response biases or random choices

```{r null_hypotheses, cache = TRUE, warning = FALSE}
if (!load_summary_data) {
  
##
## humans
##
delta_AIC_correlation_humans = compute_AIC(
  LL = STAT.slider_History_Preferred_vs_Accuracy$logLik[1],
  K =  2,
  n = length(STAT.slider_History_Preferred_vs_Accuracy$residuals),
  correction = 0
) - compute_AIC(
  LL = STAT.slider_Preferred_vs_Accuracy$logLik[1],
  K =  1,
  n = length(STAT.slider_History_vs_Preferred$residuals),
  correction = 0
) 

delta_AIC_log_reg_humans = compute_AIC(
  LL = STAT.full_logit_Sum$logLik[1],
  K =  3,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
) - compute_AIC(
  LL = STAT.reduced_logit_Sum$logLik[1],
  K =  2,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
)

##
## mice
##
delta_AIC_correlation_mice = compute_AIC(
  LL = M_STAT.slider_History_Preferred_vs_Accuracy$logLik[1],
  K =  2,
  n = length(M_STAT.slider_History_Preferred_vs_Accuracy$residuals),
  correction = 0
) - compute_AIC(
  LL = M_STAT.slider_Preferred_vs_Accuracy$logLik[1],
  K =  1,
  n = length(M_STAT.slider_Preferred_vs_Accuracy$residuals),
  correction = 0
) 

delta_AIC_log_reg_mice = compute_AIC(
  LL = M_STAT.full_logit_Sum$logLik[1],
  K =  3,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
) - compute_AIC(
  LL = M_STAT.reduced_logit_Sum$logLik[1],
  K =  2,
  n = length(STAT.full_logit_Sum$residuals),
  correction = 0
)
  
  if (save_summary_data) {
    save(delta_AIC_correlation_humans,
         delta_AIC_log_reg_humans,
         delta_AIC_correlation_mice,
         delta_AIC_log_reg_mice,  file = "./Summary_Data/null_hypotheses.Rdata")
  }
  
} else {
  load("./Summary_Data/null_hypotheses.Rdata")
}
```

The core assumption of bimodal inference - that ongoing changes in the sensitivity to external information are driven by internal predictions induced via perceptual history - needs to be contrasted against two alternative hypotheses: When making errors, observers may not engage with the task and respond stereotypically, i.e., exhibit stronger general biases toward one of the two potential outcomes, or simply choose randomly.

Logistic regression confirmed that perceptual history made a significant contribution to perception ($\beta$ = $`r STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum$coefficients[3,4]`$) over and above the ongoing stream of external sensory information ($\beta$ = $`r STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward ($\beta$ = $`r STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r STAT.full_logit_Sum$coefficients[1,4]`$). 

When eliminating perceptual history as a predictor of individual choices at individual trials, Akaike Information Criterion (AIC[@Akaike1987]) increased by $\delta_{AIC}$ = $`r -delta_AIC_log_reg_humans`$ (see Supplemental Figure S4A-B for parameter- and model-level inference at the level of individual observers). Likewise, when eliminating slow fluctuations in history-congruence as a predictor of slow fluctuations in stimulus-congruence across trials, we observed an increase in AIC by $\delta_{AIC}$ = $`r -delta_AIC_correlation_humans`$. These results provided model-level evidence against the null hypotheses that fluctuations in stimulus-congruence are driven exclusively by choice randomness or general response bias (see Supplemental Section 9.2 and Supplemental Figure S5 for an in-depth assessment of general response bias).

To confirm that changes in the sensitivity to external information are indicative of internal mode processing, we estimated full and history-dependent psychometric curves during internal, external, and across modes[@IBL2021]. If, as we hypothesized, internal mode processing reflects an enhanced impact of perceptual history, one would expect a history-dependent increase in biases and lapses as well as a history-independent increase in threshold. Conversely, if internal mode processing were driven by random choices, one would expect a history-independent increase in lapses and threshold, and no change in bias. In line with our prediction, we found that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section 9.3.1 and Supplemental Figure S6). This confirmed that internal mode processing is indeed driven by an enhanced impact of perceptual history.   

In line with this, the quadratic relationship between mode and confidence (Figure 2J) suggested that biases toward internal information do not reflect a post-perceptual strategy of repeating preceding choices when the subjective confidence in the perceptual decision is low. Moreover, while responses became faster with longer exposure to the experiments of the Confidence database, the frequency of history-congruent choices increased over time, speaking against the proposition that participants stereotypically repeat preceding choices when not yet familiar with the experimental task (Supplemental Section 9.4.1). 

Taken together, our results thus argue against recurring intervals of low task engagement, which may be signaled by stereotypical or random responses, as an alternative explanation for the phenomenon that we identify as bimodal inference.

## Mice fluctuate between external and internal modes of sensory processing

```{r load_preprocess_mouse_data_new, cache = TRUE}

if (!load_summary_data) {

if (preprocess_mouse_data) {
  loc = paste(root, "/Mouse/export_dataframe.csv", sep = "")
  n_permutations = 100
  
  source("./Functions/f_preprocess_mouse_data.R",
  local = knitr::knit_global())
  MwData <- f_preprocess_mouse_data(loc, n_permutations)
}

## 
## Dynamic probability of stimulus- and history-congruence
## 
if (compute_slider_mouse_data) {
source("./Functions/f_compute_slider.R", local = knitr::knit_global())

for (subj_idx in unique(MwData$subject_id)) {
print(subj_idx)
for (session_idx in unique(MwData$subject_id)) {
index = (MwData$subject_id == subj_idx & MwData$session_id == session_idx)

MwData$History_slider[index] <- f_compute_slider(MwData$History[index], sliding_window)
MwData$Accuracy_slider[index] <- f_compute_slider(MwData$Accuracy[index], sliding_window)
MwData$RT_slider[index] <- f_compute_slider(MwData$RT[index], sliding_window)
}
  
}
}

##
## Prepare diagnostics for exclusion criteria
##

if (filter_mouse_data){
MwData$keep = NA
for (subject_idx in unique(MwData$subject_id)) {
    print(subject_idx)
    for (session_idx in unique(MwData[MwData$subject_id == subject_idx,]$session_id)) { 
    print(session_idx)
      
      M_Behav_excl <- data.frame()
    M_Behav_excl <- 
    ddply(
    MwData[MwData$subject_id == subject_idx & MwData$session_id == session_idx,], 
    .(Difficulty),
    summarise,
    History = mean(History, na.rm = TRUE)*100, 
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    median_RT = median(RT, na.rm = TRUE)
  )

       if (max(M_Behav_excl[M_Behav_excl$Difficulty >= 0.5,]$Accuracy, na.rm = TRUE) > 80){
         MwData[MwData$subject_id == subject_idx & MwData$session_id == session_idx,]$keep = 1
       } else {
         MwData[MwData$subject_id == subject_idx & MwData$session_id == session_idx,]$keep = 0
       }
    
      }
}
}

## load fully preprocessed data
if (load_mouse_data){
  #MwData <- read.csv(paste(root, "/Mouse/balanced_MwData_slider.csv", sep = ""))
  MwData <- read.csv(paste(root, "/Mouse/MwData_new_preproc_filter_slider_excl.csv", sep = ""))
}


##
## Apply exlcusion criteria
##

if (apply_mouse_exclusion_criteria){
MwData <- MwData[MwData$keep == 1,]


M_Behav_excl <-
  ddply(
    MwData,
    .(subject_id, session_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100,
    History = mean(History, na.rm = TRUE)*100,
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    Imbalance = mean(abs(0.5-prob_left), na.rm = TRUE)*100 + 50,
    RT = mean(RT, na.rm = TRUE)
  )


data_select = M_Behav_excl[M_Behav_excl$Imbalance<60,c(1,2)]

sMwData = data.frame()
for (subj_idx in unique(data_select$subject_id)){
print(subj_idx)
row_select = double()
row_select = (MwData$subject_id == subj_idx & MwData$session_id %in% data_select[data_select$subject_id == subj_idx,]$session_id)
sMwData <- rbind(sMwData, MwData[which(row_select),])
}
MwData <- sMwData
}


MwData$clear_RT = MwData$RT
MwData$scaled_RT = scale(MwData$clear_RT)
MwData$clear_RT[MwData$clear_RT > median(MwData$clear_RT, na.rm = TRUE) + 3*median(MwData$clear_RT, na.rm =TRUE)] = NA

MwData$clear_RT2 = MwData$RT
MwData$clear_RT2[MwData$clear_RT2 > median(MwData$clear_RT2, na.rm = TRUE) + 1.5*median(MwData$clear_RT2, na.rm =TRUE)] = NA

##
## compute weighted stimulus
##
MwData$w_Stimulus = 0.5 + (MwData$Stimulus-0.5)*MwData$Difficulty

##
## go to milliseconds
##
MwData$clear_RT <- MwData$clear_RT*1000
MwData$clear_RT2 <- MwData$clear_RT2*1000

  n_trials_mice = nrow(MwData)
  n_participants_mice = length(unique(MwData$subject_id))
  
  if (save_summary_data) {
    save(n_trials_mice, n_participants_mice, 
         file = "./Summary_Data/size_IBL_dataset.Rdata")
  }
  
} else {
  load("./Summary_Data/size_IBL_dataset.Rdata")
}

```

In a prominent functional explanation for serial dependencies[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @St.John-Saaltink2016; @Fritsche2020; @Urai2017; @Urai2019; @Hsu2020], perceptual history is cast as an internal prediction that leverages the temporal autocorrelation of natural environments for efficient decision-making[@Dong1995; @Chopin2012; @Burr2014; @Braun2018; @Bergen2019]. Since this autocorrelation is one of the most basic features of our sensory world, fluctuating biases toward preceding perceptual choices should not be a uniquely human phenomenon. 

To test whether externally- and internally-oriented modes of processing exist beyond the human mind, we analyzed data on perceptual decision-making in mice that were extracted from the International Brain Laboratory (IBL) dataset[@IBL2021]. We restricted our analyses to the *basic* task[@IBL2021], in which mice responded to gratings of varying contrast that appeared either in the left or right hemifield of with equal probability. We excluded sessions in which mice did not respond correctly to stimuli presented at a contrast above 50% in more than 80% of trials (see Methods for details), which yielded a final sample of N = `r n_participants_mice` adequately trained mice that went through $`r n_trials_mice/1000000`$ million trials.  

```{r overview_mouse_data, cache = TRUE}
if (!load_summary_data) {
  
M_Behav <- 
  ddply(
    MwData, 
    .(subject_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100, 
    History = mean(History, na.rm = TRUE)*100, 
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    Bias = abs(0.5 - sum(Response == max(Response), na.rm = TRUE)/length(Response))*100 + 50
  )


M_Behav$null_History = M_Behav$History - 50
M_STAT.Global_History_Accuracy = t.test(M_Behav$null_History)


MwData[(is.na(MwData$Stimulus)), c("Accuracy", "History", "History_slider", "Accuracy_slider")] = NA

##
## History effects global
##

gathercol = colnames(M_Behav[, c(3,4)])
M_Behav_long  <-
gather(M_Behav,
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

M_Behav_diff <- 
   ddply(
    MwData[!is.na(MwData$Accuracy),], 
    .(subject_id, Accuracy),
    summarise,
    History = mean(History, na.rm = TRUE)*100
  )

gathercol <- colnames(M_Behav_diff[,c(3)])

M_Behav_diff_long  <-
gather(M_Behav_diff[, c(1,2,3)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

M_Behav_diff_long$Accuracy <- as.character(M_Behav_diff_long$Accuracy) 
M_Behav_diff_long$Accuracy <-
gsub("0", "error", M_Behav_diff_long$Accuracy)
M_Behav_diff_long$Accuracy <-
gsub("1", "correct", M_Behav_diff_long$Accuracy)

M_STAT.diff_History_Accuracy = t.test(M_Behav_diff[M_Behav_diff$Accuracy == 0,]$History, M_Behav_diff[M_Behav_diff$Accuracy == 1,]$History, paired = TRUE) 
  
  if (save_summary_data) {
    save(M_Behav, 
         M_STAT.Global_History_Accuracy, 
         M_Behav_long,
         M_Behav_diff,
         M_Behav_diff_long, 
         M_STAT.diff_History_Accuracy,
         file = "./Summary_Data/Summary_Behavior_IBL.Rdata")
  }
  
} else {
  load("./Summary_Data/Summary_Behavior_IBL.Rdata")
}
```

```{r compute_mouse_logreg, cache = TRUE}

if (!load_summary_data) {
  ##
  ## Logistic regression
  ##
  if (compute_mouse_logreg) {
    source("./Functions/f_compute_mouse_logreg.R",
           local = knitr::knit_global())
    M_LogAggr <- f_compute_mouse_logreg(MwData)
  } else {
    M_LogAggr <- read.csv("./Results/M_LogAggr_new_preproc.csv")
  }
  
  M_LogAggr$diff_AIC = (M_LogAggr$full_AIC - M_LogAggr$reduced_AIC)
  
  M_STAT.LogReg = t.test(M_LogAggr$diff_AIC)
  
  if (compute_mouse_logreg) {
    M_full_logit_Sum <-
      glmer(
        Response ~ Stimulus + Response_minus_1 + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1, ],
        family = binomial
      )
    
     M_reduced_logit_Sum <-
      glmer(
        Response ~ Stimulus + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1, ],
        family = binomial
      )
  } else {
    M_full_logit_Sum <-
      readRDS(file = "./Results/M_full_logit_Sum_new_preproc.rds")
    M_reduced_logit_Sum <-
      readRDS(file = "./Results/M_reduced_logit_Sum_new_preproc.rds")
  }
  M_STAT.full_logit_Sum <- summary(M_full_logit_Sum)
  M_STAT.reduced_logit_Sum <- summary(M_reduced_logit_Sum)
  
  if (save_summary_data) {
    save(M_LogAggr, 
         M_STAT.LogReg, 
         M_STAT.full_logit_Sum,
         M_STAT.reduced_logit_Sum,
         file = "./Summary_Data/LogReg_mice.Rdata")
  }
  
} else {
  load("./Summary_Data/LogReg_mice.Rdata")
}
```

We found perception to be stimulus-congruent in `r mean(M_Behav$Accuracy, na.rm = TRUE)`% ± `r sd(M_Behav$Accuracy, na.rm = TRUE)/sqrt(length(M_Behav$Accuracy))`% of trials (Figure 3A, upper panel). In line with humans, mice were biased toward perceptual history in `r mean(M_Behav$History, na.rm = TRUE)`% ± `r sd(M_Behav$History, na.rm = TRUE)/sqrt(length(M_Behav$History))`% of trials (T(`r M_STAT.Global_History_Accuracy$parameter`) = `r M_STAT.Global_History_Accuracy$statistic`, p = $`r M_STAT.Global_History_Accuracy$p.value`$; Figure 3A and Supplemental Figure S1D). 
Since the *basic* task of the IBL dataset presented stimuli at random in either the left or right hemifield[@IBL2021], we expected stronger biases toward perceptual history to decrease perceptual performance. Indeed, history-congruent choices were more frequent when perception was stimulus-incongruent (`r mean(M_Behav_diff[M_Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)`% ± `r sd(M_Behav_diff[M_Behav_diff$Accuracy == 0,]$History, na.rm = TRUE)/sqrt(length(Behav_diff[M_Behav_diff$Accuracy == 0,]$History))`%) as opposed to stimulus-congruent (`r mean(M_Behav_diff[M_Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)`% ± `r sd(M_Behav_diff[M_Behav_diff$Accuracy == 1,]$History, na.rm = TRUE)/sqrt(length(M_Behav_diff[Behav_diff$Accuracy == 1,]$History))`%, T(`r M_STAT.diff_History_Accuracy$parameter`) = `r M_STAT.diff_History_Accuracy$statistic`, p = $`r M_STAT.diff_History_Accuracy$p.value`$; T(`r M_STAT.diff_History_Accuracy$parameter`) = `r M_STAT.diff_History_Accuracy$statistic`, p = $`r M_STAT.diff_History_Accuracy$p.value`$; Figure 3A, lower panel), confirming that perceptual history was a source of bias[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] as opposed to a feature of the experimental paradigm.

```{r mouse_stats_autocorrelations, cache = TRUE}

if (!load_summary_data) {
  
##
## Summary stats autocorrelation
##

## compute_Summary_acf

source("./Functions/f_compute_mouse_group_acf.R", local = knitr::knit_global())

# compute difference to random autocorrelation and remove outliers  
MwData$diff_acf_Stimulus = exclude_3SD(MwData$acf_Stimulus- MwData$random_acf_Stimulus) 
MwData$diff_acf_History = exclude_3SD(MwData$acf_History - MwData$random_acf_History) 
MwData$diff_acf_Difficulty = exclude_3SD(MwData$acf_Difficulty - MwData$random_acf_Difficulty) 
MwData$diff_acf_RT = exclude_3SD(MwData$acf_RT - MwData$random_acf_RT)
MwData$diff_acf_HardEasy = exclude_3SD(MwData$acf_HardEasy - MwData$random_acf_HardEasy)
MwData$diff_acf_External = exclude_3SD(MwData$acf_External - MwData$random_acf_External)

acf_to_test = c('diff_acf_Stimulus', 'diff_acf_History')
max_trial = 99

if (compute_mouse_group_acf){
M_Summary_acf <- f_compute_mouse_group_acf(MwData, acf_to_test, max_trial)

} else {
  # M_Summary_acf <- read.csv("./Results/M_Summary_acf_session_corrected_normalized_3SD_new_preproc.csv")# M1
  M_Summary_acf <- read.csv("./Results/M_Summary_acf_session_full_control.csv")# M1
}
M_Summary_acf$Trial <- M_Summary_acf$Trial - 1


ID_M_Summary_acf <- ddply(
    MwData, 
    .(subject_id),
    summarise,
    lag_significant_Stimulus = min(which(diff(which(c(0, exceed_acf_Stimulus,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_History = min(which(diff(which(c(0, exceed_acf_History,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_Difficulty = min(which(diff(which(c(0, exceed_acf_Difficulty,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_External = min(which(diff(which(c(0, exceed_acf_External,0) < n_permutations/2)) != 1)) - 1
  )
ID_M_Summary_acf$lag_significant_Stimulus[ID_M_Summary_acf$lag_significant_Stimulus == Inf] = NA
ID_M_Summary_acf$lag_significant_History[ID_M_Summary_acf$lag_significant_History == Inf] = NA
ID_M_Summary_acf$lag_significant_Difficulty[ID_M_Summary_acf$lag_significant_Difficulty == Inf] = NA
ID_M_Summary_acf$lag_significant_External[ID_M_Summary_acf$lag_significant_External == Inf] = NA


##
## Exceed Plot
##

gathercol <- colnames(ID_M_Summary_acf[,c(2,3)])

ID_M_Summary_acf_long  <-
gather(ID_M_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

M_Stimulus_linear <- summary(lm(Mean ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" & M_Summary_acf$Trial < 21,]))
M_Stimulus_exponential <- summary(lm(log(Mean) ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" & M_Summary_acf$Trial < 21,]))

M_History_linear <- summary(lm(Mean ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History" & M_Summary_acf$Trial < 21,]))
M_History_exponential <- summary(lm(log(Mean) ~ Trial , data = M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History" & M_Summary_acf$Trial < 21,]))

MwData$min_diff_acf_Stimulus = MwData$diff_acf_Stimulus - min(MwData$diff_acf_Stimulus, na.rm = TRUE)
MwData$min_diff_acf_History = MwData$diff_acf_History - min(MwData$diff_acf_History, na.rm = TRUE)

M_lmer_acf_Stimulus <- lmer(log(min_diff_acf_Stimulus) ~ Trial + (1|subject_id), data = MwData[MwData$Trial > 1 & MwData$Trial < 21,])
M_STAT.lmer_acf_Stimulus <- summary(M_lmer_acf_Stimulus)
M_lmer_acf_History <- lmer(log(min_diff_acf_History) ~ Trial + (1|subject_id), data = MwData[MwData$Trial > 1 & MwData$Trial < 21,])
M_STAT.lmer_acf_History <- summary(M_lmer_acf_History)
  
  if (save_summary_data) {
    save(M_Summary_acf,
         ID_M_Summary_acf,
         ID_M_Summary_acf_long,
         M_Stimulus_linear,
         M_Stimulus_exponential,
         M_History_linear,
         M_History_exponential,
         M_STAT.lmer_acf_Stimulus,
         M_STAT.lmer_acf_History,
         file = "./Summary_Data/Autocorrelation_mice.Rdata")
  }
  
} else {
  load("./Summary_Data/Autocorrelation_mice.Rdata")
}
```

```{r logistic_regression_autocorrelation_mouse, cache = TRUE}
##
## Reproduce autocorrelation with logistic regression (correcting for difficulty and stimulus repetition)
##
if (!load_summary_data) {
  ##
  ## Mice: Trialwise logistic regression
  ##
  n_back = 100
  if (compute_mouse_Tw_LogReg == TRUE) {
    source("./Functions/f_compute_mouse_trial_wise_logreg.R",
           local = knitr::knit_global())
    
    M_Tw_LogReg <- f_compute_mouse_trial_wise_logreg(MwData, n_back)
    
  } else {
    #M_Tw_LogReg <- read.csv("./Results/M_Tw_LogReg_new_preproc_25back.csv")
    M_Tw_LogReg <- read.csv("./Results/M_Tw_LogReg_new_preproc_100back.csv")
  }
  
  gathercol = colnames(M_Tw_LogReg[, c(4, 6)])
  M_Tw_LogReg_long <-
    gather(M_Tw_LogReg[, c(1:4, 6)],
           "Variable",
           "beta",
           gathercol,
           factor_key = TRUE)
  
  M_Tw_LogReg_long$Variable <-
    gsub("History_weight",
         "Weight: History-congruence",
         M_Tw_LogReg_long$Variable)
  M_Tw_LogReg_long$Variable <-
    gsub("Stimulus_weight",
         "Weight: Stimulus-congruence",
         M_Tw_LogReg_long$Variable)
  
  
  M_Group_Tw_LogReg_long <-  ddply(
    M_Tw_LogReg_long,
    .(lag, Variable),
    summarise,
    Mean = mean(exclude_3SD(beta), na.rm = TRUE),
    Error = sd(exclude_3SD(beta), na.rm = TRUE) / sqrt(length(exclude_3SD(beta)))
  )
  
  ## trial-wise stats
  M_Group_Tw_LogReg_long$p_value = NA
  for (lag_idx in c(1:n_back)) {
    M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: History-congruence" &
                             M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
      summary(lmer(exclude_3SD(History_weight) ~ 1 + (1 |
                                                        session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
    
    M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence" &
                             M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
      summary(lmer(exclude_3SD(Stimulus_weight) ~ 1 + (1 |
                                                         session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
  }
  
  
  
  if (save_summary_data) {
    save(M_Group_Tw_LogReg_long,
         M_Tw_LogReg_long,
         M_Tw_LogReg,
         M_Group_Tw_LogReg_long,
         n_back,
         file = "./Summary_Data/log_reg_autocorrelation_mouse.Rdata")
  }
  
} else {
  load("./Summary_Data/log_reg_autocorrelation_mouse.Rdata")
}

```

```{r mouse_spectral_analysis, cache = TRUE}
##
## Power
##
if (!load_summary_data) {
  
  if (compute_mouse_power_spectra) {
source("./Functions/f_compute_power_spectra.R", local = knitr::knit_global())

sliders = c("Accuracy_slider", "History_slider")  
M_Power_Spectra <- f_compute_power_spectra(MwData, sliders)  
} else {
  M_Power_Spectra <- read.csv(paste(root, "M_power_spectra_smoothed_new_preproc.csv", sep = ""))

}

M_Power_Spectra$r_freq = round(M_Power_Spectra$freq, digits = 4)

step_size = 0.01
bins <- seq(from = min(M_Power_Spectra$freq), to = max(M_Power_Spectra$freq), by = step_size)
names <- round(bins, digits = 2)
names = names[1:length(names) - 1]

M_Power_Spectra$bin_freq <- cut(M_Power_Spectra$freq, breaks = bins, labels = names)
M_Power_Spectra$bin_freq <- as.numeric(M_Power_Spectra$bin_freq) * step_size
M_Power_Spectra$bin_freq[is.na(M_Power_Spectra$bin_freq)] = max(M_Power_Spectra$freq)

M_Power_Spectra$Coherence <- M_Power_Spectra$Coherence * 100



gathercol = colnames(M_Power_Spectra[, c(2,3)])
M_Power_Spectra_long  <-
gather(M_Power_Spectra[, c(1,2,3, 6,7,8)],
"Variable",
"Power",
gathercol,
factor_key = TRUE)

M_Power_Frequency <-  ddply(
      M_Power_Spectra_long,
      .(r_freq, Variable),
      summarise,
      
      mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
      ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE)/sqrt(length(exclude_3SD(Power)))
 )

M_Coherence_Phase_Frequency <- ddply(
      M_Power_Spectra,
      .(bin_freq),
      summarise,
    
      mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
      ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE)/sqrt(length(exclude_3SD(Coherence))),
      
      mode_phase = getmode(exclude_3SD((abs(Phase)))),
      mean_phase = getmode(exclude_3SD((abs(Phase)))),
      ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE)/sqrt(length(exclude_3SD(Phase)))
 )
 

##
## mode of coherence and phase
##
M_Summary_Power_Spectra <- 
   ddply(
 M_Power_Spectra[M_Power_Spectra$freq > 0.01 & M_Power_Spectra$freq < 0.1,],
       .(subject_id),
       summarise,
       mean_coherence = mean(exclude_3SD((
 Coherence))),
       mode_phase = getmode(exclude_3SD((
 abs(Phase))))
   )

##
## STATS Power vs Frequency (1/f noise)
##

M_lmer_power_freq_Stimulus <- lmer(log(Power_Stimulus) ~ log(freq) + (1|subject_id), data = M_Power_Spectra[M_Power_Spectra$freq > 0.01 & M_Power_Spectra$freq < 0.1 & is.finite(log(M_Power_Spectra$Power_Stimulus)),])
STAT.lmer_power_freq_Stimulus <- summary(M_lmer_power_freq_Stimulus)

M_lmer_power_freq_History <- lmer(log(Power_History) ~ log(freq) + (1|subject_id), data = M_Power_Spectra[M_Power_Spectra$freq > 0.01 & M_Power_Spectra$freq < 0.1 & is.finite(log(M_Power_Spectra$Power_History)),])
STAT.lmer_power_freq_History <- summary(M_lmer_power_freq_History)
  
  if (save_summary_data) {
    save(M_Power_Frequency,
         M_Coherence_Phase_Frequency,
         M_Summary_Power_Spectra,
         STAT.lmer_power_freq_Stimulus,
         STAT.lmer_power_freq_History,
         M_Power_Spectra,
         file = "./Summary_Data/spectral_analysis_mouse.Rdata")
  }
  
} else {
  load("./Summary_Data/spectral_analysis_mouse.Rdata")
}

```

At the group level, we found significant autocorrelations in both stimulus-congruence (`r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials) and history-congruence (`r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials; Figure 3B), while controlling for the respective autocorrelation of task difficulty and external stimulation (Supplemental Figure 2C-D). In contrast to humans, mice showed a negative autocorrelation coefficient of stimulus-congruence at trial 2, which was due to a feature of the experimental design: Errors at a contrast above 50% were followed by a high-contrast stimulus at the same location. Thus, stimulus-incongruent choices on easy trials were more likely to be followed by stimulus-congruent perceptual choices that were facilitated by high-contrast visual stimuli[@IBL2021]. 

At the level of individual mice, autocorrelation coefficients were elevated above randomly permuted data within a lag of `r mean(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_M_Summary_acf)` trials for stimulus-congruence and `r mean(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_M_Summary_acf)` trials for history-congruence (Figure 3C). We corroborated these autocorrelations in logistic regression models that successfully predicted the stimulus-/history-congruence of perception at the index trial $t = 0$ from the stimulus-/history-congruence at the `r min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]$p_value > 0.05))` preceding trials for stimulus-congruence and `r min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: History-congruence",]$p_value > 0.05))` preceding trials for history-congruence (Supplemental Figure S3). In analogy to humans, mice showed anti-phase 1/f fluctuations in the sensitivity to internal and external information (Figure 3D-F).

```{r mouse_slider_history_accuracy}
if (!load_summary_data) {
  
if (compute_slider_History_Accuracy_lmer) {
M_slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|subject_id), data = MwData)
} else {
   M_slider_History_vs_Accuracy <- readRDS(file = "./Results/M_slider_History_vs_Accuracy_new_preproc.rds")
}
M_STAT.slider_History_vs_Accuracy <- summary(M_slider_History_vs_Accuracy)
  
  if (save_summary_data) {
    save(M_STAT.slider_History_vs_Accuracy,  file = "./Summary_Data/mouse_slider_history_accuracy.Rdata")
  }
} else {
  load("./Summary_Data/mouse_slider_history_accuracy.Rdata")
}

```

```{r mouse_bias_slider, cache = TRUE}

if (!load_summary_data) {
  
  if (extract_mouse_additional_Slider_Data == TRUE) {
    source("./Functions/f_compute_slider.R", local = knitr::knit_global())
    M_Slider_Data = MwData[, c(1,2, 4, 6, 37, 38)]
    
    M_Slider_Data$Preferred = NA
    M_Slider_Data$Preferred_slider = NA
    for (subj_idx in unique(M_Slider_Data$subject_id)) {
      print(subj_idx)
      responses = M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Response
      
      outcomes = unique(responses)
      
      n = rep(NA, length(outcomes))
      for (outcome_idx in c(1,length(outcomes))) {
        n[outcome_idx] = sum(responses == outcomes[outcome_idx])
      }
      
      M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred = rep(0, length(responses))
      M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred[responses == outcomes[which(n == max(n))]] = 1
      M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred_slider = f_compute_slider(M_Slider_Data[M_Slider_Data$subject_id == subj_idx, ]$Preferred, sliding_window)
    }
  } else {M_Slider_Data <- read.csv(paste(root, "M_Slider_Data.csv", sep = ""))}
  
  M_slider_History_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ History_slider + Preferred_slider + (1|subject_id/session_id), data = M_Slider_Data)
  M_STAT.slider_History_Preferred_vs_Accuracy <- summary(M_slider_History_Preferred_vs_Accuracy)
  
  M_slider_Preferred_vs_Accuracy <- lmer(Accuracy_slider ~ Preferred_slider + (1|subject_id/session_id), data = M_Slider_Data)
  M_STAT.slider_Preferred_vs_Accuracy <- summary(M_slider_Preferred_vs_Accuracy)
  
  M_slider_History_vs_Preferred <- lmer(History_slider ~ Preferred_slider + (1|subject_id/session_id), data = M_Slider_Data)
  M_STAT.slider_History_vs_Preferred <- summary(M_slider_History_vs_Preferred)
  
  M_Global_History_Preferred_vs_Bias <- lm(Accuracy ~ History + Bias, data = M_Behav)
  M_STAT.Global_History_Preferred_vs_Bias <- summary(M_Global_History_Preferred_vs_Bias)
  
  M_Variance_History_General <- 
    ddply(
      M_Slider_Data, 
      .(subject_id, session_id),
      summarise,
      var_History = sd(History_slider, na.rm = TRUE) - sd(Preferred_slider, na.rm = TRUE)
    )
  
  M_Variance_History_vs_Preferred <- lmer(var_History ~ 1 + (1|session_id), data = M_Variance_History_General)
  M_STAT.Variance_History_vs_Preferred <- summary(M_Variance_History_vs_Preferred)
  
  if (save_summary_data) {
    save(M_STAT.slider_History_Preferred_vs_Accuracy,
         M_STAT.slider_Preferred_vs_Accuracy,
         M_STAT.slider_History_vs_Preferred,
         M_STAT.Global_History_Preferred_vs_Bias,
         M_Variance_History_General,
         M_STAT.Variance_History_vs_Preferred,
         file = "./Summary_Data/mouse_bias_slider.Rdata")
  }
  
} else {
  load("./Summary_Data/mouse_bias_slider.Rdata")
}
```

```{r mouse_mode_RT_new, cache = TRUE}
##
## Mode vs. RT
##

if (!load_summary_data) {
  
  M_Mode_gather <-  ddply(
      MwData,
      .(subject_id),
      summarise,
      
      directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
      fine_directed_mode = (Accuracy_slider -History_slider)*100,

      scaled_directed_mode = scale(directed_mode),
      strength_mode = abs(scaled_directed_mode)
      )

MwData$directed_mode = M_Mode_gather$directed_mode
MwData$fine_directed_mode = M_Mode_gather$fine_directed_mode
MwData$scaled_directed_mode = M_Mode_gather$scaled_directed_mode
MwData$strength_mode = M_Mode_gather$strength_mode

M_ID_mode <-  ddply(
      MwData,
      .(subject_id),
      summarise,
      strength_mode = mean(strength_mode, na.rm = TRUE),
      directed_mode = mean(directed_mode, na.rm = TRUE)
)


scaled_data = ddply(
      MwData,
      .(subject_id),
      summarise,
      scaled_clear_RT = scale(clear_RT)
      )

MwData$scaled_clear_RT <- scaled_data$scaled_clear_RT



M_Post_Perceptual_Modes = ddply(
      MwData[!is.na(MwData$directed_mode),],
      .(directed_mode),
      summarise,
      average_RT = mean(clear_RT, na.rm = TRUE),
      se_RT = sd(clear_RT, na.rm = TRUE)/sqrt(length(clear_RT)),
      average_RT2 = mean(clear_RT2, na.rm = TRUE),
      se_RT2 = sd(clear_RT2, na.rm = TRUE)/sqrt(length(clear_RT2)),
      n = length(clear_RT),
      n_percent = (sum(!is.na(clear_RT))/nrow(MwData))*100
      )

M_RT_vs_mode <- lmer(clear_RT ~ poly(directed_mode, 2) + (1|subject_id), data = MwData[!is.na(MwData$directed_mode),])
M_STAT.RT_vs_mode <- summary(M_RT_vs_mode)

M_RT_vs_mode2 <- lmer(clear_RT2 ~ poly(directed_mode, 2) + (1|subject_id), data = MwData[!is.na(MwData$directed_mode) & MwData$directed_mode <= 70 & MwData$directed_mode >= -20,])
M_STAT.RT_vs_mode2 <- summary(M_RT_vs_mode2)


M_RT_Behav <- 
  ddply(
    MwData[!is.na(MwData$History) & !is.na(MwData$Accuracy),], 
    .(subject_id),
    summarise,
    diff_RT_History = mean(clear_RT[History == 1], na.rm = TRUE) -  mean(clear_RT[History == 0], na.rm = TRUE),
    diff_RT_Stimulus = mean(clear_RT[Accuracy == 1], na.rm = TRUE) -  mean(clear_RT[Accuracy == 0], na.rm = TRUE))


gathercol = colnames(M_RT_Behav[, c(2,3)])
M_RT_Behav_long  <-
gather(M_RT_Behav,
"Variable",
"diff_RT",
gathercol,
factor_key = TRUE)

M_RT_Behav_long$Variable <-
gsub("diff_RT_History", "History", M_RT_Behav_long$Variable)
M_RT_Behav_long$Variable <-
gsub("diff_RT_Stimulus", "Stimulus", M_RT_Behav_long$Variable)

M_diff_RT <- lmer(diff_RT ~ Variable + (1|subject_id), data = M_RT_Behav_long)
M_STAT.diff_RT <- summary(M_diff_RT)

M_Summary_RT_Behav <- 
  ddply(
    M_RT_Behav_long, 
    .(Variable),
    summarise,
    Mean = mean(diff_RT, na.rm = TRUE),
    Error = sd(diff_RT, na.rm = TRUE)/sqrt(length(diff_RT)))


M_STAT.RT_diff_History <- t.test(M_RT_Behav$diff_RT_History)
M_STAT.RT_diff_Stimulus <- t.test(M_RT_Behav$diff_RT_Stimulus)

if (mouse_compute_RT_Accuracy_History) {
M_lmer_RT_Accuracy_History <- lmer(RT ~ Accuracy + History + (1|subject_id), data = MwData)
saveRDS(M_lmer_RT_Accuracy_History, "./Results/M_lmer_RT_Accuracy_History.rds")

} else {
M_lmer_RT_Accuracy_History <- readRDS(file = "./Results/M_lmer_RT_Accuracy_History.rds")

}
M_STAT.lmer_RT_Accuracy_History <- summary(M_lmer_RT_Accuracy_History)

  
  if (save_summary_data) {
    save(M_Mode_gather, 
         M_ID_mode, 
         M_Post_Perceptual_Modes,
         M_STAT.RT_vs_mode,
         M_STAT.RT_vs_mode2,
         M_RT_Behav,
         M_RT_Behav_long,
         M_STAT.diff_RT,
         M_Summary_RT_Behav,
         M_STAT.RT_diff_History,
         M_STAT.RT_diff_Stimulus,
         M_STAT.lmer_RT_Accuracy_History,
         file = "./Summary_Data/mouse_mode_RT.Rdata")
  }
} else {
  load("./Summary_Data/mouse_mode_RT.Rdata")
}
```

The above results confirm that fluctuations between internally- and externally-biased modes generalize to perceptual decision-making in mice. Following our hypothesis that bimodal inference operates at the level of perception, we predicted that between-mode fluctuations modulate a decision variable[@Kepecs2008] that determines not only perceptual choices, but also downstream aspects of mouse behavior[@Kepecs2008]. We therefore asked how external and internal modes relate to the trial duration (TD, a coarse measure of RT in mice that spans the interval from stimulus onset to feedback[@IBL2021]). Stimulus-congruent (as opposed to stimulus-incongruent) choices were associated with shorter TDs ($\delta$ =  $`r mean(M_RT_Behav$diff_RT_Stimulus, na.rm = TRUE)`$ ± $`r sd(M_RT_Behav$diff_RT_Stimulus, na.rm = TRUE)/sqrt(length(M_RT_Behav$diff_RT_Stimulus))`$, T(`r M_STAT.RT_diff_Stimulus$parameter`) = `r M_STAT.RT_diff_Stimulus$statistic`, p = $`r M_STAT.RT_diff_Stimulus$p.value`$), while history-congruent choices were characterized by longer TDs ($\delta$ =  $`r mean(M_RT_Behav$diff_RT_History, na.rm = TRUE)`$ ± $`r sd(M_RT_Behav$diff_RT_History, na.rm = TRUE)/sqrt(length(M_RT_Behav$diff_RT_History))`$, T(`r M_STAT.RT_diff_History$parameter`) = `r M_STAT.RT_diff_History$statistic`, p = $`r M_STAT.RT_diff_History$p.value`$; Figure 3G). 

Across the full spectrum of the available data, TDs showed a linear relationship with the mode of sensory processing, with shorter TDs during external mode ($\beta_1$ = $`r M_STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r M_STAT.RT_vs_mode$coefficients[2,2]`$, T($`r M_STAT.RT_vs_mode$coefficients[2,3]`$) = $`r M_STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r M_STAT.RT_vs_mode$coefficients[2,5]`$, Figure 3H). 
However, an explorative post-hoc analysis limited to TDs that differed from the median TD by no more than 1.5 x MAD (median absolute distance[@Leys2013]) indicated that, when mice engaged with the task more swiftly, TDs did indeed show a quadratic relationship with the mode of sensory processing ($\beta_2$ = $`r M_STAT.RT_vs_mode2$coefficients[3,1]`$ ± $`r M_STAT.RT_vs_mode2$coefficients[3,2]`$, T($`r M_STAT.RT_vs_mode2$coefficients[3,3]`$) = $`r M_STAT.RT_vs_mode2$coefficients[3,4]`$, p = $`r M_STAT.RT_vs_mode2$coefficients[3,5]`$, Figure 3I).

As in humans, it is important to ensure that ongoing changes in the sensitivity to external information are indeed driven by perceptual history and cannot be reduced to general choice biases or random behavior. 
Logistic regression confirmed a significant effect perceptual history on perceptual choices ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[3,4]`$), while controlling for external sensory information ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward one of the two outcomes ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[1,4]`$). When eliminating perceptual history as a predictor of individual choices, AIC increased by  $\delta_{AIC}$ = $`r -delta_AIC_log_reg_mice`$, arguing against the notion that choice randomness and general response bias are the only determinants of perceptual performance in mice (see Supplemental Figure S4C-D for parameter- and model-level inference in individual subjects). 

In mice, fluctuations in the strength of history-congruent biases had a significant effect on stimulus-congruence ($\beta_1$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) beyond the effect of ongoing changes in general response biases ($\beta_2$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$). Eliminating the dynamic fluctuations in history-congruence as a predictor of fluctuations in stimulus-congruence resulted in an increase in AIC by $\delta_{AIC}$ = $`r -delta_AIC_correlation_mice`$ (see Supplemental Section 9.2 and Supplemental Figure S5 for an in-depth assessment of general response bias).

When fitting full and history-conditioned psychometric curves to the IBL data[@IBL2021], we observed that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section 9.3.2 and Supplemental Figure S7). Over time, the frequency of history-congruent choices increased alongside stimulus-congruence and speed of response as mice were exposed to the experiment, arguing against the proposition that biases toward perceptual history reflected an unspecific response strategy in mice who were not sufficiently trained on the IBL task (Supplemental Section 9.4.2 and Supplemental Figure S8).  

In sum, these analyses confirmed that the observed fluctuations in sensitivity to external sensory information are driven by dynamic changes in impact of perceptual history and cannot be reduced to general response bias and random choice behavior. 

## Fluctuations in mode result from coordinated changes in the impact of external and internal information on perception

The empirical data presented above indicate that, for both humans and mice, perception fluctuates between external and modes, i.e., multi-trial epochs that are characterized by enhanced sensitivity toward either external sensory information or internal predictions generated by perceptual history. Since natural environments typically show high temporal redundancy[@Dong1995], previous experiences are often good predictors of new stimuli[@Chopin2012; @Burr2014; @Braun2018; @Bergen2019]. Serial dependencies may therefore induce autocorrelations in perception by serving as internal predictions (or *memory* processes[@Gilden1995; @Gilden2001]) that actively integrate noisy sensory information over time[@Maloney2005]. 

Previous work has shown that such internal predictions can be built by dynamically updating the estimated probability of being in a particular perceptual state from the sequence of preceding experiences[@Burr2014; @Glaze2015; @St.John-Saaltink2016]. The integration of sequential inputs may lead to accumulating effects of perceptual history that progressively override incoming sensory information, enabling internal mode processing[@Weilnhammer2021a]. However, since such a process would lead to internal biases that may eventually become impossible to overcome[@Wexler2015], changes in mode may require ongoing wave-like fluctuations[@Gilden1995; @Gilden2001] in the perceptual impact of external and internal information that occur *irrespective* of the sequence of previous experiences and temporarily de-couple the decision variable from implicit internal representations of the environment[@Weilnhammer2021a]. 

Following Bayes' theorem, binary perceptual decisions depend on the log posterior ratio $L$ of the two alternative states of the environment that participants learn about via noisy sensory information[@Glaze2015]. We computed the posterior by combining the sensory evidence available at time-point $t$ (i.e., the log likelihood ratio $LLR$) with the prior probability $\psi$, weighted by the respective precision terms $\omega_{LLR}$ and $\omega_{\psi}$:

\begin{equation}
L_t = LLR_t * \omega_{LLR} + \psi_t(L_{t-1}, H) * \omega_{\psi}
\end{equation}

We derived the prior probability $\psi$ at timepoint $t$ from the posterior probability of perceptual outcomes at timepoint $L_{t-1}$. Since a switch between the two states can occur at any time, the effect of perceptual history varies according to both the sequence of preceding experiences and the estimated stability of the external environment (i.e., the *hazard rate* $H$[@Glaze2015]): 

\begin{equation}
\psi_t(L_{t-1}, H)  = L_{t-1} + log(\frac{1-H}{H} + exp(-L_{t-1})) - log(\frac{1-H}{H} + exp(L_{t-1}))
\end{equation}

The $LLR$ was computed from inputs $s_t$ by applying a sigmoid function defined by parameter $\alpha$ that controls the sensitivity of perception to the available sensory information (see Methods for details on $s_t$ in humans and mice):

\begin{equation}
u_t = \frac{1}{1 + exp(-\alpha * s_t)}
\end{equation}

\begin{equation}
LLR_t = log(\frac{u_t}{1-u_t})
\end{equation}

To allow for bimodal inference, i.e., alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$. These anti-phase sine functions (defined by amplitudes $a_{LLR/\psi}$, frequency $f$ and phase $p$) determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014a]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$:

\begin{equation}
\omega_{LLR} = a_{LLR} * sin(f * t + p) + 1
\end{equation}

\begin{equation}
\omega_{\psi} = a_{\psi} * sin(f * t + p + \pi) + 1
\end{equation}

Finally, a sigmoid transform of the posterior $L_t$ yields the probability of observing the perceptual decision $y_t$ at a temperature determined by $\zeta^{-1}$:

\begin{equation}
P(y_t = 1) = 1 - P(y_t = 0) = \frac{1}{1 + exp(-\zeta * L_t)}
\end{equation}

```{r optimization, cache = TRUE}

if (!load_summary_data) {
  
  # type_list = c(
  #             "fit_glaze_osc_zeta_v4_LLR_amp",
  #             "fit_glaze_osc_zeta_v4_Prior_amp",
  #             "fit_glaze_osc_zeta_v4_one_amp",
  #             "fit_glaze_osc_zeta_v4",
  #             "fit_glaze_osc_zeta_v4_no_amp",
  #             "fit_glaze_osc_zeta_v4_no_integration"
  #             ) 
  
    type_list = c(
              "fit_glaze_osc_zeta_v1_LLR_amp",
              "fit_glaze_osc_zeta_v1_Prior_amp",
              "fit_glaze_osc_zeta_v1",
              "fit_glaze_osc_zeta_v1_no_amp",
              "fit_glaze_osc_zeta_v1_no_integration"
              ) 

if (run_optim_human) {
  
  # available models
  source("./Functions/fit_glaze_osc_zeta_v1.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_one_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_integration.R",
  local = knitr::knit_global())

  type_idx = 1
  
  Optim = data.frame()
  for (id in unique(PwData$subject_id)) {
  Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_RT", "clear_Confidence")]
  
  if (type_list[type_idx] == "export_data") {
  ## Starting parameters
  write.csv(Input_Data, paste("./Modeling_Data/human_subject_", as.character(id), sep = ""),
  row.names = FALSE)
    add_O = data.frame()
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {
  ## Starting parameters
  par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_one_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_LLR_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp") {
  
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_Prior_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp") {
  ## Starting parameters
  par = c(0.01, 0.01, 5)
  lower = c(0.01, 0.01, 1)
  upper = c(0.99, 20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_amp,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration") {
  ## Starting parameters
  par = c(0.01, 5)
  lower = c(0.01, 1)
  upper = c(20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_integration,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  add_O$subject_id = id
  add_O$trial_n = nrow(Input_Data)
  add_O$free_param = length(par)
  add_O$type = type_list[type_idx]
  add_O$AIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 0
  )

  add_O$cAIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 1
  )

  Optim = rbind(Optim, add_O)

  print(paste((mean(Optim$p1, na.rm = TRUE)),
  (id / max(PwData$subject_id)) * 100,
  sep = ","
  ))
  }
  
  write.csv(Optim,
  paste("./Results/", type_list[type_idx], ".csv", sep = ""),
  row.names = FALSE)
  
} else {
Optim <- read.csv("./Results/Optim_human_free_phase_zeta.csv")
}

Optim <- Optim[Optim$subject_id %in% Behav$subject_id,]

Optim_Behav <-
  ddply(
    Optim,
    .(subject_id),
    summarise,
    Hazard = mean(p1, na.rm = TRUE),
    Precision =  mean(p2, na.rm = TRUE),
    amp_prior = mean(p3, na.rm = TRUE),
    amp_LLR = mean(p4, na.rm = TRUE),
    frequency = mean(p5, na.rm = TRUE),
    phase = mean(p6, na.rm = TRUE),
    zeta = mean(p7, na.rm = TRUE)
  )
Optim_Behav$study_id = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$study_id


##
## STATS Summary Modeling (+ sanity check)
##

Optim_Behav$Stimulus_History = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$Stimulus_History
Optim_Behav$Accuracy = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$Accuracy
Optim_Behav$History = Behav[Behav$subject_id %in% Optim_Behav$subject_id,]$History
Optim_Behav$Under_Estimation = Optim_Behav$Stimulus_History - (100-Optim_Behav$Hazard*100)


Optim_Underestimation <- lmer(Under_Estimation ~ 1 + (1|study_id), data = Optim_Behav)
STAT.Optim_Underestimation <- summary(Optim_Underestimation)

Optim_Accuracy_Precision <- lmer(Accuracy ~ Precision + (1|study_id), data = Optim_Behav)
STAT.Optim_Accuracy_Precision <- summary(Optim_Accuracy_Precision)

Optim_History_Hazard <- lmer(History ~ Hazard + (1|study_id), data = Optim_Behav)
STAT.Optim_History_Hazard <- summary(Optim_History_Hazard)

##
## get trial-wise log posterior ratio
##

if (generate_output_human) {

  Output = data.frame()
 for (id in unique(Optim$subject_id)) {
    print(id)
 
    
    Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_Confidence", "clear_RT","Accuracy","History", "study_id")]
    

   source("./Functions/output_glaze_osc_zeta_v4_one_amp.R", local = knitr::knit_global())
   add_Output <- output_glaze_osc_zeta_v4_one_amp(as.double(Optim[Optim$subject_id == id,c(1:5)]), Input_Data)
  
   Input_Data$clear_Confidence_minus_1 = c(NA, Input_Data$clear_Confidence[c(1:length(Input_Data$clear_Confidence)-1)])
   Input_Data$clear_RT_minus_1 = c(NA, Input_Data$clear_RT[c(1:length(Input_Data$clear_RT)-1)])
   add_Output$mu_minus_1 = c(NA,   add_Output$mu[c(1:length(add_Output$mu)-1)])
   
   Output = rbind(Output, cbind(abs(add_Output$mu),  Input_Data$clear_RT, Input_Data$clear_Confidence,
                                abs(add_Output$mu_minus_1),  Input_Data$clear_RT_minus_1, Input_Data$clear_Confidence_minus_1,
                                Input_Data$Accuracy, Input_Data$History,
                                id, Input_Data$study_id))
    
  
 }
} else {
   Output <- read.csv(paste(root, "Output_ext_zeta.csv", sep = ""))
}

colnames(Output) <- c("Certainty", "RT", "Confidence", "Certainty_minus_1", "RT_minus_1", "Confidence_minus_1","Accuracy","History","subject_id", "study_id")

Optim_RT_Certainty <- lmer(RT ~ Certainty + (1|study_id/subject_id), data = Output)
STAT.Optim_RT_Certainty <- summary(Optim_RT_Certainty)

Optim_Confidence_Certainty <- lmer(Confidence ~ Certainty + (1|study_id/subject_id), data = Output)
STAT.Optim_Confidence_Certainty <- summary(Optim_Confidence_Certainty)

if (run_logreg_Confidence_mu_minus_1){
Optim_History_mu_minus_1 <- glmer(History ~ Certainty_minus_1 + (1|study_id/subject_id),
        data = Output,
        family = binomial)
Optim_History_Confidence_minus_1 <- glmer(History ~ Confidence_minus_1 + (1|study_id/subject_id),
        data = Output,
        family = binomial)
} else {
Optim_History_mu_minus_1 <- readRDS(file = paste(root, "Optim_History_mu_minus_1_zeta.rds", sep = ""))
Optim_History_Confidence_minus_1 <- readRDS(file = paste(root, "Optim_History_Confidence_minus_1_zeta.rds", sep = ""))
}
STAT.Optim_History_mu_minus_1 <- summary(Optim_History_mu_minus_1)
STAT.Optim_History_Confidence_minus_1 <- summary(Optim_History_Confidence_minus_1)
  
  if (save_summary_data) {
    save(Optim, Optim_Behav,
         STAT.Optim_Underestimation,
         STAT.Optim_Accuracy_Precision,
         STAT.Optim_History_Hazard,
         STAT.Optim_RT_Certainty,
         STAT.Optim_Confidence_Certainty,
         STAT.Optim_History_mu_minus_1,
         STAT.Optim_History_Confidence_minus_1,
         file = "./Summary_Data/optimization_human.Rdata")
  }
  
} else {
  load("./Summary_Data/optimization_human.Rdata")
}

```

```{r optimization_mouse, cache = TRUE}


if (!load_summary_data) {
  
  type_list = c(
              "fit_glaze_osc_zeta_v1_mouse", 
              "fit_glaze_osc_zeta_v1_one_amp_mouse",
              "fit_glaze_osc_zeta_v1_LLR_amp_mouse",
              "fit_glaze_osc_zeta_v1_Prior_amp_mouse",
              "fit_glaze_osc_zeta_v1_no_amp_mouse",
              "fit_glaze_osc_zeta_v1_no_integration_mouse"
              ) 


  if (run_optim_mouse){

  # available models
  source("./Functions/fit_glaze_osc_zeta_v1_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_one_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_amp_mouse.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_integration_mouse.R",
  local = knitr::knit_global())

  type_idx = 6
  
  Optim = data.frame()
  for (id in unique(MwData$subject_id)) {
    
  for (session_id in unique(MwData[MwData$subject_id == id,]$session_id)) {
    
    Input_Data = MwData[MwData$subject_id == id & MwData$session_id == session_id, c("Stimulus", "Response", "Difficulty")]
    
    Input_Data$Stimulus <- (Input_Data$Stimulus*2 - 1) * Input_Data$Difficulty
    Input_Data$Stimulus[Input_Data$Difficulty == 0] = 0
    
  if (type_list[type_idx] == "export_data") {
  ## Starting parameters
  write.csv(Input_Data, paste("./Modeling_Data/mouse_subject_", as.character(id), "_session_", as.character(session_id), ".csv", sep = ""),
  row.names = FALSE)
    add_O = data.frame()
  }   
    
    
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
    
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_one_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_LLR_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
  lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
  upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
  
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_Prior_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp_mouse") {
  ## Starting parameters
  par = c(0.01, 0.01, 5)
  lower = c(0.01, 0.01, 1)
  upper = c(0.99, 20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_amp_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  } 
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration_mouse") {
  ## Starting parameters
  par = c(0.01, 5)
  lower = c(0.01, 1)
  upper = c(20, 10)
  
  add_O =  optimx(
  par,
  fit_glaze_osc_zeta_v1_no_integration_mouse,
  Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
  !is.na(Input_Data$Response),],
  lower = lower,
  upper = upper
  )
  }

  add_O$subject_id = id
  add_O$session_id = session_id
  add_O$trial_n = nrow(Input_Data)
  add_O$free_param = length(par)
  add_O$type = type_list[type_idx]
  add_O$AIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 0
  )
  add_O$cAIC = compute_AIC(
  LL = -add_O$value,
  K =  add_O$free_param,
  n = add_O$trial_n,
  correction = 1
  )

  Optim = rbind(Optim, add_O)
    
  print(paste((mean(Optim$p1, na.rm = TRUE)),
  (id / max(MwData$subject_id)) * 100,
  sep = ","
  ))
  }  
    
  }
  write.csv(Optim,
  paste("./Results/", type_list[type_idx], ".csv", sep = ""),
  row.names = FALSE)
 } else {
M_Optim <- read.csv("./Results/Optim_mouse_free_phase_zeta.csv") 
}

M_Summary_Compare_Optim<-
  ddply(
    M_Lbits,
    .(model),
    summarise,
    mean_Lbits = mean(M_Lbits, na.rm = TRUE),
    error_Lbits = sd(M_Lbits, na.rm = TRUE) / sqrt(length(M_Lbits)))

M_Optim <- M_Optim[M_Optim$subject_id %in% M_Behav$subject_id,]

M_Behav_sess <- 
  ddply(
    MwData, 
    .(subject_id, session_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100, 
    History = mean(History, na.rm = TRUE)*100, 
    Accuracy = mean(Accuracy, na.rm = TRUE)*100
  )

Optim_M_Behav_sess <- 
  ddply(
    M_Optim, 
    .(subject_id, session_id),
    summarise,
    Hazard = mean(p1, na.rm = TRUE), 
    Precision =  mean(p2), 
    amp = mean(p3),
    frequency = mean(p4),
    phase = mean(p5),
    conv = convcode
  )

Optim_M_Behav <-
  ddply(
    M_Optim,
    .(subject_id),
    summarise,
    Hazard = mean(p1, na.rm = TRUE),
    Precision =  mean(p2),
    amp_prior = mean(p3),
    amp_LLR = mean(p4),
    frequency = mean(p5),
    phase = mean(p6),
    zeta = mean(p7)
  )
Optim_M_Behav$Stimulus_History = M_Behav$Stimulus_History
Optim_M_Behav$Accuracy = M_Behav$Accuracy
Optim_M_Behav$History = M_Behav$History
Optim_M_Behav$Under_Estimation = M_Behav$Stimulus_History - (100-Optim_M_Behav$Hazard*100)
##
## STATS Summary Modeling (+ sanity check)
##
Optim_M_Behav_sess$Stimulus_History = M_Behav_sess$Stimulus_History
Optim_M_Behav_sess$Accuracy = M_Behav_sess$Accuracy
Optim_M_Behav_sess$History = M_Behav_sess$History
Optim_M_Behav_sess$Under_Estimation = M_Behav_sess$Stimulus_History - (100-Optim_M_Behav_sess$Hazard*100)

M_Optim_Underestimation <- lmer(Under_Estimation ~ 1 + (1|subject_id), data = Optim_M_Behav_sess)
M_STAT.Optim_Underestimation <- summary(M_Optim_Underestimation)

M_Optim_Accuracy_Precision <- lmer(Accuracy ~ Precision + (1|subject_id), data = Optim_M_Behav_sess)
M_STAT.Optim_Accuracy_Precision <- summary(M_Optim_Accuracy_Precision)

M_Optim_History_Hazard <- lmer(History ~ Hazard + (1|subject_id), data = Optim_M_Behav_sess)
M_STAT.Optim_History_Hazard <- summary(M_Optim_History_Hazard)

##
## get trial-wise log posterior ratio
##
if (generate_output_mouse) {
M_Output = data.frame()
 for (id in unique(MwData$subject_id)) {
    print(id)
  for (session_id in unique(MwData[MwData$subject_id == id,]$session_id)) { 
    
    Input_Data = MwData[MwData$subject_id == id & MwData$session_id == session_id, c("Stimulus", "Response", "Difficulty", "clear_RT","Accuracy","History", "Stimulus_History")]
    
    Input_Data$Stimulus <- (Input_Data$Stimulus*2 - 1) * Input_Data$Difficulty
    Input_Data$Stimulus[Input_Data$Difficulty == 0] = 0
    
   source("./Functions/output_glaze_osc_mouse_zeta_v4_one_amp.R", local = knitr::knit_global())
   add_Output <- output_glaze_osc_mouse_zeta_v4_one_amp(as.double(M_Optim[M_Optim$subject_id == id & M_Optim$session_id == session_id,c(1:7)]), Input_Data)

   Input_Data$clear_RT_minus_1 = c(NA, Input_Data$clear_RT[c(1:length(Input_Data$clear_RT)-1)])
   add_Output$mu_minus_1 = c(NA,   add_Output$mu[c(1:length(add_Output$mu)-1)])

   M_Output = rbind(
     M_Output,
     cbind(
     abs(add_Output$mu),
     Input_Data$clear_RT,
     abs(add_Output$LLR),
     abs(add_Output$muhat),
     # abs(add_Output$muhat * add_Output$oscillation_ei) * (-sign(M_Optim[M_Optim$subject_id == id &
     # M_Optim$session_id == session_id, c(1:7)]$p1 - 0.5)) - log(mean(Input_Data$Stimulus_History, na.rm = TRUE) /
     # (
     # 1 - mean(Input_Data$Stimulus_History, na.rm = TRUE)
     # )),
     abs(add_Output$mu_minus_1),
     Input_Data$clear_RT_minus_1,
     Input_Data$Accuracy,
     Input_Data$History,
     id,
     session_id
     )
     )
    
  }
 }
} else {
   M_Output <- read.csv(paste(root, "M_Output_zeta.csv", sep = ""))

}

colnames(M_Output) <- c("Certainty", "RT", "LLR", "muhat", "Certainty_minus_1","RT_minus_1", "Accuracy","History", "subject_id", "session_id")

M_Optim_RT_Certainty <- lmer(RT ~ Certainty + (1|subject_id/session_id), data = M_Output)
M_STAT.Optim_RT_Certainty <- summary(M_Optim_RT_Certainty)

if (run_mouse_logreg_mu_minus_1){
M_Optim_History_mu_minus_1 <- glmer(History ~ Certainty_minus_1 + (1 + Certainty_minus_1|subject_id/session_id),
        data = M_Output,
        family = binomial)
} else {
M_Optim_History_mu_minus_1 <- readRDS(file = paste(root, "M_Optim_History_mu_minus_1_zeta.rds", sep = ""))
}
M_STAT.Optim_History_mu_minus_1 <- summary(M_Optim_History_mu_minus_1)
  
  if (save_summary_data) {
    save(M_Optim,  
         M_Behav_sess,
         Optim_M_Behav_sess,
         Optim_M_Behav,
         M_STAT.Optim_Underestimation,
         M_STAT.Optim_Accuracy_Precision,
         M_STAT.Optim_History_Hazard,
         M_STAT.Optim_RT_Certainty,
         M_STAT.Optim_History_mu_minus_1,
         file = "./Summary_Data/optimization_mouse.Rdata")
  }
  
} else {
  load("./Summary_Data/optimization_mouse.Rdata")
}
```

```{r simulation_reproduce_zeta, cache = TRUE}
##
## Reproduce behavioral data from inverted model in humans
##

if (!load_summary_data) {
  
  
if (run_simulation) {
  
type_list = c(
              "fit_glaze_osc_zeta_v1"
              ) 
source("./Functions/simulation_glaze_osc_human_zeta.R", local = knitr::knit_global())

Sim <- data.frame()
for (type_idx in c(1:length(type_list))){
  
reproduce_Optim  <- read.csv(paste("./Results/", type_list[type_idx], ".csv", sep = ""))
n_participants =  nrow(reproduce_Optim)
n = runif(n_participants, 1000, 2000)

# outcomes
outcomes = c(0,1)
for (subj_idx in c(1:n_participants)){
  
  if ((subj_idx %% 10) == 0){print(subj_idx)}
  
  random_steps = rnorm(n = n[subj_idx], mean = 0, sd = 0)
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {

  Sim_add <-
  simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  reproduce_Optim$p1[subj_idx],
  reproduce_Optim$p2[subj_idx],
  reproduce_Optim$p3[subj_idx],
  reproduce_Optim$p4[subj_idx],
  reproduce_Optim$p5[subj_idx],
  reproduce_Optim$p6[subj_idx],
  1,
  random_steps,
  sliding_window,
  n_permutations
  )

  }
  
  if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
  Sim_add <-
  simulation_glaze_osc_human_zeta_LLR_amp(
  outcomes,
  n[subj_idx],
  reproduce_Optim$p1[subj_idx],
  reproduce_Optim$p2[subj_idx],
  reproduce_Optim$p3[subj_idx],
  reproduce_Optim$p4[subj_idx],
  reproduce_Optim$p5[subj_idx],
  1,
  random_steps,
  sliding_window,
  n_permutations
  )
    }
  
  Sim_add$diff_acf_Stimulus <- exclude_3SD(Sim_add$acf_Stimulus - Sim_add$random_acf_Stimulus) 
  Sim_add$diff_acf_History <- exclude_3SD(Sim_add$acf_History - Sim_add$random_acf_History) 
  Sim_add$subject_id = subj_idx
  Sim_add$Trial = c(1:n[subj_idx])
  Sim_add$H = reproduce_Optim$p1[subj_idx]
  Sim_add$prec = reproduce_Optim$p2[subj_idx]
  Sim_add$amp = reproduce_Optim$p3[subj_idx]
  Sim_add$amp_LLR = reproduce_Optim$p4[subj_idx]
  Sim_add$frequency = reproduce_Optim$p5[subj_idx]
  Sim_add$phase =  reproduce_Optim$p6[subj_idx]
  Sim_add$zeta =  reproduce_Optim$p7[subj_idx]
  Sim_add$random_steps = random_steps
  Sim_add$type = type_list[type_idx]
  
  Sim = rbind(Sim, Sim_add)
}
}

} else {
  Sim <- read.csv(paste(root, "Sim_posterior_param_variable_n_human_zeta.csv", sep = ""))
  type_list = unique(Sim$type)
}

## compute summary of simulated behavior
Sim_Behav <- 
  ddply(
    Sim, 
    .(subject_id,type),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100, 
    Min_History = min(History_slider, na.rm = TRUE)*100,
    Max_History = max(History_slider, na.rm = TRUE)*100,
    Accuracy = sum(Accuracy, na.rm = TRUE)/length(Accuracy)*100,
    Min_Accuracy = min(Accuracy_slider, na.rm = TRUE)*100,
    Max_Accuracy = max(Accuracy_slider, na.rm = TRUE)*100
  )

Sim_Behav_diff <- 
   ddply(
    Sim[!is.na(Sim$Accuracy),], 
    .(subject_id, Accuracy),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100
  )

Sim_Behav$null_History = Sim_Behav$History - 50
Sim_STAT.Global_History_Accuracy = t.test(Sim_Behav$null_History)

compare_participants = which(Sim_Behav_diff[Sim_Behav_diff$Accuracy == 1,]$subject_id %in% Sim_Behav_diff[Sim_Behav_diff$Accuracy == 0,]$subject_id)

Sim_STAT.diff_History_Accuracy = t.test(Sim_Behav_diff[Sim_Behav_diff$Accuracy == 0 & Sim_Behav_diff$subject_id %in% compare_participants,]$History, Sim_Behav_diff[Sim_Behav_diff$Accuracy == 1 & Sim_Behav_diff$subject_id %in% compare_participants,]$History, paired = TRUE)

## compute_Summary_acf

source("./Functions/f_compute_sim_group_acf.R", local = knitr::knit_global())

acf_to_test = c('diff_acf_Stimulus', 'diff_acf_History')
max_trial = 99
Sim_Summary_acf <- f_compute_sim_group_acf(Sim, acf_to_test, max_trial, type_list)

Sim_Summary_acf$Trial <- Sim_Summary_acf$Trial - 1


ID_Sim_Summary_acf <- ddply(
    Sim, 
    .(subject_id,type),
    summarise,
    lag_significant_Stimulus = min(which(diff(which(c(0, exceed_acf_Stimulus,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_History = min(which(diff(which(c(0, exceed_acf_History,0) < n_permutations/2)) != 1)) - 1
  )
ID_Sim_Summary_acf$lag_significant_Stimulus[ID_Sim_Summary_acf$lag_significant_Stimulus == Inf] = NA
ID_Sim_Summary_acf$lag_significant_History[ID_Sim_Summary_acf$lag_significant_History == Inf] = NA

##
## Autocorrelations decay over time
##

Sim$min_diff_acf_Stimulus = Sim$diff_acf_Stimulus - min(Sim$diff_acf_Stimulus, na.rm = TRUE)
Sim$min_diff_acf_History = Sim$diff_acf_History - min(Sim$diff_acf_History, na.rm = TRUE)

Sim_lmer_acf_Stimulus <- lmer(log(min_diff_acf_Stimulus) ~ Trial + (1|subject_id), data = Sim[Sim$Trial > 1 & Sim$Trial < 21 & Sim$min_diff_acf_Stimulus > 0,])
Sim_STAT.lmer_acf_Stimulus <- summary(Sim_lmer_acf_Stimulus)

Sim_lmer_acf_History <- lmer(log(min_diff_acf_History) ~ Trial + (1|subject_id), data = Sim[Sim$Trial > 1 & Sim$Trial < 21 & Sim$min_diff_acf_History > 0,])
Sim_STAT.lmer_acf_History <- summary(Sim_lmer_acf_History)


if (compute_power_spectra_simulation) {
source("./Functions/f_compute_power_spectra.R", local = knitr::knit_global())

sliders = c("Accuracy_slider", "History_slider")  
Sim_Power_Spectra <- f_compute_power_spectra(Sim, sliders)  
} else {
  #Sim_Power_Spectra <- read.csv(paste(root, "Sim_power_spectra_smoothed_param_variable_n_human_zeta.csv", sep = ""))
  Sim_Power_Spectra <- read.csv(paste(root, "Power_Spectra_LLR.csv", sep = ""))
}

Sim_Power_Spectra$r_freq = round(Sim_Power_Spectra$freq, digits = 4)

step_size = 0.01
bins <- seq(from = min(Sim_Power_Spectra$freq), to = max(Sim_Power_Spectra$freq), by = step_size)
names <- round(bins, digits = 2)
names = names[1:length(names) - 1]

Sim_Power_Spectra$bin_freq <- cut(Sim_Power_Spectra$freq, breaks = bins, labels = names)
Sim_Power_Spectra$bin_freq <- as.numeric(Sim_Power_Spectra$bin_freq) * step_size
Sim_Power_Spectra$bin_freq[is.na(Sim_Power_Spectra$bin_freq)] = max(Sim_Power_Spectra$freq)

Sim_Power_Spectra$Coherence <- Sim_Power_Spectra$Coherence * 100



gathercol = colnames(Sim_Power_Spectra[, c(2,3)])
Sim_Power_Spectra_long  <-
gather(Sim_Power_Spectra[, c(1,2,3, 6,7,8)],
"Variable",
"Power",
gathercol,
factor_key = TRUE)

Sim_Power_Frequency <-  ddply(
      Sim_Power_Spectra_long,
      .(r_freq, Variable),
      summarise,
      
      mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
      ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE)/sqrt(length(exclude_3SD(Power)))
 )

Sim_Coherence_Phase_Frequency <- ddply(
      Sim_Power_Spectra,
      .(bin_freq),
      summarise,
    
      mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
      ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE)/sqrt(length(exclude_3SD(Coherence))),
      
      mode_phase = getmode(exclude_3SD((abs(Phase)))),
      mean_phase = getmode(exclude_3SD((abs(Phase)))),
      ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE)/sqrt(length(exclude_3SD(Phase)))
 )

##
## mode of coherence and phase
##
Sim_Summary_Power_Spectra <- 
   ddply(
 Sim_Power_Spectra[Sim_Power_Spectra$freq > 0.01 & Sim_Power_Spectra$freq < 0.1,],
       .(subject_id),
       summarise,
       mean_coherence = mean(exclude_3SD((
 Coherence))),
       mode_phase = getmode(exclude_3SD((
 abs(Phase))))
   )

##
## STATS Power vs Frequency (1/f noise)
##

Sim_lmer_power_freq_Stimulus <- lmer(log(Power_Stimulus) ~ log(freq) + (1|subject_id), data = Sim_Power_Spectra[Sim_Power_Spectra$freq > 0.01 & Sim_Power_Spectra$freq < 0.1 & is.finite(log(Sim_Power_Spectra$Power_Stimulus)),])
Sim_STAT.lmer_power_freq_Stimulus <- summary(Sim_lmer_power_freq_Stimulus)

Sim_lmer_power_freq_History <- lmer(log(Power_History) ~ log(freq) + (1|subject_id), data = Sim_Power_Spectra[Sim_Power_Spectra$freq > 0.01 & Sim_Power_Spectra$freq < 0.1 & is.finite(log(Sim_Power_Spectra$Power_History)),])
Sim_STAT.lmer_power_freq_History <- summary(Sim_lmer_power_freq_History)


##
## Simulated modes
##
Sim_Mode_gather <-  ddply(
  Sim,
  .(subject_id),
  summarise,
  
  directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
  scaled_directed_mode = scale(directed_mode),
  strength_mode = abs(scaled_directed_mode)
)

Sim$directed_mode = Sim_Mode_gather$directed_mode
Sim$scaled_directed_mode = Sim_Mode_gather$scaled_directed_mode
Sim$strength_mode = Sim_Mode_gather$strength_mode

Sim_ID_mode <-  ddply(
  Sim,
  .(subject_id),
  summarise,
  strength_mode = mean(strength_mode, na.rm = TRUE),
  directed_mode = mean(directed_mode, na.rm = TRUE)
)


##
## Simulated confidence
##

Sim$Confidence <- abs(Sim$mu) 
Sim$scaled_Confidence <- scale(Sim$Confidence)
Sim$clear_Confidence <- Sim$Confidence
Sim[Sim$clear_Confidence > median(Sim$clear_Confidence, na.rm = TRUE) + 3*median(Sim$clear_Confidence, na.rm = TRUE),]$clear_Confidence <- NA

Sim_Post_Perceptual_Modes = ddply(
  Sim[!is.na(Sim$History) & !is.na(Sim$Accuracy) & !is.na(Sim$clear_Confidence),], 
  .(directed_mode),
  summarise,
  average_Confidence = mean(clear_Confidence, na.rm = TRUE),
  se_Confidence = sd(clear_Confidence, na.rm = TRUE)/sqrt(length(clear_Confidence)),
  n = length(mu),
  n_percent = (sum(!is.na(mu))/nrow(Sim))*100
)

Sim_slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|subject_id), data = Sim)
Sim_STAT.slider_History_vs_Accuracy <- summary(Sim_slider_History_vs_Accuracy)

##
## Confidence vs mode: Prepare plot and stats
##
Sim_Confidence_Behav <- 
  ddply(
    Sim[!is.na(Sim$History) & !is.na(Sim$Accuracy) & !is.na(Sim$clear_Confidence),], 
    .(subject_id),
    summarise,
    diff_Confidence_History = mean(clear_Confidence[History == 1], na.rm = TRUE) -  mean(clear_Confidence[History == 0], na.rm = TRUE),
    diff_Confidence_Stimulus = mean(clear_Confidence[Accuracy == 1], na.rm = TRUE) -  mean(clear_Confidence[Accuracy == 0], na.rm = TRUE)
    )


gathercol = colnames(Sim_Confidence_Behav[, c(2,3)])
Sim_Confidence_Behav_long  <-
  gather(Sim_Confidence_Behav[,c(1,2,3)],
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)

Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_History", "History", Sim_Confidence_Behav_long$Variable)
Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_Stimulus", "Stimulus", Sim_Confidence_Behav_long$Variable)

Sim_diff_Confidence <- lmer(diff ~ Variable + (1|subject_id), data = Sim_Confidence_Behav_long)
Sim_STAT.diff_Confidence <- summary(Sim_diff_Confidence)

## exclude low performance mice
Summary_Sim_Confidence_Behav <- 
  ddply(
    Sim_Confidence_Behav_long, 
    .(Variable),
    summarise,
    Mean = mean(diff, na.rm = TRUE),
    Error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))

Sim_lmer_Confidence_Accuracy_History <- lmer(clear_Confidence ~ Accuracy + History + (1|subject_id), data = Sim)
Sim_STAT.lmer_Confidence_Accuracy_History <- summary(Sim_lmer_Confidence_Accuracy_History)


##
## Confidence vs. mode
##

Sim_Confidence_vs_mode <- lmer(clear_Confidence ~ poly(directed_mode, 2) + (1|subject_id), data = Sim)
Sim_STAT.Confidence_vs_mode <- summary(Sim_Confidence_vs_mode)
  
  if (save_summary_data) {
    save(Sim_Behav, 
         Sim_Behav_diff,
         Sim_STAT.Global_History_Accuracy,
         Sim_STAT.diff_History_Accuracy,
         Sim_Summary_acf,
         ID_Sim_Summary_acf,
         Sim_STAT.lmer_acf_Stimulus,
         Sim_STAT.lmer_acf_History,
         Sim_Power_Spectra_long,
         Sim_Power_Frequency,
         Sim_Coherence_Phase_Frequency,
         Sim_Summary_Power_Spectra,
         Sim_STAT.lmer_power_freq_Stimulus,
         Sim_STAT.lmer_power_freq_History,
         Sim_Post_Perceptual_Modes,
         Sim_STAT.slider_History_vs_Accuracy,
         Sim_Confidence_Behav,
         Sim_Confidence_Behav_long,
         Sim_STAT.diff_Confidence,
         Summary_Sim_Confidence_Behav,
         Sim_STAT.lmer_Confidence_Accuracy_History,
         Sim_STAT.Confidence_vs_mode,
         Sim_Power_Spectra,
        file = "./Summary_Data/simulation_reproduce_zeta.Rdata")
  }
  
} else {
  load("./Summary_Data/simulation_reproduce_zeta.Rdata")
}
```

```{r evaluate_human, cache = TRUE}
if (evaluate_human){
type_list = c(
  "fit_glaze_osc_zeta_v1",
  "fit_glaze_osc_zeta_v1_LLR_amp",
  "fit_glaze_osc_zeta_v1_Prior_amp",
  "fit_glaze_osc_zeta_v1_no_amp",
  "fit_glaze_osc_zeta_v1_no_integration"
  )


  # available models
  source("./Functions/fit_glaze_osc_zeta_v1.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_one_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_amp.R",
  local = knitr::knit_global())
  source("./Functions/fit_glaze_osc_zeta_v1_no_integration.R",
  local = knitr::knit_global())


block_size = 10
Optim_eval = data.frame()
n_start = 4120
#for (id in unique(PwData$subject_id)) {
#for (id in unique(PwData$subject_id[PwData$subject_id > max(PwData$subject_id)/2])) {
for (id in unique(PwData$subject_id[PwData$subject_id >= n_start])) {
  print(paste("human:", id, sep = " "))
  
    Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_RT", "clear_Confidence")]
  
  
    Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
                              !is.na(Input_Data$Response), ]
    
    if (nrow(Input_Data)  > block_size) {
      for (trial_idx in seq(from = 1, to = nrow(Input_Data) - block_size  + 1, by = block_size)) {
        train =  Input_Data[seq(from = trial_idx,
                                to = trial_idx + block_size  - 1,
                                by = 1), ]
        
        for (type_idx in c(1:length(type_list))) {
        #print(type_list[type_idx])
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {
            ## Starting parameters
            par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp") {
            ## Starting parameters
            par = c(0.01, 0.01, 5)
            lower = c(0.01, 0.01, 1)
            upper = c(0.99, 20, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration") {
            ## Starting parameters
            par = c(0.01, 5)
            lower = c(0.01, 1)
            upper = c(20, 10)
          }
        
          suppressWarnings({
            add_O =  optimx(
              par,
              match.fun(type_list[type_idx]),
              Input_Data = train,
              lower = lower,
              upper = upper
            )
          })
    
          
          
          add_O$subject_id = id
          add_O$block_id = trial_idx
          add_O$trial_n = nrow(train)
          add_O$free_param = length(par)
          add_O$type = type_list[type_idx]
          add_O$AIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 0
          )
          add_O$cAIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 1
          )
          
          Optim_eval = rbind(Optim_eval, add_O[, c('subject_id', 'block_id','value', 'AIC', 'type', 'free_param', 'trial_n', 'convcode')])
          #Optim_eval = rbind(Optim_eval, add_O)
        }
      
    }
    }
    
    write.csv(Optim_eval, paste("./Results/Optim_eval_human_interim_min_", as.character(n_start), ".csv", sep = ""), row.names = FALSE)
    Sum <-   ddply(Optim_eval,
                   .(type),
                   summarise,
                   error = mean(value, na.rm = TRUE))
    print(Sum)
}
}
```

```{r evaluate_mouse, cache = TRUE}
if (evaluate_mouse){
type_list = c(
  "fit_glaze_osc_zeta_v1_mouse",
  "fit_glaze_osc_zeta_v1_LLR_amp_mouse",
  "fit_glaze_osc_zeta_v1_Prior_amp_mouse",
  "fit_glaze_osc_zeta_v1_no_amp_mouse",
  "fit_glaze_osc_zeta_v1_no_integration_mouse"
  )


source("./Functions/fit_glaze_osc_zeta_v1_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_one_amp_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp_mouse.R",
       local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_no_amp_mouse.R",
       local = knitr::knit_global())
source( "./Functions/fit_glaze_osc_zeta_v1_no_integration_mouse.R",
  local = knitr::knit_global()
)
source("./Functions/fit_glaze_osc_zeta_v1_random_walk_mouse.R",
  local = knitr::knit_global()
)


block_size = 10
n_start = 100
Optim_eval = data.frame()
#for (id in unique(MwData$subject_id)) {
#for (id in unique(MwData$subject_id[MwData$subject_id > max(MwData$subject_id)/2])) {
for (id in unique(MwData$subject_id[MwData$subject_id > n_start])) {
  print(paste("mouse:", id, sep = " "))
  for (session_id in unique(MwData[MwData$subject_id == id, ]$session_id)) {
    print(paste("session:", session_id, sep = " "))
    Input_Data = MwData[MwData$subject_id == id &
                          MwData$session_id == session_id, c("Stimulus", "Response", "Difficulty")]
    
    Input_Data$Stimulus <-
      (Input_Data$Stimulus * 2 - 1) * Input_Data$Difficulty
    Input_Data$Stimulus[Input_Data$Difficulty == 0] = 0
    Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
                              !is.na(Input_Data$Response), ]
    
    if (nrow(Input_Data)  > block_size) {
      for (trial_idx in seq(from = 1, to = nrow(Input_Data) - block_size  + 1, by = block_size)) {
      #for (trial_idx  in c(1)) {
        train =  Input_Data[seq(from = trial_idx,
                                to = trial_idx + block_size  - 1,
                                by = 1), ]
        
        for (type_idx in c(1:length(type_list))) {

          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
            lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
            upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 5)
            lower = c(0.01, 0.01, 1)
            upper = c(0.99, 20, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration_mouse") {
            ## Starting parameters
            par = c(0.01, 5)
            lower = c(0.01, 1)
            upper = c(20, 10)
          }
          
          if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_random_walk_mouse") {
            ## Starting parameters
            par = c(0.01, 0.01, 3, 0.1, 5)
            lower = c(0.01, 0.01,-5, 0.01, 1)
            upper = c(0.99, 20,+5, 1, 10)
          }
          
          add_O =  optimx(
            par,
            match.fun(type_list[type_idx]),
            Input_Data = train,
            lower = lower,
            upper = upper
          )
          
          
          add_O$subject_id = id
          add_O$session_id = session_id
          add_O$block_id = trial_idx
          add_O$trial_n = nrow(train)
          add_O$free_param = length(par)
          add_O$type = type_list[type_idx]
          add_O$AIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 0
          )
          add_O$cAIC = compute_AIC(
            LL = -add_O$value,
            K =  add_O$free_param,
            n = add_O$trial_n,
            correction = 1
          )
       Optim_eval = rbind(Optim_eval, add_O[, c('subject_id', 'session_id', 'block_id', 'value', 'type', 'free_param', 'trial_n', 'convcode')])   
        }
      }
    }
  }
  
          write.csv(Optim_eval,
              paste("./Results/Optim_eval_mouse_interim_min_", as.character(n_start), ".csv", sep = ""),
              row.names = FALSE)
          
          # Sum <- ddply(Optim_eval,
          #          .(type),
          #          summarise,
          #          error = mean(abs(value), na.rm = TRUE))
          # print(Sum)
}
}
```

```{r model_comparison, cache = TRUE}

if (!load_summary_data) {
  
## load human data
# Optim_eval = data.frame()
# human_files = list.files(path='./Results', pattern='Optim_eval_human_')
# for (load_idx in human_files){
# Optim_eval = rbind(Optim_eval, read.csv(paste("./Results/", load_idx, sep = "")))
# }
Optim_eval = read.csv("./Results/Optim_eval_human_complete.csv")  
Optim_eval$species = "humans"
Optim_eval$unique_id = as.character(Optim_eval$subject_id * rand(1))
Optim_eval$study_id = NA
for (idx in unique(Optim_eval$subject_id)){
Optim_eval[Optim_eval$subject_id == idx,]$study_id =  Behav[Behav$subject_id == idx,]$study_id 
}

## load mouse data
# Optim_eval_mice = data.frame()
# mice_files = list.files(path='./Results', pattern='Optim_eval_mouse_')
# for (load_idx in mice_files){
# Optim_eval_mice = rbind(Optim_eval_mice, read.csv(paste("./Results/", load_idx, sep = "")))
# }
Optim_eval_mice = read.csv("./Results/Optim_eval_mouse_complete.csv")
Optim_eval_mice$species = "mice"
Optim_eval_mice$unique_id = as.character(Optim_eval_mice$subject_id * Optim_eval_mice$session_id * rand(1))
Optim_eval_mice$study_id = NA

## group and rename
Optim_eval = rbind(Optim_eval[,c("value", "type", "free_param", "trial_n", "species", "convcode", "study_id", "unique_id")], Optim_eval_mice[,c("value", "type", "free_param", "trial_n", "species", "convcode", "study_id", "unique_id")])
Optim_eval$type = gsub("_mouse", "", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_no_integration", "5: no-evidence-accumulation", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_no_amp", "4: normative-evidence-accumulation", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_Prior_amp", "3: prior-oscillation-only model", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_LLR_amp", "2: likelihood-oscillation-only model", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1" , "1: Bimodal inference", Optim_eval$type)

## summarize
Sum_AIC_Models <-   ddply(Optim_eval[Optim_eval$convcode == 0,],
                   .(species, type),
                   summarise,
                   median_error = median(value, na.rm = TRUE),
                   sum_error = sum(value, na.rm = TRUE),
                   k = mean(free_param),
                   AIC = 2*k + 2 * sum(value, na.rm = TRUE)
                ) 

Sum_Subj_Model_AIC <-   ddply(Optim_eval[Optim_eval$convcode == 0,],
                   .(unique_id, type, species),
                   summarise,
                   median_error = median(value, na.rm = TRUE),
                   sum_error = sum(value, na.rm = TRUE),
                   k = mean(free_param),
                   AIC = 2*k + 2 * sum(value, na.rm = TRUE),
                   study_id = unique(study_id)
                )   

STAT.M1_M2 = summary(lmer(AIC ~ as.factor(type) + (1|study_id), 
                  data = Sum_Subj_Model_AIC[(Sum_Subj_Model_AIC$type == "1: Bimodal inference" | Sum_Subj_Model_AIC$type == "2: likelihood-oscillation-only model") & Sum_Subj_Model_AIC$species == "humans", ]))

M_STAT.M1_M2 = t.test(Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type == "1: Bimodal inference" & Sum_Subj_Model_AIC$species == "mice", ]$AIC - Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type == "2: likelihood-oscillation-only model" & Sum_Subj_Model_AIC$species == "mice", ]$AIC)  

  if (save_summary_data) {
    save(Optim_eval,
         Sum_AIC_Models,
         Sum_Subj_Model_AIC,
         STAT.M1_M2,
         M_STAT.M1_M2,
         file = "./Summary_Data/model_comparison.Rdata")
  }
  
} else {
  load("./Summary_Data/model_comparison.Rdata")
}
#print(Sum_AIC_Models)
```

We used a maximum likelihood procedure to fit the bimodal inference model (M1, Figure 1F) to the behavioral data from the Confidence database[@Rahnev2020] and the IBL database[@IBL2021], optimizing the parameters $\alpha$, $H$, $amp_{LLR}$, $amp_{\psi}$, $f$, $p$ and $\zeta$ (see Methods for details and Supplemental Table T2 for a summary of the parameters of the bimodal inference model). We validated our model in three steps:

First, to show that bimodal inference does not emerge spontaneously in normative Bayesian models of evidence accumulation, but requires the ad-hoc addition of anti-phase oscillations in prior and likelihood precision, we compared the bimodal inference model to four control models (M2-5, Figure 1G). In these models, we successively removed the anti-phase oscillations (M2-M4) and the integration of information across trials (M5) from the bimodal inference model and performed a model comparison based on AIC. 

Model M2 ($AIC_2$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in mice) and Model M3 ($AIC_3$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in mice) incorporated only oscillations of either likelihood or prior precision. Model M4 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model M5 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information (Figure 1G). 

The bimodal inference model achieved the lowest AIC across the full model space ($AIC_1$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in mice) and was clearly superior to the normative Bayesian model of evidence accumulation ($\delta_{AIC}$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice; Supplemental Figure S9).

As a second validation of the bimodal inference model, we tested whether the posterior model predicted within-training and out-of-training variables. The bimodal inference model characterizes each subject by a sensitivity parameter $\alpha$ (humans: $\alpha$ = $`r mean(Optim_Behav$Precision, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$Precision, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $\alpha$ = $`r mean(Optim_M_Behav$Precision, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$Precision, na.rm = TRUE)/nrow(Optim_M_Behav)`$) that captures how strongly perception is driven by the available sensory information, and a hazard rate parameter $H$ (humans: $H$ = $`r mean(Optim_Behav$Hazard, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$Hazard, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $H$ = $`r mean(Optim_M_Behav$Hazard, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$Hazard, na.rm = TRUE)/nrow(Optim_M_Behav)`$) that controls how heavily perception is biased by perceptual history. The parameter $f$ captures the dominant time scale at which likelihood (amplitude humans: $a_{LLR}$ = $`r mean(Optim_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $a_{LLR}$ = $`r mean(Optim_M_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_M_Behav)`$) and prior precision (amplitude humans: $a_{\psi}$ = $`r mean(Optim_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $a_{\psi}$ = $`r mean(Optim_M_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_M_Behav)`$) fluctuated and was estimated at $`r mean(Optim_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$frequency, na.rm = TRUE)/nrow(Optim_Behav)`$ 1/$N_{trials}$ and $`r mean(Optim_M_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$frequency, na.rm = TRUE)/nrow(Optim_M_Behav)`$ 1/$N_{trials}$ in mice.

As a sanity check for model fit, we tested whether the frequency of stimulus- and history-congruent trials in the Confidence database[@Rahnev2020] and IBL database[@IBL2021] correlated with the estimated parameters $\alpha$ and $H$, respectively. As expected, the estimated sensitivity toward stimulus information $\alpha$ was positively correlated with the frequency of stimulus-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r STAT.Optim_Accuracy_Precision$coefficients[2,5]`$; mice: $\beta$ = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r M_STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,5]`$). Likewise, $H$ was negatively correlated with the frequency of history-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r STAT.Optim_History_Hazard$coefficients[2,5]`$; mice:  $\beta$ = $`r M_STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r M_STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r M_STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r M_STAT.Optim_History_Hazard$coefficients[2,5]`$).

Our behavioral analyses reveal that humans and mice show significant effects of perceptual history that impaired performance in randomized psychophysical experiments[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] (Figure 2A and 3A). We therefore expected that humans and mice underestimated the true hazard rate $\hat{H}$ of the experimental environments (Confidence database[@Rahnev2020]: $\hat{H}_{Humans}$ = $`r mean((100 - Behav$Stimulus_History)/100, na.rm = TRUE)`$ ± $`r sd((100 - Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(Behav)`$); IBL database[@IBL2021]: $\hat{H}_{Mice}$ = $`r mean((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)`$ ± $`r sd((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(M_Behav)`$). Indeed, when fitting the bimodal inference model to the trial-wise perceptual choices, we found that the estimated (i.e., subjective) hazard rate $H$ was lower than $\hat{H}$ for both humans ($\beta$ = $`r STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r STAT.Optim_Underestimation$coefficients[1,5]`$) and mice ($\beta$ = $`r M_STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r M_STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r M_STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r M_STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r M_STAT.Optim_Underestimation$coefficients[1,5]`$).

To further probe the validity of the bimodal inference model, we asked whether posterior model quantities could explain aspects of the behavioral data that the model was not fitted to. We predicted that the posterior decision variable $L_t$ not only encodes perceptual choices (i.e., the variable used for model estimation), but also predicts the speed of response and subjective confidence[@Kepecs2008; @Braun2018]. Indeed, the estimated trial-wise posterior decision certainty $|L_t|$ correlated negatively with RTs in humans ($\beta$ = $`r STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_RT_Certainty$coefficients[2,5]`$) and TDs mice ($\beta$ = $`r M_STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r M_STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r M_STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r M_STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r M_STAT.Optim_RT_Certainty$coefficients[2,5]`$). Likewise, subjective confidence reports were positively correlated with the estimated posterior decision certainty in humans ($\beta$ = $`r STAT.Optim_Confidence_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_Confidence_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_Confidence_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_Confidence_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_Confidence_Certainty$coefficients[2,5]`$). 

The dynamic accumulation of information inherent to our model entails that biases toward perceptual history are stronger when the posterior decision certainty at the preceding trial is high[@Glaze2015; @Braun2018; @Bergen2019]. Due to the link between posterior decision certainty and confidence, confident perceptual choices should be more likely to induce history-congruent perception at the subsequent trial[@Braun2018; @Bergen2019]. In line with our prediction, logistic regression indicated that history-congruence was predicted by the posterior decision certainty $|L_{t-1}|$ extracted from the model (humans: $\beta$ = $`r STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_mu_minus_1$coefficients[2,4]`$; mice: $\beta$ = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,4]`$) and the subjective confidence reported by the participants (humans: $\beta$ = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,4]`$) at the preceding trial.

As a third validation of the bimodal inference model, we used the posterior model parameters to simulate synthetic perceptual choices and repeated the behavioral analyses conducted for the empirical data. Simulations from the bimodal inference model closely replicated our empirical results: Simulated perceptual decisions resulted from a competition of perceptual history with incoming sensory signals (Figure 4A). Stimulus- and history-congruence were significantly autocorrelated (Figure 4B-C), fluctuating in anti-phase as a scale-invariant process with a 1/f power law (Figure 4D-F). Simulated posterior certainty[@Kepecs2008; @Urai2017; @Braun2018] (i.e., the absolute of the log posterior ratio $|L_t|$) showed a quadratic relationship to the mode of sensory processing (Figure 4H), mirroring the relation of RTs and confidence reports to external and internal biases in perception (Figure 2G-H and Figure 3G-H). Crucially, the overlap between empirical and simulated data broke down when we removed the anti-phase oscillations or the accumulation of evidence over time from the bimodal inference model (Supplemental Figures S10-13).

In sum, computational modeling suggested that between-mode fluctuations are best explained by two interlinked processes (Figure 1E and F): (i), the dynamic accumulation of information across successive trials mandated by normative Bayesian models of evidence accumulation and, (ii), ongoing anti-phase oscillations in the impact of external and internal information.

# Discussion

This work investigates the behavioral and computational characteristics of ongoing fluctuations in perceptual decision-making using two large-scale datasets in humans[@Rahnev2020] and mice[@IBL2021]. We found that humans and mice cycle through recurring intervals of reduced sensitivity to external sensory information, during which they rely more strongly on perceptual history, i.e., an internal prediction that is provided by the sequence of preceding choices. Computational modeling indicated that these slow periodicities are governed by two interlinked factors: (i), the dynamic integration of sensory inputs over time and, (ii), anti-phase oscillations in the strength at which perception is driven by internal versus external sources of information. These cross-species results suggest that ongoing fluctuations in perceptual decision-making arise not merely as a noise-related epiphenomenon of limited processing capacity, but result from a structured and adaptive mechanism that fluctuates between internally- and externally-oriented modes of sensory analysis.

## Bimodal inference represents a pervasive aspect of perceptual decision-making in humans and mice

A growing body of literature has highlighted that perception is modulated by preceding choices[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Urai2019; @Hsu2020; @Braun2018]. Our work provides converging cross-species evidence supporting the notion that such serial dependencies are a pervasive and general phenomenon of perceptual decision-making (Figures 2 and 3). While introducing errors in randomized psychophysical designs[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] (Figures 2A and 3A), we found that perceptual history facilitates post-perceptual processes such as speed of response[@Cicchini2018] (Figure 2G and 3G) and subjective confidence in humans (Figure 2I). 

At the level of individual traits, increased biases toward preceding choices were associated with reduced sensitivity to external information (Supplemental Figure 1C-D) and lower metacognitive efficiency. When investigating how serial dependencies evolve over time, we observed dynamic changes in the strength of perceptual history (Figures 2 and 3B) that created wavering biases toward internally- and externally-biased modes of sensory processing. Between-mode fluctuations may thus provide a new explanation for ongoing changes in perceptual performance[@Verplanck1952; @Atkinson1963; @Dehaene1993; @Gilden1995; @Gilden1995a; @Monto2008]. 

In computational terms, serial dependencies may leverage the temporal autocorrelation of natural environments[@St.John-Saaltink2016; @Bergen2019] to increase the efficiency of decision-making[@Burr2014; @Kiyonaga2017]. Such temporal smoothing[@St.John-Saaltink2016] of sensory inputs may be achieved by updating dynamic predictions about the world based on the sequence of noisy perceptual experiences[@fischer_serial_2014; @Bergen2019], using algorithms based on sequential Bayes[@Cicchini2014; @Glaze2015; @Cicchini2018] such as Kalman[@Burr2014] or Hierarchical Gaussian filtering[@Mathys2014a]. At the level of neural mechanisms, the integration of internal with external information may be realized by combining feedback from higher levels in the cortical hierarchy with incoming sensory signals that are fed forward from lower levels[@Sterzer2018]. 

Yet relying too strongly on serial dependencies may come at a cost: When accumulating over time, internal predictions may eventually override external information, leading to circular and false inferences about the state of the environment[@Jardri2017]. Akin to the wake-sleep-algorithm in machine learning[@Bengio2015], bimodal inference may help to determine whether errors result from external input or from internally-stored predictions: During internal mode, sensory processing is more strongly constrained by predictive processes that auto-encode the agent's environment. Conversely, during external mode, the network is driven predominantly by sensory inputs[@Honey2017]. Between-mode fluctuations may thus generate an unambiguous error signal that aligns internal predictions with the current state of the environment in iterative test-update-cycles[@Bengio2015]. On a broader scale, between-mode fluctuations may thus regulate the balance between feedforward versus feedback contributions to perception and thereby play a adaptive role in metacognition and reality monitoring[@Dijkstra2021].

We hypothesized that observers have certain hyperpriors that are apt for accommodating fluctuations in the predictability of their environment, i.e., people believe that their world is inherently volatile. To be Bayes optimal, it is therefore necessary to periodically re-evaluate posterior beliefs about the parameters that define an internal generative model of the external sensory environment. One way to do this is to periodically suspend the precision of prior beliefs and increase the precision afforded to sensory evidence, thus updating Bayesian beliefs about model parameters.

The empirical evidence above suggests that the timescale of this periodic scheduling of evidence accumulation may be scale-invariant. This means that there may exist a timescale of periodic fluctuations in precision over every window or length of perceptual decision-making. Bimodal inference predicts perceptual decisions under a generative model (based upon a hazard function to model serial dependencies between subsequent trials) with periodic fluctuations in the precision of sensory evidence relative to prior beliefs at a particular timescale. Remarkably, a systematic model comparison based on AIC indicated that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. This ad-hoc addition of oscillations to a normative Bayesian model of evidence accumulation[@Glaze2015] allowed us to quantify the dominant timescale of periodic fluctuations mode at approximately 0.11 1/$N_{trials}$ in humans and mice that is appropriate for these kinds of paradigms.

## Bimodal inference versus normative Bayesian evidence accumulation

Could bimodal inference emerge spontaneously in normative models of perceptual decision-making? In predictive processing, the relative precision of prior and likelihood determines their integration into the posterior that determines the content of perception. At the level of individual trials, the perceptual impact of internal predictions generated from perceptual history (prior precision) and external sensory information (likelihood precision) are thus necessarily anti-correlated. The same holds for mechanistic models of drift diffusion, which understand choice history biases as driven by changes in the starting point[@Glaze2015] or the drift rate of evidence accumulation[@Urai2019]. Under the former formulation, perceptual history is bound to have a stronger influence on perception when less weight is given to incoming sensory evidence, assuming that the last choice is represented as a starting point bias. The effects of choice history in normative Bayesian and mechanistic drift diffusion models can be mapped onto one another via the Bayesian formulation of drift diffusion[@Bitzer2014], where the inverse of likelihood precision determines the amount of noise in the accumulation of new evidence, and prior precision determines the absolute shift in its starting point[@Bitzer2014].

While it is thus clear that the impact of perceptual history and sensory evidence are anti-correlated *at each individual trial*, we here introduce anti-phase oscillations as an ad-hoc modification to model slow fluctuations in prior and likelihood precision that evolve *over many consecutive trials* and are not mandated by normative Bayesian or mechanistic drift diffusion models. The bimodal inference model provides a reasonable explanation of the linked autocorrelations in stimulus- and history-congruence, as evidenced by formal model comparison, successful prediction of RTs and confidence as out-of-training variables, and a qualitative reproduction of our empirical data from posterior model parameter as evidence against over- or under-fitting. 

Of note, similar non-stationarities have been observed in descriptive models that assume continuous[@Roy2021] or discrete[@Ashwood2022] changes in the latent states that modulate perceptual decision-making at slow timescales. A recent computational study[@Ashwood2021] has used a Hidden Markov model to investigate perceptual decision-making in the IBL database[@IBL2021]. In analogy to our findings, the authors observed that mice switch between temporally extended *strategies* that last for more than 100 trials: During *engaged* states, perception was highly sensitive to external sensory information. During *disengaged* states, in turn, choice behavior was prone to errors due to enhanced biases toward one of the two perceptual outcomes[@Ashwood2021]. Despite the conceptual differences to our approach (discrete states in a Hidden Markov model that correspond to switches between distinct decision-making strategies[@Ashwood2021] vs. gradual changes in mode that emerge from sequential Bayesian inference and ongoing oscillations in the impact of external relative to internal information), it is tempting to speculate that engaged/disengaged states and between-mode fluctuations might tap into the same underlying phenomenon.

## Task engagement and residual motor activation as alternative explanations for bimodal inference

As a functional explanation for bimodal inference, we propose that perception temporarily disengages from internal predictions to form stable inferences about the statistical properties of the sensory environment. Between-mode fluctuations may thus elude circular inferences that occur when both the causes and the encoding of sensory stimuli are volatile[@Jardri2017; @Weilnhammer2021a]. By the same token, we suggest that fluctuations in mode occur at the level of perceptual processing[@St.John-Saaltink2016; @Cicchini2017; @Braun2018; @Cicchini2021], and are not a passive phenomenon that is primarily driven by factors situated up- or downstream of sensory analysis.

How does attention relate to phenomenon of between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. From this perspective, fluctuations between external and internal mode can be understood as ongoing shifts in the attention afforded to either external sensory information (regulated via likelihood precision) or internal predictions (regulated via prior precision). When the precision of either likelihood or prior increases, posterior precision increases, which leads to faster RTs and higher confidence. Therefore, when defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may provide a plausible explanation for the quadratic relationship of mode to RTs and confidence (Figure 2H and J; Figure 3I, Figure 4I).

Outside of the predictive processing field, attention is often understood in the context of task engagement[@Matthews2010], which varies according to the availability of cognitive resources that are modulated by factors such as tonic arousal, familiarity with the task, or fatigue[@Matthews2010]. Our results suggest that internal mode processing cannot be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, choices during internal mode were not random or globally biased, but driven by perceptual history (Supplemental Section). Moreover, our computational model identified the dominant timescale of between-mode fluctuations at 0.11 1/$N_{trials}$, which may be compatible with fluctuations in arousal[@McGinley2015], but is faster than to be expected for the development of task familiarity or fatigue. 

However, in interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects on task performance[@Beerendonk2023]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. 

Of note, high phasic arousal has been shown to suppress biases in decision-making in humans and mice across domains[@Gee2014; @Gee2017; @Gee2020], including biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. While the increase in response speed and history congruence over time (Supplemental Section 9.4) may argue against insufficient training as an alternative explanation for internal mode processing, it may also be indicative of waning arousal. The multiple mechanistic mappings to RTs and confidence warrant more direct measures of arousal (such as pupil size[@Gee2014; @McGinley2015b; @Urai2017; @Gee2020; @Gee2022; @Beerendonk2023], motor behavior[@McGinley2015b; @Gee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.

Residual activation of the motor system may provide another contribution to serial biases in perceptual choices[@Mawase2018]. Such motor-driven priming may lead to errors in randomized psychophysical designs, resembling the phenomenon that we identify as internally-biased processing[@Pomper2023]. Moreover, residual activation of the motor system may lead to faster responses, and thus constitutes an alternative explanation for the quadratic relationship of mode with RTs[@Mawase2018]. The observation of elevated confidence for stronger biases toward internal mode speaks against the proposition that residual activation of the motor system is the primary driver of serial choice biases, since strong motor-driven priming should lead to frequent lapses that are typically associated reduced confidence[@Kepecs2012]. Likewise, perceptual history effects have repeatedly been replicated in experiments with counter-balanced stimulus-response mappings[@Braun2018: Feigin2021].

No-response paradigms, in which perceptual decision are inferred from eye-movements alone, could help to better differentiate perceptual from motor-related effects. Likewise, video-tracking of response behavior and neural recording from motor- and premotor, which has recently been released for the IBL database[IBL2023], may provide further insight into the relation of motor behavior to the perceptual phenomenon of between-mode fluctuations. 

## Limitations and open questions

Our results suggest bimodal inference as a pervasive aspect of perceptual decision-making in humans and mice. However, a number of limitations and open questions have to be considered: 

First, this work sought to understand whether fluctuations between internal and external mode, which we initially observed in an experiment on bistable perception in humans[@Weilnhammer2021a], represent a general phenomenon that occurs across a diverse set of perceptual decision-making tasks. Our analysis of the Confidence database[@Rahnev2020] therefore collapsed across all available experiments on binary perceptual decision-making. Individual experiments differed with respect to the stimuli, the manipulation of difficulty, the timing of trials, and the way responses were collected, but were highly comparable with respect to the central variables of stimulus- and history-congruence (Supplemental Figure S1A-B). 

The variability across experiments, which we considered as random effects in all statistical analyses, enabled us to assess whether bimodal inference represents a general phenomenon in perceptual decision-making, but limited the precision at which we were able to investigate the relation of mode to behavioral variables such as timing, task difficulty, RT or confidence. This issue is partially resolved by our analyses of the IBL database, which replicated our findings in an experiment that was highly standardized with respect to timing, task difficulty, and behavioral read-out[@IBL2021]. It will be an important task for future research to validate our results on bimodal inference in a standardized dataset of comparable volume in humans, which is, to our knowledge, not yet available.

Second, our results point to an attraction of perception toward preceding choices. Previous work has shown that perceptual decision-making is concurrently affected by both attractive and repulsive serial biases that operate on distinct time-scales and serve complementary functions for sensory processing[@Fritsche2017; @Gekas2019; @Fritsche2020]: Short-term attraction may serve the decoding of noisy sensory inputs and increase the stability of perception, whereas long-term repulsion may enable efficient encoding and sensitivity to change[@Fritsche2020]. In the data analyzed here, history biases tended to be repetitive (Figure 2A, Figure 3A, Supplemental Figure S6 and S7), and only `r sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)` of the `r nrow(Study_Behav)` experiments of the Confidence database[@Rahnev2020] showed significant alternating biases (Supplemental Figure S1). However, as we show in Supplemental Figure S14, fluctuations in both alternating and repeating history biases generate overlapping autocorrelation curves. Our analysis of between-mode fluctuations is therefore not tied exclusively to repeating biases, but accommodates alternating biases as well, such that both may lead to internally-biased processing and reduced sensitivity to external sensory information. Future work could apply our approach to paradigms that boost alternating as opposed to repeating biases, as this would help to better understand how repetition and alternation are linked in terms of their computational function and neural implementation[@Fritsche2020].

A third open question concerns the computational underpinnings of bimodal inference. The addition of slow anti-phase oscillations to the integration of prior and likelihood represents an ad-hoc modification of a normative Bayesian model of evidence accumulation[@Glaze2015]. While the bimodal inference model is supported by formal model comparison, the successful prediction of out-of-training variables and the qualitative reproduction of our empirical data in simulations from posterior model parameters, it is an important task for future research to test (i), whether between-mode fluctuations can emerge spontaneously in hierarchical models of Bayesian inference, (ii), whether modes are continuous[@Weilnhammer2021a] or discrete[@Ashwood2021], and (iii), whether bimodal inference can be causally manipulated by experimental variables. We speculate that between-mode fluctuations may separate the perceptual contribution of internal predictions and external sensory data in time, creating unambiguous learning signals that benefit inference about the precision of prior and likelihood, respectively. This proposition should be tested empirically by relating the phenomenon of bimodal inference to performance in, e.g., reversal learning, probabilistic reasoning, or metacognition. 

A final important avenue for further research on bimodal inference is to elucidate its neurobiological underpinnings. Since between-mode fluctuations were found in humans and mice, future studies can apply non-invasive and invasive neuro-imaging and electrophysiology to better understand the neural mechanisms that generate ongoing changes in mode in terms of their neuro-anatomy, -chemistry and -circuitry. 

Establishing the neural correlates of externally- an internally-biased modes will enable exiting opportunities to investigate their role for adaptive perception and decision-making: Causal interventions via pharmacological challenges, optogenetic manipulations or (non-)invasive brain stimulation will help to understand whether between-mode fluctuations are implicated in resolving credit-assignment problems[@Honey2017; @Weilnhammer2020] or in calibrating metacognition and reality monitoring[@Dijkstra2021]. Answers to these questions may provide new insights into the pathophysiology of hallucinations and delusions, which have been characterized by an imbalance in the impact of external versus internal information[@Fletcher2009; @Sterzer2018; @Corlett2019a] and are typically associated with metacognitive failures and a departure from consensual reality[@Corlett2019a]. 

\newpage
# Methods

## Resource availability

### Lead contact

Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Veith Weilnhammer (veith.weilnhammer@gmail.com).

### Materials availability

This study did not generate new unique reagents.

### Data and code availability

All custom code and behavioral data are available on <https://github.com/veithweilnhammer/Modes>. This manuscript was created using the *R Markdown* framework, which integrates all data-related computations and the formatted text within one document. With this, we wish to make our approach fully transparent and reproducible for reviewers and future readers.

## Experimental model and subject details

```{r Dataset_Information, cache = TRUE}
load("./Summary_Data/Experiment_information.Rdata")
```

### Confidence database
We downloaded the human data from the Confidence database[@Rahnev2020] on 10/21/2020, limiting our analyses to the category *perception*. Within this category, we selected studies in which participants made binary perceptual decisions between two alternatives. We excluded two experiments in which the average perceptual accuracy fell below 50%. After excluding these experiments, our sample consisted of $`r n_trials_human/100000`$ million trials obtained from `r n_participants_human` human participants and `r n_studies_human` individual experiments (Supplemental Table 1). Out of the `r n_studies_human` included experiments, `r sum(Exp_rep$domain == "Vision")` investigated visual, `r sum(Exp_rep$domain == "Auditory")` auditory, `r sum(Exp_rep$domain == "Vestibular self-motion")` proprioceptive, and `r sum(Exp_rep$domain == "Multimodal")` multimodal perception. `r sum(Exp_rep$Decision_type == "Discrimination")` experiments were based on discrimination and `r sum(Exp_rep$Decision_type == "Detection")` on detection, with one investigating both.

### IBL database
We downloaded the data from the IBL database[@IBL2021] on 04/28/2021. We limited our analyses to the *basic task*, during which mice responded to gratings that appeared with equal probability in the left or right hemifield. Within each mouse, we excluded sessions in which perceptual accuracy was below 80% for stimuli presented at a contrast $\geq$ 50%. After exclusion, our sample consisted of $`r n_trials_mice/1000000`$ million trials trials obtained from N = `r n_participants_mice` mice.

## Method details

### Variables of interest

**Primary variables of interest:** We extracted trial-wise data on the presented stimulus and the associated perceptual decision. Stimulus-congruent choices were defined by perceptual decisions that matched the presented stimuli. History-congruent choices were defined by perceptual choices that matched the perceptual choice at the immediately preceding trial. The dynamic probabilities of stimulus- and history-congruence were computed in sliding windows of ±`r sliding_window/2` trials.

The *mode* of sensory processing was derived by subtracting the dynamic probability of history-congruence from the dynamic probability of stimulus-congruence, such that positive values indicate externally-oriented processing, whereas negative values indicate internally-oriented processing. When visualizing the relation of the mode of sensory processing to confidence, RTs or trial duration (see below), we binned the mode variable in 10% intervals. We excluded bins that contained less than 0.5% of the total number of available data-points.

**Secondary variables of interest**: From the Confidence Database[@Rahnev2020], we furthermore extracted trial-wise confidence reports and RTs. Out of the `r sum(Exp_rep$RT_available == 1)` experiments that provide information on RTs, `r sum(Exp_rep$RT_type == "cued")` cued the response by the onset of a response screen or an additional response cue, whereas `r sum(Exp_rep$RT_type == "free")` allowed participants to response at any time after stimulus onset.  If RTs were available for both the perceptual decision and the confidence report, we only extracted the RT associated with the perceptual decision. To enable comparability between studies, we normalized RTs and confidence reports within individual studies using the *scale* R function. If not available for a particular study, RTs were treated as missing variables.
From the IBL database[@IBL2021], we extracted trial durations (TDs) as defined by interval between stimulus onset and feedback, which represents a coarse measure of RT[@IBL2021]. 

**Exclusion criteria for individual data-points:** For non-normalized data (TDs from the IBL database[@IBL2021]; d-prime, meta-dprime and M-ratio from the Confidence database[@Rahnev2020] and simulated confidence reports), we excluded data-points that differed from the median by more than 3 x MAD (median absolute distance[@Leys2013]). For normalized data (RTs and confidence reports from the Confidence database[@Rahnev2020]), we excluded data-points that differed from the mean by more than 3 x SD (standard deviation). 

### Control variables

Next to the sequence of presented stimuli, we assessed the autocorrelation of task difficulty as an alternative explanation for any autocorrelation in stimulus- and history-congruence. In the Confidence Database[@Rahnev2020], `r sum(Exp_rep$Difficulty_available == 0)` of the `r nrow(Exp_rep)` included experiments used fixed difficulty levels, whereas `r sum(Exp_rep$Difficulty_available == 1)` manipulated difficulty levels within participants. Difficulty was manipulated via noise masks, contrast, luminance, presentation time, or stimulus probability for gabors, dot coherence for random dot kinematograms, difference in elements and set size for comparisons of numerosity, difference in clicks for auditory discrimination, temporal distance for meta-contrast masking, and amount of self-motion for proprioception. We treated task difficulty as a missing variable for the experiments that fixed it at the participant-level, as this precluded the computation of autocorrelation curves. In analogy to RTs and confidence, difficulty levels were normalized within individual studies. For the IBL Database[@IBL2021], task difficulty was defined by the contrast of the presented grating.

### Autocorrelations

For each participant, trial-wise autocorrelation coefficients were estimated using the R-function *acf* with a maximum lag defined by the number of trials available per subject. Autocorrelation coefficients are displayed against the lag (in numbers of trials, ranging from 1 to 20) relative to the index trial (t = 0; Figure 2B-C, 3B-C and 4B-C). To account for spurious autocorrelations that occur due to imbalances in the analyzed variables, we estimated autocorrelations for randomly permuted data (100 iterations). For group-level autocorrelations, we computed the differences between the true autocorrelation coefficients and the mean autocorrelation observed for randomly permuted data and averaged across participants. 

At a given trial, group-level autocorrelation coefficients were considered significant when linear mixed effects modeling indicated that the difference between real and permuted autocorrelation coefficients was above zero at an alpha level of 0.05%. To test whether the autocorrelation of stimulus- and history-congruence remained significant when controlling for task difficulty and the sequence of presented stimuli, we added the respective autocorrelation as an additional factor to the linear mixed effects model that computed the group-level statistics (see also *Mixed effects modeling*).

To assess autocorrelations at the level of individual participants, we counted the number of subsequent trials (starting at the first trial after the index trial) for which less than 50% of the  permuted autocorrelation coefficients exceeded the true autocorrelation coefficient. For example, a count of zero indicates that the true autocorrelation coefficients exceeded *less than 50%* of the autocorrelation coefficients computed for randomly permuted data at the first trial following the index trial. A count of five indicates that, for the first five trials following the index trial, the true autocorrelation coefficients exceeded *more than 50%* of the respective autocorrelation coefficients for the randomly permuted data; at the 6th trial following the index trial, however, *less than 50%* of the autocorrelation coefficients exceeded the respective permuted autocorrelation coefficients. 

### Spectral analysis

We used the R function *spectrum* to compute the spectral densities for the dynamic probabilities of stimulus- and history-congruence as well as the phase (i.e., frequency-specific shift between the two time-series ranging from 0 to $2*\pi$) and squared coherence (frequency-specific variable that denotes the degree to which the shift between the two time-series in constant, ranging from 0 to 100%). Periodograms were smoothed using modified Daniell smoothers at a width of 50. 

Since the dynamic probabilities of history- and stimulus-congruence were computed using a sliding windows of ±`r sliding_window/2` trials (i.e., intervals containing a total of `r sliding_window + 1` trials), we report the spectral density, coherence and phase for frequencies below  1/`r sliding_window + 1` 1/$N_{trials}$. Spectral densities have one value per subject and frequency (data shown in Figures 2D and 3D). To assess the relation between stimulus- and history-congruence in this frequency range, we report average phase and average squared coherence for all frequencies below 1/`r sliding_window + 1` 1/$N_{trials}$ (i.e., one value per subject; data shown in Figure 2E-F and 3E-F).

Since the data extracted from the Confidence Database[@Rahnev2020] consist of a large set of individual studies that differ with respect to inter-trial intervals, we defined the variable *frequency* in the dimension of cycles per trial 1/$N_{trials}$ rather than cycles per second (Hz). For consistency, we chose 1/$N_{trials}$ as the unit of frequency for the IBL database[@IBL2021] as well.

## Quantification and statistical procedures

All aggregate data are reported and displayed with errorbars as mean ± standard error of the mean. 

### Mixed effects modeling

Unless indicated otherwise, we performed group-level inference using the R-packages *lmer* and *afex* for linear mixed effects modeling and *glmer* with a binomial link-function for logistic regression. We compared models based on AIC. To account for variability between the studies available from the Confidence Database[@Rahnev2020], mixed modeling was conducted using random intercepts defined for each study. To account for variability across experimental session within the IBL database[@IBL2021], mixed modeling was conducted using random intercepts defined for each individual session. When multiple within-participant datapoints were analyzed, we estimated random intercepts for each participant that were *nested* within the respective study of the Confidence database[@Rahnev2020]. By analogy, for the IBL database[@IBL2021], we estimated random intercepts for each session that were nested within the respective mouse. We report $\beta$ values referring to the estimates provided by mixed effects modeling, followed by the respective T statistic (linear models) or z statistic (logistic models).  

The effects of stimulus- and history-congruence on RTs and confidence reports (Figure 2, 3 and 4, subpanels G-I) were assessed in linear mixed effects models that tested for main effects of both stimulus- and history-congruence as well as the between-factor interaction. Thus, the significance of any effect of history-congruence on RTs and confidence reports was assessed while controlling for the respective effect of stimulus-congruence (and vice versa).

### Psychometric function

We obtained psychometric curves by fitting the following error function to the behavioral data:

\begin{equation}
y_p = \gamma + (1 - \gamma - \delta) *  (erf(\frac{s_w + \mu}{t}) + 1) / 2
\end{equation}

We used the Broyden–Fletcher–Goldfarb–Shanno algorithm in maximum likelihood estimation[@Nash2011] to predict individual choices $y$ (outcome A: $y = 0$; outcome B: $y = 1$) from the choice probability $y_p$. In humans, we computed $s_w$ by multiplying the inputs $s$ (stimulus A: 0; outcome B: 1) with the task difficulty $D_b$ (binarized across 7 levels):

\begin{equation}
s_w = (s - 0.5) * D_b
\end{equation}

In mice, $s_w$ was defined by the respective stimulus contrast in the two hemifields:

\begin{equation}
s_w = Contrast_{Right} - Contrast_{Left}
\end{equation}

Parameters of the psychometric error function were fitted using the R-package *optimx*[@Nash2011]. The psychometric error function was defined via the parameters $\gamma$ (lower lapse; lower bound = 0, upper bound = 0.5), $\delta$ (upper lapse; lower bound = 0, upper bound = 0.5), $\mu$ (bias; lower bound humans = -5; upper bound humans = 5, lower bound mice = -0.5, upper bound mice = 0.5) and threshold $t$ (lower bound humans = 0.5, upper bound humans = 25; lower bound mice = 0.01, upper bound mice = 1.5).

### Computational modeling

**Model definition**: Our modeling analysis is an extension of a model proposed by Glaze et al.[@Glaze2015], who defined a normative account of evidence accumulation for decision-making. In this model, trial-wise choices are explained by applying Bayes theorem to infer moment-by-moment changes in the state of environment from trial-wise noisy observations across trials.

Following Glaze et al.[@Glaze2015], we applied Bayes rule to compute the posterior evidence for the two alternative choices (i.e., the log posterior ratio $L$) from the sensory evidence available at time-point $t$ (i.e., the log likelihood ratio $LLR$) with the prior probability $\psi$, weighted by the respective precision terms $\omega_{LLR}$ and $\omega_{\psi}$:

\begin{equation}
L_t = LLR_t * \omega_{LLR} + \psi_t(L_{t-1}, H) * \omega_{\psi}
\end{equation}

In the trial-wise design studied here, a transition between the two states of the environment (i.e., the sources generating the noisy observations available to the participant) can occur at any time. Despite the random nature of the psychophysical paradigms studied here[@Rahnev2020; @IBL2021], humans and mice showed significant biases toward preceding choices (Figure 2A and 3A). We thus assumed that the prior probability of the two possible outcomes depends on the posterior choice probability at the preceding trial and the hazard rate $H$ assumed by the participant. Following Glaze et al.[@Glaze2015], the prior $\psi$ is thus computed as follows: 

\begin{equation}
\psi_t(L_{t-1}, H)  = L_{t-1} + log(\frac{1-H}{H} + exp(-L_{t-1})) - log(\frac{1-H}{H} + exp(L_{t-1}))
\end{equation}

In this model, humans, mice and simulated agents make perceptual choices based on noisy observations $u$. The are computed by applying a sensitivity parameter $\alpha$ to the content of external sensory information $s$. For humans, we defined the input $s$ by the two alternative states of the environment (stimulus A: $s = 0$; stimulus B: $s = 1$), which generated the observations $u$ through a sigmoid function that applied a sensitivity parameter $\alpha$:

\begin{equation}
u_t = \frac{1}{1+exp(-\alpha*(s_t-0.5))}
\end{equation}

In mice, the inputs $s$ were defined by the respective stimulus contrast in the two hemifields:

\begin{equation}
s_t = Contrast_{Right} - Contrast_{Left}
\end{equation}

As in humans, we derived the input $u$ by applying a sigmoid function with a sensitivity parameter $\alpha$ to input $s$: 

\begin{equation}
u_t = \frac{1}{1 + exp(-\alpha * s_t)}
\end{equation}

For humans, mice and in simulations, the log likelihood ratio $LLR$ was computed from $u$ as follows: 

\begin{equation}
LLR_t = log(\frac{u_t}{1-u_t})
\end{equation}

To allow for long-range autocorrelation in stimulus- and history-congruence (Figure 2B and 3B), our modeling approach differed from Glaze et al.[@Glaze2015] in that it allowed for systematic fluctuation in the impact of sensory information (i.e., $LLR$) and the prior probability of choices $\psi$ on the posterior probability $L$. This was achieved by multiplying the log likelihood ratio and the log prior ratio with coherent anti-phase fluctuations according to $\omega_{LLR} = a_{LLR} * sin(f * t + phase) + 1$ and $\omega_{\psi} = a_{\psi} * sin(f * t + phase + \pi) + 1$.

**Model fitting**: In model fitting, we predicted the trial-wise choices $y_t$ (option A: 0; option B: 1) from inputs $s$. To this end, we minimized the log loss between $y_t$ and the choice probability $y_{p t}$ in the unit interval. $y_{p t}$ was derived from $L_t$ using a sigmoid function defined by the inverse decision temperature $\zeta$: 

\begin{equation}
y_{p t} = \frac{1}{1 + exp(-\zeta * L_t)}
\end{equation}

This allowed us to infer the free parameters $H$ (lower bound = 0, upper bound = 1; human posterior = $`r mean(Optim_Behav$Hazard, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$Hazard, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$Hazard, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$Hazard, na.rm = TRUE)/nrow(Optim_M_Behav)`$),
$\alpha$ (lower bound = 0, upper bound = 5; human posterior = $`r mean(Optim_Behav$Precision, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$Precision, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$Precision, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$Precision, na.rm = TRUE)/nrow(Optim_M_Behav)`$), 
$a_{\psi}$ (lower bound = 0, upper bound = 10; human posterior = $`r mean(Optim_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_M_Behav)`$), $amp_{LLR}$ (lower bound = 0, upper bound = 10; human posterior = $`r mean(Optim_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_M_Behav)`$),
frequency $f$ (lower bound = 1/40, upper bound = 1/5; human posterior = $`r mean(Optim_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$frequency, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$frequency, na.rm = TRUE)/nrow(Optim_M_Behav)`$), 
$p$ (lower bound = 0, upper bound = $2*\pi$; human posterior = $`r mean(Optim_Behav$phase, na.rm = TRUE)` ± `r sd(Optim_Behav$phase, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$phase, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$phase, na.rm = TRUE)/nrow(Optim_M_Behav)`$) 
and inverse decision temperature $\zeta$ (lower bound = 1, upper bound = 10; human posterior = $`r mean(Optim_Behav$zeta, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$zeta, na.rm = TRUE)/nrow(Optim_Behav)`$; mouse posterior = $`r mean(Optim_M_Behav$zeta, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$zeta, na.rm = TRUE)/nrow(Optim_M_Behav)`$) using maximum likelihood estimation with the Broyden–Fletcher–Goldfarb–Shanno algorithm as implemented in the R-function *optimx*[@Nash2011] (see Supplemental Table T2 for a description of our model parameters).

We validated the bimodal inference model in three steps: a formal model comparison to reduced models based on AIC (Figure 1F-G; Supplemental Figure S9), the prediction of within-training (stimulus- and history-congruence) as well as out-of-training variables (RT and confidence), and a qualitative reproduction of the empirical data from model simulations based on estimated parameters (Figure 4). 

**Model comparison.** We assessed the following model space based on AIC:

- The full *bimodal inference model* (M1; Figure 1F) incorporates the influence of sensory information according to the parameter $\alpha$ (likelihood); the integration of evidence across trials according to the parameter $H$ (prior); anti-phase oscillations in between likelihood and prior precision according to $\omega_{LLR}$ and $\omega_{\psi}$ with parameters $a_{LLR}$ (amplitude likelihood fluctuation), $a_{\psi}$ (amplitude prior fluctuation), $f$ (frequency) and $p$ (phase). 

- The *likelihood-oscillation-only model* (M2; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to parameter $H$ (prior); oscillations in likelihood precision according to $\omega_{LLR}$ with parameters $a_{LLR}$ (amplitude likelihood fluctuation), $f$ (frequency) and $p$ (phase).

- The *prior-oscillation-only model* (M3; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to parameter $H$ (prior); oscillations in the prior precision according to $\omega_{\psi}$ with parameters $a_{\psi}$ (amplitude prior fluctuation), $f$ (frequency) and $p$ (phase). Please note that all models M1-3 lead to shifts in the relative precision of likelihood and prior. 

- The *normative-evidence-accumulation* (M4; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to parameter $H$ (prior), There are no additional oscillations. Model M4 thus corresponds to the model proposed by Glaze et al. and captures normative evidence accumulation in unpredictable environments using a Bayesian update scheme[@Glaze2015]. The comparison against M4 tests the null hypothesis that fluctuations in mode emerge from a normative Bayesian model without the ad-hoc addition of oscillations as in models M1-3.

- The *no-evidence-accumulation* (M5; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood). The model lacks integration of evidence across trials (flat prior) and oscillations. The comparison against M5 tests the null hypothesis that observers do not use prior information derived from serial dependency in perception. 

**Prediction of within-training and out-of-training variables.** To validate our model, we correlated individual posterior parameter estimates with the respective conventional variables. As a sanity check, we tested (i), whether the estimated hazard rate $H$ correlated negatively with the frequency of history-congruent choices and, (ii), whether the estimated sensitivity to sensory information $\alpha$ correlated positively with the frequency of stimulus-congruent choices. In addition, we tested whether the posterior decision certainty (i.e.. the absolute of the log posterior ratio) correlated negatively with RTs and positively with confidence. This allowed us to assess whether our model could explain aspects of the data it was not fitted to (i.e., RTs and confidence). 

**Simulations.** Finally, we used simulations (Figure 4, Supplemental Figures S10-13) to show that all model components, including the anti-phase oscillations governed by $a_{\psi}$, $a_{LLR}$, $f$ and $p$, were necessary for our model to reproduce the characteristics of the empirical data. This enabled us to assess over- or under-fitting in the bimodal inference model and all reduced models M2-M5. We used the posterior model parameters observed for humans ($H$, $\alpha$, $a_{\psi}$, $a_{LLR}$, $f$, $p$ and $\zeta$) to define individual parameters for simulation in `r nrow(Optim_Behav)` simulated participants (i.e., equivalent to the number of human participants). For each participant, the number of simulated trials was drawn at random between 300 to 700. Inputs $s$ were drawn at random for each trial, such that the sequence of inputs to the simulation did not contain any systematic seriality. Noisy observations $u$ were generated by applying the posterior parameter $\alpha$ to inputs $s$, thus generating stimulus-congruent choices in $`r mean(Sim_Behav$Accuracy, na.rm = TRUE)`$ ± $`r sd(Sim_Behav$Accuracy, na.rm = TRUE)/nrow(Sim_Behav)`$% of trials. Choices were simulated based on the trial-wise choice probabilities $y_{p}$ obtained from our model. Simulated data were analyzed in analogy to the human and mouse data. As a substitute of subjective confidence, we computed the absolute of the trial-wise log posterior ratio $|L|$ (i.e., the posterior decision certainty).

\newpage
# Figures

## Figure 1
```{r Figure_1}
##
## Timecourse plot
##
Individual_TC_long <- read.csv("./Results/Individual_TC_long.csv")
  
p_Individual_TC_0 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Input" | Individual_TC_long$Slider == "Response",],
aes(x = Trial, y = Variable, linetype = Slider),
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"Congruency", linetype = "", subtitle = "A") + xlim(0, 300) +
theme(legend.position = "none") + scale_y_continuous(name="Stimulus", breaks = c(0,1), labels = c("B", "A"), limits=c(-0.1, 1.1))

p_Individual_TC_1 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Stimulus" | Individual_TC_long$Slider == "History",],
aes(x = Trial, y = Variable, color = Slider),
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"Congruency", color = "Congruence to", subtitle = "B") + xlim(0, 300) +
theme(legend.position = "none") + scale_y_continuous(name="Congruent", breaks = c(0,1), labels = c("no", "yes"), limits=c(-0.1, 1.1))


p_Individual_TC_2 <- ggplot() +
geom_line(
data = Individual_TC_long[Individual_TC_long$Slider == "Stimulus_slider" | Individual_TC_long$Slider == "History_slider",],
aes(x = Trial, y = Variable, color = Slider),
size = 0.5,
 alpha = 0.5
) + geom_hline(
yintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) +
scale_color_brewer(palette = "Set1", direction = -1) +
theme_classic(base_size = 6) + labs(x = NULL,  y =
"z(Probability)", subtitle = "C") + xlim(0, 300) +
theme(legend.position = "none")  + scale_y_continuous(name="Probability")

p_Individual_TC_3 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Mode",],
aes(x = Trial, y = Variable, color = Slider),
color = "darkorchid4",
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Set1", direction = 1) +
theme_classic(base_size = 6) + labs(x = "Trial",  y =
"Int.< Mode (%) > Ext.", subtitle = "D") + xlim(0, 300)  +
theme(legend.position = "none") +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

p_Individual_TC_4 <- ggplot() +
geom_step(
data = Individual_TC_long[Individual_TC_long$Slider == "Prior" | Individual_TC_long$Slider == "LLR" | Individual_TC_long$Slider == "Posterior",],
aes(x = Trial, y = Variable, color = Slider),
size = 0.5,
 alpha = 0.5
) +
scale_color_brewer(palette = "Dark2", direction = 1) +
theme_classic(base_size = 6) + labs(x = "Trial",  y =
"Log ratio", subtitle = "E", color = " ") + xlim(0, 300)  +
theme(legend.position = "none") +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

lay <- rbind(c(1), c(2), c(3), c(4), c(5))
grid.arrange(
p_Individual_TC_0,
p_Individual_TC_1,
p_Individual_TC_2,
p_Individual_TC_3,
p_Individual_TC_4,
layout_matrix = lay,
heights = c(0.75,0.75,1,1,1),
widths = c(1))
```

**Figure 1. Concept.** 

A. In binary perceptual decision-making, a participant is presented with stimuli from two categories (A vs. B; dotted line) and reports consecutive perceptual choices via button presses (sold line). All panels below refer to these stimulated example data. 

B. When the response matches the external stimulus information (i.e., overlap between dotted and solid line in panel A), perceptual choices are *stimulus-congruent* (red line). When the response matches the response at the preceding trial, perceptual choices are *history-congruent* (blue line).

C. The dynamic probabilities of stimulus- and history-congruence (i.e., computed in sliding windows of ±`r sliding_window/2` trials) fluctuate over time. 

D. The *mode* of perceptual processing is derived by computing the difference between the dynamic probabilities of stimulus- and history-congruence. Values above 0% indicate a bias toward external information, whereas values below 0% indicate a bias toward internal information. 

E. In computational modeling, internal mode is caused by an enhanced impact of perceptual history. This causes the posterior (black line) to be close to the prior (blue line). Conversely, during external mode, the posterior is close to the sensory information (log likelihood ratio, red line).

F. The bimodal inference model (M1) explains fluctuations between externally- and externally-biased modes (left panel) by two interacting factors: a normative accumulation of evidence according to parameters $H$ (middle panel), and anti-phase oscillations in the precision terms $\omega_{LLR}$ and $\omega_{\psi}$ (right panel). 

G. The control models M2-M5 were constructed by successively removing the anti-phase oscillations and the integration of information from the bimodal inference model. Please note that the normative-evidence-accumulation-model (M4) corresponds to the model proposed by Glaze et al.[@Glaze2015]. In the no-evidence-accumulation model (M5), perceptual decisions depend only on likelihood information (flat priors). 

\newpage
## Figure 2

```{r Figure_2}
##
## Global probability Stimulus- and History-congruence
##


gathercol <- colnames(Behav[,c(3,6)])

Behav_long  <-
gather(Behav[, c(1,2,3,6)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

Behav_long$Frequency <-
gsub("Accuracy", "Stimulus", Behav_long$Frequency)

p_distribution_Behav <- ggplot(Behav_long, aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
   # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
   geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
    geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

##
## Probability of History-congruence depending on perceptual accuracy
##


gathercol <- colnames(Behav_diff[,c(4)])

Behav_diff_long  <-
gather(Behav_diff[, c(1,2,3,4)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

Behav_diff_long$Accuracy <- as.character(Behav_diff_long$Accuracy) 
Behav_diff_long$Accuracy <-
gsub("0", "error", Behav_diff_long$Accuracy)
Behav_diff_long$Accuracy <-
gsub("1", "correct", Behav_diff_long$Accuracy)

p_distribution_Behav_diff <- ggplot(Behav_diff_long, aes(x = History, color = -Accuracy, fill = Accuracy)) + 
   # geom_histogram(aes(y=..density..), colour="white", fill="black", position="ide ntity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
   geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
    geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "top")

##
## Acf Plot
##
Plot_Acf_Data =  Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus" | Summary_acf$Variable == "diff_acf_History" ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

y_min = -0.002
p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = y_min,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
   
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocorrelation coefficient"), subtitle = "B", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25)) + ylim(y_min, 0.035)   


##
## Exceed Plot
##


gathercol <- colnames(ID_Summary_acf[,c(3,4)])

ID_Summary_acf_long  <-
gather(ID_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

p_exceed_acf <- ggplot(ID_Summary_acf_long, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + ylim(0,0.35) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~
Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())


##
## Power Spectra
##

p_power_frequency <-
  ggplot(data = Power_Frequency[Power_Frequency$r_freq > 0.01 & Power_Frequency$r_freq < 0.09,],
    mapping = aes(x = (r_freq), y = (mean_power), ymin = mean_power - ci_power, ymax = mean_power - ci_power, color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.5, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.4) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency (1/Trials)",  y =
"Spectral Density", subtitle = "D")

p_density_phase <-
ggplot(Power_Spectra[Power_Spectra$r_freq > 0.01 & Power_Spectra$r_freq < 0.09,], aes(
x = abs(Phase)
))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Phase",  y =
"Density", subtitle = "E") + theme_classic(base_size = 6) + theme(legend.position =
"none") +
geom_vline(xintercept = pi, linetype = "dashed",
    color = "black",
    size = 0.25) + xlim(-0.2, 3.4)

p_density_coherence <-
ggplot(Power_Spectra[Power_Spectra$bin_freq < 0.09,], aes(
x = Coherence
))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.1) + labs(x = "Squared coherence (%)",  y =
"Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position =
"none") + geom_vline(xintercept = mean(Power_Spectra[Power_Frequency$r_freq > 0.01 & Power_Frequency$r_freq <= 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",
    color = "black",
    size = 0.25) + xlim(-1, 26)

p_RT_mode <- ggplot(data = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_RT,
      ymin = average_RT - se_RT,
      ymax = average_RT + se_RT)) +
  geom_point(
    alpha = 0.8,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"z(RT)", subtitle = "H")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = max(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5 & Post_Perceptual_Modes$average_RT == max(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

p_Confidence_mode <- ggplot(data = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_Confidence,
      ymin = average_Confidence - se_Confidence,
      ymax = average_Confidence + se_Confidence)) +
  geom_point(
    alpha = 0.5,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"z(Confidence)", subtitle = "J")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = min(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5 & Post_Perceptual_Modes$average_Confidence == min(Post_Perceptual_Modes[Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )


p_RT <- ggplot() +
  geom_point(data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "RT",],
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Summary_RT_Confidence_Behav[ Summary_RT_Confidence_Behav$Type == "RT",],
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff RT (zscore)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.25, 0.05)

p_Confidence <- ggplot() +
  geom_point(data = RT_Confidence_Behav_long[RT_Confidence_Behav_long$Type == "Confidence",],
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Summary_RT_Confidence_Behav[ Summary_RT_Confidence_Behav$Type == "Confidence",],
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff Confidence (zscore)", subtitle = "I") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.25, 1)

lay <- rbind(c(1,2,2,3), c(9,2,2,3), c(4,5, 10,7), c(4,6,11,8))
grid.arrange(
p_distribution_Behav, p_acf, p_exceed_acf,
p_power_frequency, p_density_phase, p_density_coherence,
p_RT_mode, p_Confidence_mode, p_distribution_Behav_diff, p_RT, p_Confidence,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Figure 2. Internal and external modes in human perceptual decision-making.** 

A. In humans, perception was stimulus-congruent in `r mean(Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Accuracy, na.rm = TRUE)/sqrt(length(Behav$Accuracy))`% (in red) and history-congruent in `r mean(Behav$History, na.rm = TRUE)`% ± `r sd(Behav$History, na.rm = TRUE)/sqrt(length(Behav$History))`% of trials (in blue; upper panel). History-congruent perceptual choices were more frequent when perception was stimulus-incongruent (i.e., on *error* trials; lower panel), indicating that history effects impair performance in randomized psychophysical designs. 

B. Relative to randomly permuted data, we found highly significant autocorrelations of stimulus-congruence and history-congruence (dots indicate intercepts $\neq$ 0 in trial-wise linear mixed effects modeling at p < 0.05). Across trials, the autocorrelation coefficients were best fit by an exponential function (adjusted $R^2$ for stimulus-congruence: `r Stimulus_exponential$adj.r.squared`; history-congruence: `r History_exponential$adj.r.squared`) as compared to a linear function (adjusted $R^2$ for stimulus-congruence: `r Stimulus_linear$adj.r.squared`; history-congruence: `r History_linear$adj.r.squared`), decaying at a rate of $\gamma$ = $`r STAT.lmer_acf_Stimulus$coefficients[2,1]`$ ± $`r STAT.lmer_acf_Stimulus$coefficients[2,2]`$ (T($`r STAT.lmer_acf_Stimulus$coefficients[2,3]`$) = $`r STAT.lmer_acf_Stimulus$coefficients[2,4]`$, p = $`r STAT.lmer_acf_Stimulus$coefficients[2,5]`$) for stimulus-congruence and at a rate of $\gamma$ = $`r STAT.lmer_acf_History$coefficients[2,1]`$ ± $`r STAT.lmer_acf_History$coefficients[2,2]`$ (T($`r STAT.lmer_acf_History$coefficients[2,3]`$) = $`r STAT.lmer_acf_History$coefficients[2,4]`$, p = $`r STAT.lmer_acf_History$coefficients[2,5]`$) for history-congruence.

C. Here, we depict the number of consecutive trials at which autocorrelation coefficients exceeded the respective autocorrelation of randomly permuted data within individual participants. For stimulus-congruence (upper panel), the lag of positive autocorrelation amounted to $`r mean(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)`$ ± $`r sd(ID_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_Summary_acf)`$ on average, showing a peak at trial t+1 after the index trial. For history-congruence (lower panel), the lag of positive autocorrelation amounted to $`r mean(ID_Summary_acf$lag_significant_History, na.rm = TRUE)`$ ± $`r sd(ID_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_Summary_acf)`$ on average, peaking at trial t+2 after the index trial.

D. The smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law, i.e., at power densities that were inversely proportional to the frequency. 

E. The distribution of phase shift between fluctuations in stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). 

F. The average squared coherence between fluctuations in stimulus- and history-congruence (black dotted line) amounted to $`r mean(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)`$ ± $`r sd(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(Summary_Power_Spectra$mean_coherence)`$%

G. We observed faster RTs for both stimulus-congruence (as opposed to stimulus-incongruence, $\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$) 
and history-congruence ($\beta$ = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$).

H. The mode of perceptual processing (i.e., the difference between the smoothed probability of stimulus- vs. history-congruence) showed a quadratic relationship to RTs, with faster RTs for stronger biases toward both external sensory information and internal predictions provided by perceptual history ($\beta_2$ = $`r STAT.RT_vs_mode$coefficients[3,1]`$ ± $`r STAT.RT_vs_mode$coefficients[3,2]`$, T($`r STAT.RT_vs_mode$coefficients[3,3]`$) = $`r STAT.RT_vs_mode$coefficients[3,4]`$, p = $`r STAT.RT_vs_mode$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate maximum RT and the associated mode, respectively.

I. Confidence was enhanced for both stimulus-congruence (as opposed to stimulus-incongruence, $\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$) 
and history-congruence ($\beta$ = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$). 

J. In analogy to RTs, we found a quadratic relationship between the mode of perceptual processing and confidence, which increased when both externally- and internally-biased modes grew stronger ($\beta_2$ = $`r STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r STAT.Confidence_vs_mode$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum confidence and the associated mode, respectively.

\newpage
## Figure 3

```{r Figure_3}
mouse_p_distribution_Behav <- ggplot(M_Behav_long, aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

mouse_p_distribution_Behav_diff <- ggplot(M_Behav_diff_long, aes(x = History, color = -Accuracy, fill = Accuracy)) + 
   # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
   geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
    geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "top")



Plot_Acf_Data =  M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" | M_Summary_acf$Variable == "diff_acf_History",]
# Plot_Acf_Data =  M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus" | M_Summary_acf$Variable == "diff_acf_History",]
# Plot_Acf_Data =  M_Summary_acf

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_Difficulty", "Difficulty", Plot_Acf_Data$Variable)

y_min = -0.002
mouse_p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 1,
    fill = "white",
    position = position_dodge(width = 0.5)
  )  + 
  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = y_min,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
   
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocorrelation coefficient"), subtitle = "B", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(-0.5, 25)) #+ ylim(y_min, 0.03)  



mouse_p_exceed_acf <- ggplot(ID_M_Summary_acf_long, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + ylim(0,0.35) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~
Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())

mouse_p_power_frequency <-
  ggplot(data = M_Power_Frequency[M_Power_Frequency$r_freq > 0.01 & M_Power_Frequency$r_freq < 0.09,],
         mapping = aes(x = (r_freq), y = (mean_power), ymin = mean_power - ci_power, ymax = mean_power - ci_power, color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.25, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE, level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.35) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency",  y = "Spectral Density", subtitle = "D")

mouse_p_density_phase <-
  ggplot(M_Power_Spectra[M_Power_Spectra$r_freq > 0.01 & M_Power_Spectra$r_freq < 0.09,], aes(
    x = abs(Phase)
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Phase",  y =
                                                                                  "Density", subtitle = "E") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") +
  geom_vline(xintercept = pi, linetype = "dashed",
             color = "black",
             size = 0.25) #+ xlim(-0.2, 3.4)

mouse_p_density_coherence <-
  ggplot(M_Power_Spectra[M_Power_Spectra$bin_freq < 0.09,], aes(
    x = Coherence
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Squred coherence (%)",  y =
                                                                                  "Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") + geom_vline(xintercept = mean(M_Power_Spectra[M_Power_Frequency$r_freq > 0.01 & M_Power_Frequency$r_freq < 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",
                                                                                                                                                                            color = "black",
                                                                                                                                                                            size = 0.25) + xlim(-1, 26)

mouse_p_RT_mode <- ggplot(data = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_RT,
      ymin = average_RT - se_RT,
      ymax = average_RT + se_RT)) +
  geom_point(
    alpha = 0.8,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"TD (msec)", subtitle = "H")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5 & M_Post_Perceptual_Modes$average_RT == max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

mouse_p_RT_mode2 <- ggplot(data = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,], aes(
      x = directed_mode,
      y = average_RT2,
      ymin = average_RT2 - se_RT2,
      ymax = average_RT2 + se_RT2)) +
  geom_point(
    alpha = 0.8,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
"engaged TD (msec)", subtitle = "I")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
    level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT2),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5 & M_Post_Perceptual_Modes$average_RT2 == max(M_Post_Perceptual_Modes[M_Post_Perceptual_Modes$n_percent > 0.5,]$average_RT2),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

p_mouse_RT_Congruency <- ggplot() +
  geom_point(data = M_RT_Behav_long,
    aes(
      x = Variable,
      y = diff_RT,
      color = Variable),
      alpha = 0.25,
      size = 0.01,
    fill = "white",
    position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = M_Summary_RT_Behav,
    aes(
      x = Variable,
      ymin = Mean - Error,
      ymax = Mean + Error,
      color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff TD (msec)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

lay <- rbind(c(1,2,2,3), c(9,2,2,3), c(4,5, 7,8), c(4,6,7,10))
grid.arrange(
mouse_p_distribution_Behav, mouse_p_acf, mouse_p_exceed_acf,
mouse_p_power_frequency, mouse_p_density_phase, mouse_p_density_coherence,
p_mouse_RT_Congruency, mouse_p_RT_mode, mouse_p_distribution_Behav_diff, mouse_p_RT_mode2,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))

```

**Figure 3. Internal and external modes in mouse perceptual decision-making.** 

A. In mice, `r mean(M_Behav$Accuracy, na.rm = TRUE)`% ± `r sd(M_Behav$Accuracy, na.rm = TRUE)/sqrt(length(M_Behav$Accuracy))`% of trials were stimulus-congruent (in red) and `r mean(M_Behav$History, na.rm = TRUE)`% ± `r sd(M_Behav$History, na.rm = TRUE)/sqrt(length(M_Behav$History))`% of trials were history-congruent (in blue; upper panel). History-congruent perceptual choices were not a consequence of the experimental design, but a source of error, as they were more frequent on stimulus-incongruent trials (lower panel).

B. Relative to randomly permuted data, we found highly significant autocorrelations of stimulus-congruence and history-congruence (dots indicate intercepts $\neq$ 0 in trial-wise linear mixed effects modeling at p < 0.05). Please note that the negative autocorrelation of stimulus-congruence at trial 2 was a consequence of the experimental design (Supplemental Figure 2D-F). As in humans, autocorrelation coefficients were best fit by an exponential function (adjusted $R^2$ for stimulus-congruence: $`r M_Stimulus_exponential$adj.r.squared`$; history-congruence: $`r M_History_exponential$adj.r.squared`$) as compared to a linear function (adjusted $R^2$ for stimulus-congruence: $`r M_Stimulus_linear$adj.r.squared`$; history-congruence: $`r M_History_linear$adj.r.squared`$), decaying at a rate of $\gamma$ = $`r M_STAT.lmer_acf_Stimulus$coefficients[2,1]`$ ± $`r M_STAT.lmer_acf_Stimulus$coefficients[2,2]`$ (T($`r M_STAT.lmer_acf_Stimulus$coefficients[2,3]`$) = $`r M_STAT.lmer_acf_Stimulus$coefficients[2,4]`$, p = $`r M_STAT.lmer_acf_Stimulus$coefficients[2,5]`$) for stimulus-congruence and at a rate of $\gamma$ = $`r M_STAT.lmer_acf_History$coefficients[2,1]`$ ± $`r M_STAT.lmer_acf_History$coefficients[2,2]`$ (T($`r M_STAT.lmer_acf_History$coefficients[2,3]`$) = $`r M_STAT.lmer_acf_History$coefficients[2,4]`$, p = $`r M_STAT.lmer_acf_History$coefficients[2,5]`$) for history-congruence.

C. For stimulus-congruence (upper panel), the lag of positive autocorrelation was longer in comparison to humans (`r mean(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_M_Summary_acf)` on average). For history-congruence (lower panel), the lag of positive autocorrelation was slightly shorter relative to humans (`r mean(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_M_Summary_acf)` on average, peaking at trial t+2 after the index trial).

D. In mice, the dynamic probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law.

E. The distribution of phase shift between fluctuations in stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). 

F. The average squared coherence between fluctuations in stimulus- and history-congruence (black dotted line) amounted to `r mean(M_Summary_Power_Spectra$mean_coherence, na.rm = TRUE)` ± `r sd(M_Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(M_Summary_Power_Spectra$mean_coherence)`%.

G. We observed shorter trial durations (TDs) for stimulus-congruence (as opposed to stimulus-incongruence, $\beta$ = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,1]`$ ± $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,2]`$, T($`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,3]`$) = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,4]`$, p = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[2,5]`$), 
but longer TDs for history-congruence ($\beta$ = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,1]`$ ± $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,2]`$, T($`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,3]`$) = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,4]`$, p = $`r M_STAT.lmer_RT_Accuracy_History$coefficients[3,5]`$). 

H. TDs decreased monotonically for stronger biases toward external mode ($\beta_1$ = $`r M_STAT.RT_vs_mode$coefficients[2,1]`$ ± $`r M_STAT.RT_vs_mode$coefficients[2,2]`$, T($`r M_STAT.RT_vs_mode$coefficients[2,3]`$) = $`r M_STAT.RT_vs_mode$coefficients[2,4]`$, p = $`r M_STAT.RT_vs_mode$coefficients[2,5]`$). The horizontal and vertical dotted lines indicate maximum TD and the associated mode, respectively.

I. For TDs that differed from the median TD by no more than 1.5 x MAD (median absolute distance[@Leys2013]), mice exhibited a quadratic component in the relationship between the mode of sensory processing and TDs ($\beta_2$ = $`r M_STAT.RT_vs_mode2$coefficients[3,1]`$ ± $`r M_STAT.RT_vs_mode2$coefficients[3,2]`$, T($`r M_STAT.RT_vs_mode2$coefficients[3,3]`$) = $`r M_STAT.RT_vs_mode2$coefficients[3,4]`$, p = $`r M_STAT.RT_vs_mode2$coefficients[3,5]`$). This explorative post-hoc analysis focuses on trials at which mice engage more swiftly with the experimental task. The horizontal and vertical dotted lines indicate maximum TD and the associated mode, respectively.

\newpage
## Figure 4

```{r Figure_4}
gathercol <- colnames(Sim_Behav[,c(3,6)])

Sim_Behav_long  <-
  gather(Sim_Behav[, c(1,2,3,6)],
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Sim_Behav_long$Frequency <-
  gsub("Accuracy", "Stimulus", Sim_Behav_long$Frequency)

sim_p_distribution_Behav <- ggplot(Sim_Behav_long, aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

##
## Probability of History-congruence depending on perceptual accuracy
##


gathercol <- colnames(Sim_Behav_diff[3])

Sim_Behav_diff_long  <-
  gather(Sim_Behav_diff,
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Sim_Behav_diff_long$Accuracy <- as.character(Sim_Behav_diff_long$Accuracy) 
Sim_Behav_diff_long$Accuracy <-
  gsub("0", "error", Sim_Behav_diff_long$Accuracy)
Sim_Behav_diff_long$Accuracy <-
  gsub("1", "correct", Sim_Behav_diff_long$Accuracy)

sim_p_distribution_Behav_diff <- ggplot(Sim_Behav_diff_long, aes(x = Frequency, color = -Accuracy, fill = Accuracy)) + 
  # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "bottom")


##
## Acf Plot
##
Plot_Acf_Data =  Sim_Summary_acf[(Sim_Summary_acf$Variable == "diff_acf_Stimulus" | Sim_Summary_acf$Variable == "diff_acf_History") ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

y_min = -0.002
sim_p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
             aes(
               x = Trial,
               y = Mean,
               fill = Variable,
               color = Variable),
             alpha = 0.8,
             size = 1,
             fill = "white",
             position = position_dodge(width = 0.5)
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = y_min,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
                aes(
                  x = Trial,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  fill = Variable,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
                                                                                                                               aes(
                                                                                                                                 x = Trial,
                                                                                                                                 y = Mean,
                                                                                                                                 fill = Variable,
                                                                                                                                 color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
  
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
                                        paste("Autocorrelation coefficient"), subtitle = "B", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.8, 0.9), legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25)) #+ ylim(-0.004, 0.03)  


##
## Exceed Plot
##


gathercol <- colnames(ID_Sim_Summary_acf[,c(3,4)])

ID_Sim_Summary_acf_long  <-
  gather(ID_Sim_Summary_acf,
         "Variable",
         "Lag",
         gathercol,
         factor_key = TRUE)

sim_p_exceed_acf <- ggplot(ID_Sim_Summary_acf_long, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
  scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())

##
## Power Spectra
##

Sim_p_power_frequency <-
  ggplot(data = Sim_Power_Frequency[Sim_Power_Frequency$r_freq > 0.01 & Sim_Power_Frequency$r_freq < 0.09,],
         mapping = aes(x = (r_freq), y = (mean_power), ymin = (mean_power - ci_power), ymax = (mean_power - ci_power), color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.5, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
                                                level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.2) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency",  y = "Spectral Density", subtitle = "D")

Sim_p_density_phase <-
  ggplot(Sim_Power_Spectra[Sim_Power_Spectra$r_freq > 0.01 & Sim_Power_Spectra$r_freq < 0.09,], aes(
    x = abs(Phase)
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Phase",  y =
                                                                                  "Density", subtitle = "E") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") +
  geom_vline(xintercept = pi, linetype = "dashed",
             color = "black",
             size = 0.25) #+ xlim(-0.2, 3.4)

Sim_p_density_coherence <-
  ggplot(Sim_Power_Spectra[Sim_Power_Spectra$bin_freq < 0.09,], aes(
    x = Coherence
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + labs(x = "Squared coherence (%)",  y =
                                                                                  "Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position =
                                                                                                                                                       "none") + geom_vline(xintercept = mean(Sim_Power_Spectra[Sim_Power_Frequency$r_freq > 0.01 & Sim_Power_Frequency$r_freq < 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",
                                                                                                                                                                            color = "black",
                                                                                                                                                                            size = 0.25) + xlim(-1, 26)


sim_p_Confidence_mode <- ggplot(data = Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0.5,], aes(
  x = directed_mode,
  y = average_Confidence,
  ymin = average_Confidence - se_Confidence,
  ymax = average_Confidence + se_Confidence)) +
  geom_point(
    alpha = 0.5,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y =
         "Simulated posterior certainty", subtitle = "H")  + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
                                                                                                                                                                                                                                                                     level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = min(Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0,]$average_Confidence),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0 & Sim_Post_Perceptual_Modes$average_Confidence == min(Sim_Post_Perceptual_Modes[Sim_Post_Perceptual_Modes$n_percent > 0,]$average_Confidence),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  )

sim_p_Confidence <- ggplot() +
  geom_point(data = Sim_Confidence_Behav_long,
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Summary_Sim_Confidence_Behav,
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff posterior certainty (zscore)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.1, 0.25)

lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
grid.arrange(
sim_p_distribution_Behav, sim_p_acf, sim_p_exceed_acf,
Sim_p_power_frequency, Sim_p_density_phase, Sim_p_density_coherence,
sim_p_Confidence_mode, sim_p_distribution_Behav_diff, sim_p_Confidence,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Figure 4. Internal and external modes in simulated perceptual decision-making.**

A. Simulated perceptual choices were stimulus-congruent in `r mean(Sim_Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Sim_Behav$Accuracy, na.rm = TRUE)/sqrt(length(Sim_Behav$Accuracy))`% (in red) and history-congruent in `r mean(Sim_Behav$History, na.rm = TRUE)`% ± `r sd(Sim_Behav$History, na.rm = TRUE)/sqrt(length(Sim_Behav$History))`% of trials (in blue; T($`r Sim_STAT.Global_History_Accuracy$parameter`$) = $`r Sim_STAT.Global_History_Accuracy$statistic`$, p = $`r Sim_STAT.Global_History_Accuracy$p.value`$; upper panel). 
Due to the competition between stimulus- and history-congruence, history-congruent perceptual choices were more frequent when perception was stimulus-incongruent (i.e., on *error* trials; T(`r Sim_STAT.diff_History_Accuracy$parameter`) = `r Sim_STAT.diff_History_Accuracy$statistic`, p = $`r Sim_STAT.diff_History_Accuracy$p.value`$; lower panel) and thus impaired performance in the randomized psychophysical design simulated here. 

B. At the simulated group level, we found significant autocorrelations in both stimulus-congruence (`r min(which(Sim_Summary_acf[Sim_Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials) and history-congruence (`r min(which(Sim_Summary_acf[Sim_Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials). 
<!-- In line with humans and mice, the autocorrelation of history-congruence peaked at the first trial, after which is decayed exponentially (rate $\gamma$ = $`r Sim_STAT.lmer_acf_History$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_acf_History$coefficients[2,2]`$, T($`r Sim_STAT.lmer_acf_History$coefficients[2,3]`$) = $`r Sim_STAT.lmer_acf_History$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_acf_History$coefficients[2,5]`$). -->

C. On the level of individual simulated participants, autocorrelation coefficients exceeded the autocorrelation coefficients of randomly permuted data within a lag of $`r mean(ID_Sim_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)`$ ± $`r sd(ID_Sim_Summary_acf$lag_significant_Stimulus, na.rm = TRUE)/nrow(ID_Sim_Summary_acf)`$ trials for stimulus-congruence and $`r mean(ID_Sim_Summary_acf$lag_significant_History, na.rm = TRUE)`$ ± $`r sd(ID_Sim_Summary_acf$lag_significant_History, na.rm = TRUE)/nrow(ID_Sim_Summary_acf)`$ trials for history-congruence.

D. The smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,2]`$, T($`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,3]`$) = $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_power_freq_Stimulus$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Sim_STAT.lmer_power_freq_History$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_power_freq_History$coefficients[2,2]`$, T($`r Sim_STAT.lmer_power_freq_History$coefficients[2,3]`$) = $`r Sim_STAT.lmer_power_freq_History$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_power_freq_History$coefficients[2,5]`$).

E. The distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). The dynamic probabilities of simulated stimulus- and history-congruence were therefore were strongly anti-correlated ($\beta$ = $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,1]`$ ± $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,2]`$, T($`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,3]`$) = $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,4]`$, p = $`r Sim_STAT.slider_History_vs_Accuracy$coefficients[2,5]`$).

F. The average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) amounted to $`r mean(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)`$ ± $`r sd(Summary_Power_Spectra$mean_coherence, na.rm = TRUE)/length(Summary_Power_Spectra$mean_coherence)`$%.

G. Simulated confidence was enhanced for stimulus-congruence ($\beta$ = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,1]`$ ± $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,2]`$, T($`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,3]`$) = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,4]`$, p = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[2,5]`$) 
and history-congruence ($\beta$ = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,1]`$ ± $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,2]`$, T($`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,3]`$) = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,4]`$, p = $`r Sim_STAT.lmer_Confidence_Accuracy_History$coefficients[3,5]`$). 

H. In analogy to humans, the simulated data showed a quadratic relationship between the mode of perceptual processing and posterior certainty, which increased for stronger external and internal biases ($\beta_2$ = $`r Sim_STAT.Confidence_vs_mode$coefficients[3,1]`$ ± $`r Sim_STAT.Confidence_vs_mode$coefficients[3,2]`$, T($`r Sim_STAT.Confidence_vs_mode$coefficients[3,3]`$) = $`r Sim_STAT.Confidence_vs_mode$coefficients[3,4]`$, p = $`r Sim_STAT.Confidence_vs_mode$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
# Supplemental Items

## Internal mode processing is driven by choice history as opposed to stimulus history

```{r choice_history, cache = TRUE}
if (!load_summary_data) {
  
## humans

## General effect of stimulus-history
Behav$null_Stimulus_History = Behav$Stimulus_History - 50
mean_Stimulus_History = mean(Behav$Stimulus_History, na.rm = TRUE)
error_Stimulus_History = sd(Behav$Stimulus_History, na.rm = TRUE)/sqrt(length(Behav$Stimulus_History))
  
Global_Stimulus_History_Accuracy <- lmer(null_Stimulus_History ~ Accuracy + (1 | study_id), data = Behav)
STAT.Global_Stimulus_History_Accuracy = summary(Global_Stimulus_History_Accuracy)

## Difference between choice- and stimulus-history
Difference_Stimulus_Choice_History = lmer(null_Stimulus_History-null_History ~ 1 + (1 | study_id), data = Behav)
STAT.Difference_Stimulus_Choice_History = summary(Difference_Stimulus_Choice_History)

if (compute_logreg) {
## Comparison of the explanatory power of stimulus- and choice history
PwData$Stimulus_minus_1 = c(NA, PwData$Stimulus[1:length(PwData$Stimulus)-1])
PwData$Stimulus_minus_1[PwData$Trial == 1] = NA
full_logit_Sum_Stimulus_History <-
    glmer(Response ~ Stimulus + Stimulus_minus_1 + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1 & PwData$Stimulus_minus_1 != PwData$Response_minus_1,],
        family = binomial)

full_logit_Sum_Choice_History <-
    glmer(Response ~ Stimulus + Response_minus_1 + (1|study_id/subject_id),
        data = PwData[PwData$Response >= 0 & PwData$Response <= 1 & PwData$Stimulus_minus_1 != PwData$Response_minus_1,],
        family = binomial)
} else {
  full_logit_Sum_Stimulus_History <- readRDS(file = "./Results/full_logit_Sum_Stimulus_History.rds")
  full_logit_Sum_Choice_History <- readRDS(file = "./Results/full_logit_Sum_Choice_History.rds")
}
STAT.full_logit_Sum_Stimulus_History <- summary(full_logit_Sum_Stimulus_History) 
STAT.full_logit_Sum_Choice_History <- summary(full_logit_Sum_Choice_History)

##
## mice
##
## General effect of stimulus-history
M_Behav$null_Stimulus_History = M_Behav$Stimulus_History - 50
M_mean_Stimulus_History = mean(M_Behav$Stimulus_History, na.rm = TRUE)
M_error_Stimulus_History = sd(M_Behav$Stimulus_History, na.rm = TRUE)/sqrt(length(M_Behav$Stimulus_History))
M_STAT.Global_Stimulus_History_Accuracy = t.test(M_Behav$null_Stimulus_History)

## Difference between choice- and stimulus-history
M_STAT.Difference_Stimulus_Choice_History = t.test(M_Behav$null_Stimulus_History - M_Behav$null_History)

if (compute_mouse_logreg) {
    M_full_logit_Sum_Stimulus_History <-
      glmer(
        Response ~ Stimulus + Stimulus_minus_1 + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1 & MwData$Stimulus_minus_1 != MwData$Response_minus_1, ],
        family = binomial
      )
    
    M_full_logit_Sum_Choice_History <-
      glmer(
        Response ~ Stimulus + Response_minus_1 + (1 | subject_id),
        data = MwData[MwData$Response >= 0 & MwData$Response <= 1 & MwData$Stimulus_minus_1 != MwData$Response_minus_1, ],
        family = binomial
      )
    
  } else {
    M_full_logit_Sum_Stimulus_History <-
      readRDS(file = "./Results/M_full_logit_Sum_Stimulus_History.rds")
     M_full_logit_Sum_Choice_History <-
      readRDS(file = "./Results/M_full_logit_Sum_Choice_History.rds")
   
  }
  M_STAT.full_logit_Sum_Stimulus_History <- summary(M_full_logit_Sum_Stimulus_History)
  M_STAT.full_logit_Sum_Choice_History <- summary(M_full_logit_Sum_Choice_History)

  if (save_summary_data) {
    save(mean_Stimulus_History,
         error_Stimulus_History,
         STAT.Global_Stimulus_History_Accuracy, 
         STAT.Difference_Stimulus_Choice_History,
         STAT.full_logit_Sum_Stimulus_History,
         STAT.full_logit_Sum_Choice_History,
         M_mean_Stimulus_History,
         M_error_Stimulus_History,
         M_STAT.Global_Stimulus_History_Accuracy, 
         M_STAT.Difference_Stimulus_Choice_History,
         M_STAT.full_logit_Sum_Stimulus_History,
         M_STAT.full_logit_Sum_Choice_History,
         file = "./Summary_Data/choice_history.Rdata")
  }
  
} else {
  load("./Summary_Data/choice_history.Rdata")
}
```

The main manuscript reports the effects of perceptual history, which we defined as the impact of the choice at the preceding trial on the choice at the current trial (henceforth *choice history*). *Stimulus history*, which is defined as the impact of the stimulus presented at the preceding trial on the choice at the present trial, represents an alternative approach to this. Here, we compare the effects of choice history to the effects of stimulus history. 

We observed a significant bias toward stimulus history (humans: `r mean_Stimulus_History`% ± `r  error_Stimulus_History`% of trials, $\beta$ = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,1]`$ ± $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,2]`$, T($`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,3]`$) = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,4]`$, p = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,5]`$; mice: `r M_mean_Stimulus_History`% ± `r  M_error_Stimulus_History`% of trials,  T(`r M_STAT.Global_Stimulus_History_Accuracy$parameter`) = `r M_STAT.Global_Stimulus_History_Accuracy$statistic`, p = $`r M_STAT.Global_Stimulus_History_Accuracy$p.value`$). The bias toward stimulus history was smaller than the bias toward choice history (humans: $\beta$ = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,1]`$ ± $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,2]`$, T($`r STAT.Difference_Stimulus_Choice_History$coefficients[1,3]`$) = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,4]`$, p = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,5]`$; mice: T(`r M_STAT.Difference_Stimulus_Choice_History$parameter`) = `r M_STAT.Difference_Stimulus_Choice_History$statistic`, p = $`r M_STAT.Difference_Stimulus_Choice_History$p.value`$).

The attraction of choices toward both preceding choices and stimuli is expected, as perception was *stimulus-congruent* on approximately 75% of trials, causing choices and stimuli to be highly correlated. We therefore compared the effects of choice history and stimulus history after  *stimulus-incongruent* (i.e., *error*) trials, since those trials lead to opposite predictions regarding the perceptual choice at the subsequent trial.

As expected from the findings presented in the main manuscript, perceptual choices were attracted toward perceptual choices when the inducing trial was stimulus-incongruent (i.e., a positive effect of choice history; humans: $\beta$ = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum_Choice_History$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,4]`$: mice: $\beta$ = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,4]`$). By contrast, perceptual choices tended to be repelled away from the stimulus presented at preceding stimulus-incongruent trial (i.e., a negative effect of stimulus history; humans: $\beta$ = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,4]`$: mice: $\beta$ = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,4]`$). This repulsion of choices away from stimuli presented at stimulus-incongruent trials confirmed that choices (which are anti-correlated to stimuli at stimulus-incongruent trials) were the primary driver of attracting serial effects in perception.

In sum, the above results suggest that, in both humans and mice, serial dependencies were better explained by the effects of choice history as opposed to the effects of stimulus history. This aligns with a result recently published for the IBL database, where mice were shown to follow an *action-kernel* as opposed to a *stimulus-kernel* model when integrating information across trials[@Findling2023].

## Fluctuations between internal and external mode modulate perceptual performance beyond the effect of general response biases

The hypothesis that perception cycles through opposing internally- and externally-biased modes is motivated by the assumption that recurring intervals of stronger perceptual history temporally reduce the participants' sensitivity to external information. Importantly, the history-dependent biases that characterize internal mode processing must be differentiated from general response biases. In binary perceptual decision-making, general response biases are defined by a propensity to choose one of the two outcomes more often than the alternative. Indeed, human participants selected the more frequent of the two possible outcomes in `r mean(Behav$Bias, na.rm = TRUE)`% ± `r sd(Behav$Bias, na.rm = TRUE)/sqrt(length(Behav$Bias))`% of trials, and mice selected the more frequent of the two possible outcomes in `r mean(M_Behav$Bias, na.rm = TRUE)`% ± `r sd(M_Behav$Bias, na.rm = TRUE)/sqrt(length(M_Behav$Bias))`% of trials.

Two caveats have to be considered to make sure that the effect of history-congruence is distinct from the effect of general response biases. First, history-congruent states become more likely for larger response biases that cause an increasing imbalance in the likelihood of the two outcomes (humans: $\beta$ = $`r STAT.slider_History_vs_Preferred$coefficients[2,1]`$ ± $`r STAT.slider_History_vs_Preferred$coefficients[2,2]`$, T($`r STAT.slider_History_vs_Preferred$coefficients[2,3]`$) = $`r STAT.slider_History_vs_Preferred$coefficients[2,4]`$, p = $`r STAT.slider_History_vs_Preferred$coefficients[2,5]`$; 
mice: $\beta$ = $`r M_STAT.slider_History_vs_Preferred$coefficients[2,1]`$ ± $`r M_STAT.slider_History_vs_Preferred$coefficients[2,2]`$, T($`r M_STAT.slider_History_vs_Preferred$coefficients[2,3]`$) = $`r M_STAT.slider_History_vs_Preferred$coefficients[2,4]`$, p = $`r M_STAT.slider_History_vs_Preferred$coefficients[2,5]`$). One may thus ask whether the autocorrelation of history-congruence could be entirely driven by general response biases. 

Importantly, our autocorrelation analyses account for general response biases by computing group-level autocorrelations (Figure 2-4B) relative to randomly permuted data (i.e., by subtracting the autocorrelation of randomly permuted data from the raw autocorrelation curve). This precludes that general response biases contribute to the observed autocorrelation of history-congruence (see Supplemental Figure S5 for a visualization of the correction procedure for simulated data with general response biases ranging from 60 to 90%). 

Second, it may be argued that fluctuations in perceptual performance may be solely driven by ongoing changes in the strength of general response biases. To assess the links between dynamic fluctuations in stimulus-congruence on the one hand and history-congruence as well as general response bias on the other hand, we computed all variables as dynamic probabilities in sliding windows of ± 5 trials (Figure 1C). Linear mixed effects modeling indicated that fluctuations in history-congruent biases were larger in amplitude than the corresponding fluctuations in general response biases in humans ($\beta_0$ = $`r STAT.Variance_History_vs_Preferred$coefficients[1,1]`$ ± $`r STAT.Variance_History_vs_Preferred$coefficients[1,2]`$, T($`r STAT.Variance_History_vs_Preferred$coefficients[1,3]`$) = $`r STAT.Variance_History_vs_Preferred$coefficients[1,4]`$, p = $`r STAT.Variance_History_vs_Preferred$coefficients[1,5]`$), 
but slightly smaller in mice ($\beta_0$ = $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,1]`$ ± $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,2]`$, T($`r M_STAT.Variance_History_vs_Preferred$coefficients[1,3]`$) = $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,4]`$, p = $`r M_STAT.Variance_History_vs_Preferred$coefficients[1,5]`$).

Crucially, ongoing fluctuations in history-congruence had a significant negative effect on stimulus-congruence (humans: $\beta_1$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$;
mice: $\beta_1$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) 
beyond the effect of ongoing changes in general response biases (humans: $\beta_2$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$;
mice: $\beta_2$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$). In sum, the above control analyses confirmed that, in both humans and mice, the observed influence of preceding choices on perceptual decision-making cannot be reduced to general response biases.


## Internal mode is characterized by lower thresholds as well as by history-dependent changes in biases and lapses

Random or stereotypical responses may provide an alternative explanation for the reduced sensitivity to external sensory information that we attribute to internal mode processing. To test this hypothesis, we asked whether history-independent changes in biases and lapses may provide an alternative explanation of the reduced sensitivity during internal mode. 

To this end, we estimated full and history-conditioned psychometric curves to investigate how internal and external mode relate to biases (i.e., the horizontal position of the psychometric curve), lapses (i.e., the asymptotes of the psychometric curve) and thresholds (i.e., 1/sensitivity, estimated from the slope of the psychometric curve). We used a maximum likelihood procedure to predict trial-wise choices $y$ ($y = 0$ and $y = 1$ for outcomes A and B respectively) from the choice probabilities $y_p$. $y_p$ was computed from the difficulty-weighted inputs $s_w$ via a parametric error function defined by the parameters $\gamma$ (lower lapse), $\delta$ (upper lapse), $\mu$ (bias) and $t$ (threshold; see Methods for details):

\begin{equation}
y_p = \gamma + (1 - \gamma - \delta) *  (erf(\frac{s_w + \mu}{t}) + 1) / 2
\end{equation}

Under our main hypothesis that periodic reductions in sensitivity to external information are driven by increases in the impact of perceptual history, one would expect (i) a history-dependent increase in biases and lapses (effects of perceptual history), and (ii), a history-independent increase in threshold (reduced sensitivity to external information). Conversely, if what we identified as internal mode processing was in fact driven by random choices, one would expect (i), a history-independent increase in lapses (choice randomness), (ii), no change in bias (no effect of perceptual history), and (iii), reduced thresholds (reduced sensitivity to external information).

### Humans
```{r psychometric_function, cache = TRUE}
if (!load_summary_data) {
  Behav_PM <-
    ddply(
      PwData,
      .(subject_id, study_id, round(Difficulty, digits = 0)),
      summarise,
      p_Accuracy = sum(Accuracy == 1, na.rm = TRUE) / length(Accuracy),
      n_Accuracy = length(Accuracy)
    )
  
  colnames(Behav_PM) <-
    c("subject_id",
      "study_id",
      "Difficulty",
      "p_Accuracy",
      "n_Accuracy")
  
  Behav_PM <-
    Behav_PM[Behav_PM$Difficulty >= -3 &
               Behav_PM$Difficulty <= 3 &
               !is.na(Behav_PM$Difficulty), ]
  
  PwData$rDifficulty <- round(PwData$Difficulty, digits = 0)
  
  P_Behav = Behav
  P_Behav = P_Behav[P_Behav$subject_id %in% unique(PwData[!is.na(PwData$Difficulty),]$subject_id),]
  
  N_PM <- ddply(PwData,
                .(rDifficulty),
                summarise,
                n_trials = length(rDifficulty))
  
  difficulty_levels = seq(-3, 3, 1)
  
  type_list = c(
    "full_b",
    "full_0",
    "full_1",
    "external_b",
    "external_0",
    "external_1",
    "internal_b",
    "internal_0",
    "internal_1"
  )
  
  P_Behav = cbind(P_Behav, matrix(NA, ncol = length(type_list) * 5, nrow = nrow(P_Behav)))
  colnames(P_Behav) <-
    c(
      "subject_id",
      "study_id",
      "History",
      "Min_History",
      "Max_History",
      "Accuracy",
      "Min_Accuracy",
      "Max_Accuracy",
      "Stimulus_History",
      "Stimulus_Bias",
      "Response_Bias",
      "Bias",
      "null_History",
      "full_b_lower_lapse",
      "full_b_higher_lapse",
      "full_b_av_lapse",
      "full_b_bias",
      "full_b_threshold",
      "full_0_lower_lapse",
      "full_0_higher_lapse",
      "full_0_av_lapse",
      "full_0_bias",
      "full_0_threshold",
      "full_1_lower_lapse",
      "full_1_higher_lapse",
      "full_1_av_lapse",
      "full_1_bias",
      "full_1_threshold",
      "external_b_lower_lapse",
      "external_b_higher_lapse",
      "external_b_av_lapse",
      "external_b_bias",
      "external_b_threshold",
      "external_0_lower_lapse",
      "external_0_higher_lapse",
      "external_0_av_lapse",
      "external_0_bias",
      "external_0_threshold",
      "external_1_lower_lapse",
      "external_1_higher_lapse",
      "external_1_av_lapse",
      "external_1_bias",
      "external_1_threshold",
      "internal_b_lower_lapse",
      "internal_b_higher_lapse",
      "internal_b_av_lapse",
      "internal_b_bias",
      "internal_b_threshold",
      "internal_0_lower_lapse",
      "internal_0_higher_lapse",
      "internal_0_av_lapse",
      "internal_0_bias",
      "internal_0_threshold",
      "internal_1_lower_lapse",
      "internal_1_higher_lapse",
      "internal_1_av_lapse",
      "internal_1_bias",
      "internal_1_threshold"
    )
  
  
  ##
  ## FIT
  ##
  
  for (subj_idx in unique(P_Behav$subject_id)) {
    print(subj_idx)
    
    Input_Data = PwData[PwData$subject_id == subj_idx &
                          PwData$rDifficulty %in% difficulty_levels, c(
                            "Stimulus",
                            "Response",
                            "rDifficulty",
                            "Response_minus_1",
                            "scaled_directed_mode"
                          )]
    
    max_env = which(Input_Data$Stimulus == max(Input_Data$Stimulus, na.rm = TRUE))
    min_env = which(Input_Data$Stimulus == min(Input_Data$Stimulus, na.rm = TRUE))
    Input_Data$Stimulus[max_env] = 1
    Input_Data$Stimulus[min_env] = 0
    
    max_response = which(Input_Data$Response == max(Input_Data$Response, na.rm = TRUE))
    min_response = which(Input_Data$Response == min(Input_Data$Response, na.rm = TRUE))
    Input_Data$Response[max_response] = 1
    Input_Data$Response[min_response] = 0
    
    max_response_minus_1 = which(Input_Data$Response_minus_1 == max(Input_Data$Response_minus_1, na.rm = TRUE))
    min_response_minus_1 = which(Input_Data$Response_minus_1 == min(Input_Data$Response_minus_1, na.rm = TRUE))
    Input_Data$Response_minus_1[max_response_minus_1] = 1
    Input_Data$Response_minus_1[min_response_minus_1] = 0
    
    for (type_idx in type_list) {
      if (type_idx == "full_b") {
        trial_selector = rep(TRUE, nrow(Input_Data))
      } else if (type_idx == "full_0") {
        trial_selector =
          Input_Data$Response_minus_1 == 0
      } else if (type_idx == "full_1") {
        trial_selector =
          Input_Data$Response_minus_1 == 1
      } else if (type_idx == "internal_b") {
        trial_selector =
          Input_Data$scaled_directed_mode < 0
      } else if (type_idx == "internal_0") {
        trial_selector =
          Input_Data$Response_minus_1 == 0 &
          Input_Data$scaled_directed_mode < 0
      } else if (type_idx == "internal_1") {
        trial_selector =
          Input_Data$Response_minus_1 == 1 &
          Input_Data$scaled_directed_mode < 0
      } else if (type_idx == "external_b") {
        trial_selector =
          Input_Data$scaled_directed_mode > 0
      } else if (type_idx == "external_0") {
        trial_selector =
          Input_Data$Response_minus_1 == 0 &
          Input_Data$scaled_directed_mode > 0
      } else if (type_idx == "external_1") {
        trial_selector =
          Input_Data$Response_minus_1 == 1 &
          Input_Data$scaled_directed_mode > 0
      }
      
      source("./Functions/fit_psychometric_function.R")
      par = c(0.0, 0.0, 0, 5)
      lower = c(0.0, 0.0,-5, 0.5)
      upper = c(0.5, 0.5, 5, 25)
      
      
      add_PM =  optimx(
        par,
        fit_psychometric_function,
        Input_Data = Input_Data[trial_selector[!is.na(trial_selector)], ],
        lower = lower,
        upper = upper,
        method = "L-BFGS-B"
      )
      
      P_Behav[P_Behav$subject_id == subj_idx, grepl(type_idx, colnames(P_Behav))] <-
        c(add_PM$p1, add_PM$p2, mean(c(add_PM$p1, add_PM$p2)), add_PM$p3, add_PM$p4)
    }
  }
  
  STAT.PM_History_lapse_bias_threshold <-
    summary(lmer(
      History ~ 1 + full_b_av_lapse + abs(full_b_bias) + full_b_threshold + (1 |
                                                                               study_id),
      data = P_Behav
    ))
  
  STAT.PM_Accuracy_History_lapse <-
    summary(lmer(Accuracy ~ 1 + History + full_b_av_lapse + (1 |
                                                               study_id),
                 data = P_Behav))
  
  P_Behav$threshold_diff = P_Behav$external_b_threshold - P_Behav$internal_b_threshold
  P_Behav$bias_diff = abs(P_Behav$external_b_bias) - abs(P_Behav$internal_b_bias)
  P_Behav$lapse_diff = P_Behav$external_b_av_lapse - P_Behav$internal_b_av_lapse
  
  STAT.PM_diff_bias <-
    summary(lmer(bias_diff ~ 1 + lapse_diff + threshold_diff + (1 |
                                                                  study_id),
                 data = P_Behav))
  
  STAT.PM_diff_lapse <-
    summary(lmer(lapse_diff ~ 1 + bias_diff + threshold_diff + (1 |
                                                                  study_id),
                 data = P_Behav))
  
  STAT.PM_diff_threshold <-
    summary(lmer(threshold_diff ~ 1 + bias_diff + lapse_diff + (1 |
                                                                  study_id),
                 data = P_Behav))
  
    STAT.PM_zero_bias_b <-
    summary(lmer(full_b_bias ~ 1 + (1 | study_id), data = P_Behav))
    
    STAT.PM_zero_bias_0 <-
    summary(lmer(full_0_bias ~ 1 + (1 | study_id), data = P_Behav))
    
    STAT.PM_zero_bias_1 <-
    summary(lmer(full_0_bias ~ 1 + (1 | study_id), data = P_Behav))
    
    
  STAT.PM_diff_lapse_1_lower <- summary(lmer(external_1_lower_lapse - internal_1_lower_lapse ~ 1 + (1|study_id), data = P_Behav))
  STAT.PM_diff_lapse_0_lower <- summary(lmer(external_0_lower_lapse - internal_0_lower_lapse ~ 1 + (1|study_id), data = P_Behav))
  STAT.PM_diff_lapse_1_higher <- summary(lmer(external_1_higher_lapse - internal_1_higher_lapse ~ 1 + (1|study_id), data = P_Behav))
  STAT.PM_diff_lapse_0_higher <- summary(lmer(external_0_higher_lapse - internal_0_higher_lapse ~ 1 + (1|study_id), data = P_Behav))
  
  STAT.PM_diff_threshold_0_1 <- summary(lmer(full_0_threshold - full_1_threshold ~ 1 + (1|study_id), data = P_Behav))
  
##
## Generate parameter summary
##
gathercol = colnames(P_Behav[, seq(14, ncol(P_Behav) - 3)])
P_Behav_long  <-
  gather(P_Behav,
         "Variable",
         "Estimate",
         gathercol,
         factor_key = TRUE)

P_Behav_long$Conditioned = NA
P_Behav_long$Type = NA

P_Behav_long[grepl("full", P_Behav_long$Variable), ]$Type = "full"
P_Behav_long[grepl("internal", P_Behav_long$Variable), ]$Type = "internal"
P_Behav_long[grepl("external", P_Behav_long$Variable), ]$Type = "external"
P_Behav_long$Type <-
  factor(P_Behav_long$Type, levels = c("full", "internal", "external"))

P_Behav_long[grepl("_b", P_Behav_long$Variable), ]$Conditioned = "all"
P_Behav_long[grepl("_0", P_Behav_long$Variable), ]$Conditioned = "y(t-1) = 0"
P_Behav_long[grepl("_1", P_Behav_long$Variable), ]$Conditioned = "y(t-1) = 1"

P_Behav_long$Variable_full = P_Behav_long$Variable
P_Behav_long$Variable <- gsub(c("full_"), "", P_Behav_long$Variable)
P_Behav_long$Variable <-
  gsub("internal_", "", P_Behav_long$Variable)
P_Behav_long$Variable <-
  gsub("external_", "", P_Behav_long$Variable)
P_Behav_long$Variable <- gsub("b_", "", P_Behav_long$Variable)
P_Behav_long$Variable <- gsub("1_", "", P_Behav_long$Variable)
P_Behav_long$Variable <- gsub("0_", "", P_Behav_long$Variable)

##
## simulate
##
Sim_PM = data.frame()
stim_range = seq(-5,+5, 0.25)
for (subj_idx in unique(P_Behav$subject_id)) {
  print(subj_idx)
  for (type_idx in type_list) {
  par = P_Behav_long[P_Behav_long$subject_id == subj_idx & grepl(type_idx, P_Behav_long$Variable_full), ]$Estimate[c(1, 2, 4, 5)]
  lower_lapse = par[1]
  higher_lapse = par[2]
  bias = par[3]
  contrast_threshold = par[4]
  
  Sim_PM = rbind(
    Sim_PM,
    add_sim_PM = data.frame(
      subject_id = subj_idx,
      study_id = unique(P_Behav[P_Behav$subject_id == subj_idx,]$study_id),
      Stimulus = stim_range,
      y_prob = lower_lapse + (1 - lower_lapse - higher_lapse) *  (erf((stim_range - bias) / contrast_threshold) + 1) / 2,
      Variable = type_idx
    )
)
}
}

Sim_PM$Conditioned = NA
Sim_PM$Type = NA

Sim_PM[grepl("full", Sim_PM$Variable), ]$Type = "full"
Sim_PM[grepl("internal", Sim_PM$Variable), ]$Type = "internal"
Sim_PM[grepl("external", Sim_PM$Variable), ]$Type = "external"
Sim_PM$Type <-
  factor(Sim_PM$Type, levels = c("full", "internal", "external"))


Sim_PM[grepl("_b", Sim_PM$Variable), ]$Conditioned = "all"
Sim_PM[grepl("_0", Sim_PM$Variable), ]$Conditioned = "y(t-1) = 0"
Sim_PM[grepl("_1", Sim_PM$Variable), ]$Conditioned = "y(t-1) = 1"

Sim_PM$Conditioned <- as.factor(Sim_PM$Conditioned)
Sim_PM$subject_id <- as.factor(Sim_PM$subject_id)

Sim_PM_group <-
    ddply(
    Sim_PM,
    .(Stimulus, Conditioned, Type),
    summarise,
    mean_y_prob = mean(y_prob, na.rm = TRUE),
    error_y_prob = sd(y_prob, na.rm = TRUE)/sqrt(length(y_prob))
  )

Sim_PM_group$Type <-
  factor(Sim_PM_group$Type, levels = c("full", "internal", "external"))
  

  if (save_summary_data) {
    save(
      P_Behav,
      STAT.PM_History_lapse_bias_threshold,
      STAT.PM_Accuracy_History_lapse,
      STAT.PM_diff_threshold,
      STAT.PM_diff_lapse,
      STAT.PM_diff_bias,
      STAT.PM_zero_bias_b,
      STAT.PM_zero_bias_0,
      STAT.PM_zero_bias_1,
      STAT.PM_diff_lapse_1_lower,
      STAT.PM_diff_lapse_0_lower,
      STAT.PM_diff_lapse_1_higher,
      STAT.PM_diff_lapse_0_higher,
      STAT.PM_diff_threshold_0_1,
      P_Behav_long,
      Sim_PM,
      Sim_PM_group,
      file = "./Summary_Data/psychometric_data.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/psychometric_data.Rdata")
}

```

Across all data provided by the Confidence database[@Rahnev2020] (i.e., irrespective of the preceding perceptual choice $y_{t-1}$), biases $\mu$ were distributed around zero (`r mean(P_Behav$full_b_bias, na.rm = TRUE)` ± `r sd(P_Behav$full_b_bias, na.rm = TRUE)/sqrt(length(P_Behav$full_b_bias))`; $\beta_0$ = $`r STAT.PM_zero_bias_b$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_b$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_b$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_b$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_b$coefficients[1,5]`$; Supplemental Figure 6A-B, upper panel). 
When conditioned on perceptual history, biases $\mu$ varied according to the preceding perceptual choice, with negative biases for $y_{t-1} = 0$ (`r -mean(P_Behav$full_0_bias, na.rm = TRUE)` ± `r sd(P_Behav$full_0_bias, na.rm = TRUE)/sqrt(length(P_Behav$full_0_bias))`; $\beta_0$ = $`r STAT.PM_zero_bias_0$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_0$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_0$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_0$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_0$coefficients[1,5]`$; Supplemental Figure 6A-B, upper panel) 
and positive biases for $y_{t-1} = 1$ (`r -mean(P_Behav$full_1_bias, na.rm = TRUE)` ± `r sd(P_Behav$full_1_bias, na.rm = TRUE)/sqrt(length(P_Behav$full_1_bias))`; $\beta_0$ = $`r STAT.PM_zero_bias_1$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_1$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_1$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_1$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_1$coefficients[1,5]`$; Supplemental Figure 6A-B, lower panel).
Absolute biases $|\mu|$ were larger in internal mode (`r mean(abs(P_Behav$internal_b_bias), na.rm = TRUE)` ± `r sd(abs(P_Behav$internal_b_bias), na.rm = TRUE)/sqrt(length(abs(P_Behav$internal_b_bias)))`) as compared to external mode (`r mean(abs(P_Behav$external_b_bias), na.rm = TRUE)` ± `r sd(abs(P_Behav$external_b_bias), na.rm = TRUE)/sqrt(length(abs(P_Behav$external_b_bias)))`; $\beta_0$ = $`r STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r STAT.PM_diff_bias$coefficients[1,2]`$, T($`r STAT.PM_diff_bias$coefficients[1,3]`$) = $`r STAT.PM_diff_bias$coefficients[1,4]`$, p = $`r STAT.PM_diff_bias$coefficients[1,5]`$; controlling for differences in lapses and thresholds). 

Lower and upper lapses amounted to $\gamma$ = $`r mean(P_Behav$full_b_lower_lapse, na.rm = TRUE)`$ ± $`r sd(P_Behav$full_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_Behav$full_b_higher_lapse))`$ and $\delta$ = $`r mean(P_Behav$full_b_higher_lapse, na.rm = TRUE)`$ ± $`r sd(P_Behav$full_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_Behav$full_b_higher_lapse))`$ (Supplemental Figure 6A, C and D). 
Lapses were larger in internal mode ($\gamma$ = $`r mean(P_Behav$internal_b_lower_lapse, na.rm = TRUE)`$ ± $`r sd(P_Behav$internal_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_Behav$internal_b_higher_lapse))`$, $\delta$ = $`r mean(P_Behav$internal_b_higher_lapse, na.rm = TRUE)`$ ± $`r sd(P_Behav$internal_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_Behav$internal_b_higher_lapse))`$) as compared to external mode ($\gamma$ = $`r mean(P_Behav$external_b_lower_lapse, na.rm = TRUE)`$ ± $`r sd(P_Behav$external_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_Behav$external_b_higher_lapse))`$, $\delta$ = $`r mean(P_Behav$external_b_higher_lapse, na.rm = TRUE)`$ ± $`r sd(P_Behav$external_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_Behav$external_b_higher_lapse))`$; $\beta_0$ = $`r STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse$coefficients[1,5]`$; controlling for differences in biases and thresholds). 

Conditioning on the previous perceptual choice revealed that the between-mode difference in lapse was not general, but depended on perceptual history: 
For $y_{t-1} = 0$, only higher lapses $\delta$ differed between internal and external mode ($\beta_0$ = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,5]`$), 
whereas lower lapses $\gamma$ did not ($\beta_0$ = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,5]`$). 
Vice versa, 
for $y_{t-1} = 1$, 
lower lapses $\gamma$ differed between internal and external mode ($\beta_0$ = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,5]`$), 
whereas higher lapses $\delta$ did not ($\beta_0$ = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,5]`$).

Thresholds $t$ were estimated at `r mean(P_Behav$full_b_threshold, na.rm = TRUE)` ± `r sd(P_Behav$full_b_threshold, na.rm = TRUE)/sqrt(length(P_Behav$full_b_threshold))` (Supplemental Figure 6A and E). Thresholds $t$ were larger in internal mode (`r mean(P_Behav$internal_b_threshold, na.rm = TRUE)` ± `r sd(P_Behav$internal_b_threshold, na.rm = TRUE)/sqrt(length(P_Behav$internal_b_threshold))`) as compared to external mode (`r mean(P_Behav$external_b_threshold, na.rm = TRUE)` ± `r sd(P_Behav$external_b_threshold, na.rm = TRUE)/sqrt(length(P_Behav$external_b_bias))`; $\beta_0$ = $`r STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold$coefficients[1,2]`$, T($`r STAT.PM_diff_threshold$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold$coefficients[1,5]`$; controlling for differences in biases and lapses). 
In contrast to the bias $\mu$ and the lapse rates $\gamma$ and $\delta$, thresholds $t$ were not modulated by perceptual history ($\beta_0$ = $`r STAT.PM_diff_threshold_0_1$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold_0_1$coefficients[1,2]`$, T($`r   STAT.PM_diff_threshold_0_1$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold_0_1$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold_0_1$coefficients[1,5]`$).

### Mice
```{r psychometric_function_mouse, cache = TRUE}
if (!load_summary_data) {
  
   N_PM <- ddply(
      MwData,
      .(Difficulty),
      summarise,
      n_trials = length(Difficulty))
  
   difficulty_levels = N_PM$Difficulty[N_PM$n_trials > 100]
   
  P_M_Behav = M_Behav
  
  type_list = c(
    "full_b",
    "full_0",
    "full_1",
    "external_b",
    "external_0",
    "external_1",
    "internal_b",
    "internal_0",
    "internal_1"
  )
  
  P_M_Behav = cbind(P_M_Behav, matrix(NA, ncol = length(type_list) * 5, nrow = nrow(P_M_Behav)))
  colnames(P_M_Behav) <-
    c(
      "subject_id",
      "Stimulus_History",
      "History",
      "Accuracy",
      "Bias",
      "null_History",
      "full_b_lower_lapse",
      "full_b_higher_lapse",
      "full_b_av_lapse",
      "full_b_bias",
      "full_b_threshold",
      "full_0_lower_lapse",
      "full_0_higher_lapse",
      "full_0_av_lapse",
      "full_0_bias",
      "full_0_threshold",
      "full_1_lower_lapse",
      "full_1_higher_lapse",
      "full_1_av_lapse",
      "full_1_bias",
      "full_1_threshold",
      "external_b_lower_lapse",
      "external_b_higher_lapse",
      "external_b_av_lapse",
      "external_b_bias",
      "external_b_threshold",
      "external_0_lower_lapse",
      "external_0_higher_lapse",
      "external_0_av_lapse",
      "external_0_bias",
      "external_0_threshold",
      "external_1_lower_lapse",
      "external_1_higher_lapse",
      "external_1_av_lapse",
      "external_1_bias",
      "external_1_threshold",
      "internal_b_lower_lapse",
      "internal_b_higher_lapse",
      "internal_b_av_lapse",
      "internal_b_bias",
      "internal_b_threshold",
      "internal_0_lower_lapse",
      "internal_0_higher_lapse",
      "internal_0_av_lapse",
      "internal_0_bias",
      "internal_0_threshold",
      "internal_1_lower_lapse",
      "internal_1_higher_lapse",
      "internal_1_av_lapse",
      "internal_1_bias",
      "internal_1_threshold"
    )
  
  for (subj_idx in unique(P_M_Behav$subject_id)) {
  print(subj_idx)
    
  for (type_idx in type_list){
   
   if (type_idx == "full_b") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels
   } else if (type_idx == "full_0") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 0
   } else if (type_idx == "full_1") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 1
   } else if (type_idx == "internal_b") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$scaled_directed_mode < 0
   } else if (type_idx == "internal_0") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 0 &
       MwData$scaled_directed_mode < 0
   } else if (type_idx == "internal_1") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 1 &
       MwData$scaled_directed_mode < 0
   } else if (type_idx == "external_b") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$scaled_directed_mode > 0
   } else if (type_idx == "external_0") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 0 &
       MwData$scaled_directed_mode > 0
   } else if (type_idx == "external_1") {
     trial_selector =
       MwData$subject_id == subj_idx &
       MwData$Difficulty %in% difficulty_levels &
       MwData$Response_minus_1 == 1 &
       MwData$scaled_directed_mode > 0 
   }
   
    Input_Data <- data.frame (
      Stimulus = MwData[trial_selector, ]$Stimulus,
      Difficulty = MwData[trial_selector, ]$Difficulty,
      Response = MwData[trial_selector, ]$Response,
      Response_minus_1 = MwData[trial_selector, ]$Response_minus_1
    )
   
    source("./Functions/fit_psychometric_function_mouse.R")
    par = c(0.0, 0.0, 0, 0.05)
    lower = c(0.0, 0.0,-0.5, 0.01)
    upper = c(0.25, 0.25, 0.5, 1.5)
    
    add_PM =  optimx(
      par,
      fit_psychometric_function_mouse,
      Input_Data = Input_Data,
      lower = lower,
      upper = upper,
      method ="L-BFGS-B"
    )
    
    P_M_Behav[P_M_Behav$subject_id == subj_idx,grepl(type_idx, colnames(P_M_Behav))] <- 
      c(add_PM$p1, add_PM$p2, mean(c(add_PM$p1, add_PM$p2)), add_PM$p3, add_PM$p4)
 }   
}
  
  ##
  ## STATS
  ##
  M_STAT.PM_History_lapse_bias_threshold <-
    summary(lm(
      History ~ 1 + full_b_av_lapse + abs(full_b_bias) + full_b_threshold,
      data = P_M_Behav
    ))
  
  M_STAT.PM_Accuracy_History_lapse <-
    summary(lm(
      Accuracy ~ 1 + History + full_b_av_lapse,
      data = P_M_Behav
    ))
  
    P_M_Behav$threshold_diff = P_M_Behav$external_b_threshold - P_M_Behav$internal_b_threshold
    P_M_Behav$bias_diff = abs(P_M_Behav$external_b_bias) - abs(P_M_Behav$internal_b_bias)
    P_M_Behav$lapse_diff = P_M_Behav$external_b_av_lapse - P_M_Behav$internal_b_av_lapse
    
  M_STAT.PM_diff_bias <-
    summary(lm(
      bias_diff ~ 1 + lapse_diff + threshold_diff,
      data = P_M_Behav
    ))
  
  M_STAT.PM_diff_lapse <-
    summary(lm(
      lapse_diff ~ 1 + bias_diff + threshold_diff,
      data = P_M_Behav
    ))
  
  M_STAT.PM_diff_threshold <-
    summary(lm(
      threshold_diff ~ 1 + bias_diff + lapse_diff,
      data = P_M_Behav
    ))
  
  M_STAT.PM_zero_bias_b = t.test(P_M_Behav$full_b_bias)
M_STAT.PM_zero_bias_1 = t.test(P_M_Behav$full_1_bias)
M_STAT.PM_zero_bias_0 = t.test(P_M_Behav$full_0_bias)


M_STAT.PM_diff_lapse_1_higher = t.test(P_M_Behav$external_1_higher_lapse - P_M_Behav$internal_1_higher_lapse)
M_STAT.PM_diff_lapse_0_higher = t.test(P_M_Behav$external_0_higher_lapse - P_M_Behav$internal_0_higher_lapse)
M_STAT.PM_diff_lapse_1_lower = t.test(P_M_Behav$external_1_lower_lapse - P_M_Behav$internal_1_lower_lapse)
M_STAT.PM_diff_lapse_0_lower = t.test(P_M_Behav$external_0_lower_lapse - P_M_Behav$internal_0_lower_lapse)

M_STAT.PM_diff_lapse_1_higher_vs_lower = t.test((P_M_Behav$external_1_higher_lapse - P_M_Behav$internal_1_higher_lapse) - (P_M_Behav$external_1_lower_lapse - P_M_Behav$internal_1_lower_lapse))

M_STAT.PM_diff_lapse_0_higher_vs_lower = t.test((P_M_Behav$external_0_higher_lapse - P_M_Behav$internal_0_higher_lapse) - (P_M_Behav$external_0_lower_lapse - P_M_Behav$internal_0_lower_lapse))

M_STAT.PM_diff_threshold_0_1 <- t.test(P_M_Behav$full_0_threshold - P_M_Behav$full_1_threshold)
  
##
## Generate parameter summary
##
gathercol = colnames(P_M_Behav[, seq(7, ncol(P_M_Behav) - 3)])
P_M_Behav_long  <-
  gather(P_M_Behav,
         "Variable",
         "Estimate",
         gathercol,
         factor_key = TRUE)

P_M_Behav_long$Conditioned = NA
P_M_Behav_long$Type = NA

P_M_Behav_long[grepl("full", P_M_Behav_long$Variable), ]$Type = "full"
P_M_Behav_long[grepl("internal", P_M_Behav_long$Variable), ]$Type = "internal"
P_M_Behav_long[grepl("external", P_M_Behav_long$Variable), ]$Type = "external"
P_M_Behav_long$Type <-
  factor(P_M_Behav_long$Type, levels = c("full", "internal", "external"))


P_M_Behav_long[grepl("_b", P_M_Behav_long$Variable), ]$Conditioned = "all"
P_M_Behav_long[grepl("_0", P_M_Behav_long$Variable), ]$Conditioned = "y(t-1) = 0"
P_M_Behav_long[grepl("_1", P_M_Behav_long$Variable), ]$Conditioned = "y(t-1) = 1"

P_M_Behav_long$Variable_full = P_M_Behav_long$Variable
P_M_Behav_long$Variable <- gsub(c("full_"), "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("internal_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("external_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("b_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("1_", "", P_M_Behav_long$Variable)
P_M_Behav_long$Variable <- gsub("0_", "", P_M_Behav_long$Variable)

##
## simulate
##
Sim_PM_M = data.frame()
stim_range = seq(-1,+1, 0.01)
for (subj_idx in unique(P_M_Behav$subject_id)) {
  print(subj_idx)
  for (type_idx in type_list) {
  par = P_M_Behav_long[P_M_Behav_long$subject_id == subj_idx & grepl(type_idx, P_M_Behav_long$Variable_full), ]$Estimate[c(1, 2, 4, 5)]
  lower_lapse = par[1]
  higher_lapse = par[2]
  bias = par[3]
  contrast_threshold = par[4]
  
  Sim_PM_M = rbind(
    Sim_PM_M,
    add_sim_PM = data.frame(
      subject_id = subj_idx,
      Stimulus = stim_range,
      y_prob = lower_lapse + (1 - lower_lapse - higher_lapse) *  (erf((stim_range - bias) / contrast_threshold) + 1) / 2,
      Variable = type_idx
    )
)
}
}

Sim_PM_M$Conditioned = NA
Sim_PM_M$Type = NA

Sim_PM_M[grepl("full", Sim_PM_M$Variable), ]$Type = "full"
Sim_PM_M[grepl("internal", Sim_PM_M$Variable), ]$Type = "internal"
Sim_PM_M[grepl("external", Sim_PM_M$Variable), ]$Type = "external"
Sim_PM_M$Type <-
  factor(Sim_PM_M$Type, levels = c("full", "internal", "external"))


Sim_PM_M[grepl("_b", Sim_PM_M$Variable), ]$Conditioned = "all"
Sim_PM_M[grepl("_0", Sim_PM_M$Variable), ]$Conditioned = "y(t-1) = 0"
Sim_PM_M[grepl("_1", Sim_PM_M$Variable), ]$Conditioned = "y(t-1) = 1"

Sim_PM_M$Conditioned <- as.factor(Sim_PM_M$Conditioned)
Sim_PM_M$subject_id <- as.factor(Sim_PM_M$subject_id)

Sim_PM_M_group <-
    ddply(
    Sim_PM_M,
    .(Stimulus, Conditioned, Type),
    summarise,
    mean_y_prob = mean(y_prob, na.rm = TRUE),
    error_y_prob = sd(y_prob, na.rm = TRUE)/sqrt(length(y_prob))
  )

Sim_PM_M_group$Type <-
  factor(Sim_PM_M_group$Type, levels = c("full", "internal", "external"))
  
  if (save_summary_data) {
    save(
      P_M_Behav,
      M_STAT.PM_History_lapse_bias_threshold,
      M_STAT.PM_Accuracy_History_lapse,
      M_STAT.PM_diff_threshold,
      M_STAT.PM_diff_lapse,
      M_STAT.PM_diff_bias,
      M_STAT.PM_zero_bias_b,
      M_STAT.PM_zero_bias_1,
      M_STAT.PM_zero_bias_0,
      M_STAT.PM_diff_lapse_1_higher,
      M_STAT.PM_diff_lapse_0_higher,
      M_STAT.PM_diff_lapse_1_lower,
      M_STAT.PM_diff_lapse_0_lower,
      M_STAT.PM_diff_lapse_1_higher_vs_lower,
      M_STAT.PM_diff_lapse_0_higher_vs_lower,
      M_STAT.PM_diff_threshold_0_1,
      P_M_Behav_long,
      Sim_PM_M,
      Sim_PM_M_group,
      file = "./Summary_Data/psychometric_data_mouse.Rdata"
    )
  }
  
} else {
  load("./Summary_Data/psychometric_data_mouse.Rdata")
}
```

When estimated based on the full dataset provided in the IBL database[@IBL2021] (i.e., irrespective of the preceding perceptual choice $y_{t-1}$), biases $\mu$ were distributed around zero ($`r mean(P_M_Behav$full_b_bias, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$full_b_bias, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_bias))`$; T(`r M_STAT.PM_zero_bias_b$parameter`) = `r M_STAT.PM_zero_bias_b$statistic`, p = $`r M_STAT.PM_zero_bias_b$p.value`$; Supplemental Figure 7A-B, upper panel). 
When conditioned on the preceding perceptual choice, biases were negative for $y_{t-1} = 0$ ($`r -mean(P_M_Behav$full_0_bias, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$full_0_bias, na.rm = TRUE)/sqrt(length(P_M_Behav$full_0_bias))`$; T($`r M_STAT.PM_zero_bias_0$parameter`) = `r -M_STAT.PM_zero_bias_0$statistic`$, p = $`r M_STAT.PM_zero_bias_0$p.value`$; Supplemental Figure 7A-B, middle panel) 
and positive for $y_{t-1} = 1$ ($`r -mean(P_M_Behav$full_1_bias, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$full_1_bias, na.rm = TRUE)/sqrt(length(P_M_Behav$full_1_bias))`$; T($`r M_STAT.PM_zero_bias_1$parameter`$) = $`r -M_STAT.PM_zero_bias_1$statistic`$, p = $`r M_STAT.PM_zero_bias_1$p.value`$; Supplemental Figure 7A-B, lower panel). 
As in humans, mice showed larger biases during internal mode ($`r mean(abs(P_M_Behav$internal_b_bias), na.rm = TRUE)`$ ± $`r sd(abs(P_M_Behav$internal_b_bias), na.rm = TRUE)/sqrt(length(abs(P_M_Behav$internal_b_bias)))`$) as compared to external mode ($`r mean(abs(P_M_Behav$external_b_bias), na.rm = TRUE)`$ ± $`r sd(abs(P_M_Behav$external_b_bias), na.rm = TRUE)/sqrt(length(abs(P_M_Behav$external_b_bias)))`$; $\beta_0$ = $`r M_STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_bias$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_bias$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_bias$coefficients[1,4]`$; controlling for differences in lapses and thresholds). 

Lower and upper lapses amounted to $\gamma$ = $`r mean(P_M_Behav$full_b_lower_lapse, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$full_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_higher_lapse))`$ and $\delta$ = $`r mean(P_M_Behav$full_b_higher_lapse, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$full_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_higher_lapse))`$ (Supplemental Figure 7A, C and D). 
Lapse rates were higher in internal mode ($\gamma$ = $`r mean(P_M_Behav$internal_b_lower_lapse, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$internal_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$internal_b_higher_lapse))`$, $\delta$ = $`r mean(P_M_Behav$internal_b_higher_lapse, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$internal_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$internal_b_higher_lapse))`$) as compared to external mode ($\gamma$ = $`r mean(P_M_Behav$external_b_lower_lapse, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$external_b_lower_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$external_b_higher_lapse))`$, $\delta$ = $`r mean(P_M_Behav$external_b_higher_lapse, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$external_b_higher_lapse, na.rm = TRUE)/sqrt(length(P_M_Behav$external_b_higher_lapse))`$; $\beta_0$ = $`r M_STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_lapse$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_lapse$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_lapse$coefficients[1,4]`$; controlling for differences in biases and thresholds). 

For $y_{t-1} = 0$, the difference between internal and external mode was more pronounced for higher lapses $\delta$ (T(`r M_STAT.PM_diff_lapse_1_higher_vs_lower$parameter`) = $`r M_STAT.PM_diff_lapse_1_higher_vs_lower$statistic`$, p = $`r M_STAT.PM_diff_lapse_1_higher_vs_lower$p.value`$). 
Conversely, for $y_{t-1} = 1$, the difference between internal and external mode was more pronounced for lower lapses $\gamma$ (T($`r M_STAT.PM_diff_lapse_0_higher_vs_lower$parameter`$) = $`r M_STAT.PM_diff_lapse_0_higher_vs_lower$statistic`$, p = $`r M_STAT.PM_diff_lapse_0_higher_vs_lower$p.value`$). 
In contrast to the human data, higher lapses $\delta$ and lower lapses $\gamma$ were significantly elevated during internal mode irrespective of the preceding perceptual choice 
(higher lapses $\delta$ for $y_{t-1} = 1$: T(`r M_STAT.PM_diff_lapse_1_higher$parameter`) = `r M_STAT.PM_diff_lapse_1_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher$p.value`$;
higher lapses $\delta$ for $y_{t-1} = 0$: T(`r M_STAT.PM_diff_lapse_0_higher$parameter`) = `r M_STAT.PM_diff_lapse_0_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher$p.value`$;
lower lapses $\gamma$ for $y_{t-1} = 1$: T(`r M_STAT.PM_diff_lapse_1_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_lower$p.value`$;
lower lapses $\gamma$ for $y_{t-1} = 0$: T(`r M_STAT.PM_diff_lapse_0_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_lower$p.value`$).

In mice, thresholds $t$ amounted to $`r mean(P_M_Behav$full_b_threshold, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$full_b_threshold, na.rm = TRUE)/sqrt(length(P_M_Behav$full_b_threshold))`$ (Supplemental Figure 7A and E) and were higher in internal mode ($`r mean(P_M_Behav$internal_b_threshold, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$internal_b_threshold, na.rm = TRUE)/sqrt(length(P_M_Behav$internal_b_threshold))`$) as compared to external mode ($`r mean(P_M_Behav$external_b_threshold, na.rm = TRUE)`$ ± $`r sd(P_M_Behav$external_b_threshold, na.rm = TRUE)/sqrt(length(P_M_Behav$external_b_threshold))`$; $\beta_0$ = $`r M_STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_threshold$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_threshold$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_threshold$coefficients[1,4]`$; controlling for differences in biases and lapses).
Thresholds $t$ were not modulated by perceptual history (T(`r M_STAT.PM_diff_threshold_0_1$parameter`) = `r M_STAT.PM_diff_threshold_0_1$statistic`, p = $`r M_STAT.PM_diff_threshold_0_1$p.value`$).

In sum, the above analyses showed that, in both humans and mice, internal and external mode differ with respect to biases, lapses and thresholds. Internally-biased processing was characterized by higher thresholds, indicating a reduced sensitivity to sensory information, as well as by larger biases and lapses. Importantly, between-mode differences in biases and lapses strongly depended on perceptual history. This confirmed that internal mode processing cannot be explained solely on the ground of a general (i.e., history-independent) increase in lapses or bias indicative of random of stereotypical responses. 

## Internal mode processing can not be reduced to insufficient task familiarity

It may be assumed that participants tend to repeat preceding choices when they are not yet familiar with the experimental task, leading to history-congruent choices that are caused by insufficient training. To assess this alternative explanation, we contrasted the correlates of bimodal inference with training effects in humans and mice.

### Humans
```{r human_training, cache = TRUE}
if (!load_summary_data) {
  
training_RT <- lmer(clear_RT ~ Trial + (1 |study_id/subject_id), data = PwData)
STAT.training_RT <- summary(training_RT)

if (compute_training_history){
training_History <- glmer(History ~ Trial + (1|study_id/subject_id),
        data = PwData,
        family = binomial)
} else {
 training_History <- readRDS(paste(root, "training_history.rds", sep = "")) 
}
STAT.training_History <- summary(training_History)  
  
  if (save_summary_data) {
    save(STAT.training_RT, STAT.training_History,  file = "./Summary_Data/training_human.Rdata")
  }
  
} else {
  load("./Summary_Data/training_human.Rdata")
}

```

In the Confidence database[@Rahnev2020], training effects were visible from RTs that were shortened by increasing exposure to the task ($\beta$ = $`r STAT.training_RT$coefficients[2,1]`$ ± $`r STAT.training_RT$coefficients[2,2]`$, T($`r STAT.training_RT$coefficients[2,3]`$) = $`r STAT.training_RT$coefficients[2,4]`$, p = $`r STAT.training_RT$coefficients[2,5]`$). 
Intriguingly, however, history-congruent choices became more frequent with increased exposure to the task ($\beta$ = $`r STAT.training_History$coefficients[2,1]`$ ± $`r STAT.training_History$coefficients[2,2]`$, z = $`r STAT.training_History$coefficients[2,3]`$, p = $`r STAT.training_History$coefficients[2,4]`$), speaking against the proposition that insufficient training induces seriality in response behavior.

### Mice
```{r mouse_behavior_during_training, cache = TRUE}

if (!load_summary_data) {
  
  if (compute_pretraining_data_mouse){
fully_trained <- unique(MwData$subject_id)
Global_MwData <- read.csv(paste(root, "/Mouse/MwData_new_preproc_filter_slider.csv", sep = ""))

Global_MwData$clear_RT = Global_MwData$RT
Global_MwData$clear_RT[Global_MwData$clear_RT > median(Global_MwData$clear_RT, na.rm = TRUE) + 3*median(Global_MwData$clear_RT, na.rm =TRUE)] = NA
Global_MwData$clear_RT <- Global_MwData$clear_RT*1000

Global_MwData <- Global_MwData[which(Global_MwData$subject_id %in% fully_trained),]

M_Behav_training <-
  ddply(
    Global_MwData,
    .(subject_id, session_id),
    summarise,
    Stimulus_History = mean(Stimulus_History, na.rm = TRUE)*100,
    History = mean(History, na.rm = TRUE)*100,
    Accuracy = mean(Accuracy, na.rm = TRUE)*100,
    Imbalance = mean(abs(0.5-prob_left), na.rm = TRUE)*100 + 50,
    RT = mean(clear_RT, na.rm = TRUE)
  )
} else {
  M_Behav_training <- read.csv("./Results/Mouse_Behav_training.csv")
}

gathercol = colnames(M_Behav_training[, c(4,5,7)])
M_Behav_training_long   <-
gather(M_Behav_training[M_Behav_training$Imbalance < 60,c(1,2,4,5,7)],
"Variable",
"Frequency",
gathercol,
factor_key = TRUE)

M_Behav_training_long$Variable <-
gsub("Accuracy", "Stimulus", M_Behav_training_long$Variable)

Summary_M_Behav_training_long <-   ddply(
    M_Behav_training_long ,
    .(session_id, Variable),
    summarise,
    Mean = mean(Frequency, na.rm = TRUE),
    Error = sd(Frequency, na.rm = TRUE)/sqrt(length(Frequency))
  )

M_lmer_History_training <- lmer(History ~ session_id + (1|subject_id), data = M_Behav_training)
M_STAT.lmer_History_training <- summary(M_lmer_History_training)

M_lmer_Stimulus_training <- lmer(Accuracy ~ session_id + (1|subject_id), data = M_Behav_training)
M_STAT.lmer_Stimulus_training <- summary(M_lmer_Stimulus_training)

M_lmer_RT_training <- lmer(RT ~ session_id + (1|subject_id), data = M_Behav_training)
M_STAT.lmer_RT_training <- summary(M_lmer_RT_training)

M_training_RT <- lmer(clear_RT ~ Trial + (1|subject_id/session_id), data = MwData)
M_STAT.training_RT <- summary(M_training_RT)

if (compute_training_history){
M_training_History <- glmer(History ~ Trial + (1|subject_id/session_id),
        data = MwData,
        family = binomial)
} else {
 M_training_History <- readRDS(paste(root, "training_history.rds", sep = "")) 
}
M_STAT.training_History <- summary(M_training_History)
  
  if (save_summary_data) {
    save(M_Behav_training, 
         M_Behav_training_long,
         Summary_M_Behav_training_long,
         M_STAT.lmer_History_training,
         M_STAT.lmer_Stimulus_training,
         M_STAT.lmer_RT_training,
         M_STAT.training_RT,
         M_STAT.training_History,
         file = "./Summary_Data/mouse_behavior_training.Rdata")
  }
  
} else {
  load("./Summary_Data/mouse_behavior_training.Rdata")
}
```

As in humans, it is an important caveat to consider whether the observed serial dependencies in mice reflect a phenomenon of perceptual inference, or, alternatively, an unspecific strategy that occurs at the level of reporting behavior. We reasoned that, if mice indeed tended to repeat previous choices as a general response pattern, history effects should decrease during training of the perceptual task. We therefore analyzed how stimulus- and history-congruent perceptual choices evolved across sessions in mice that, by the end of training, achieved proficiency (i.e., stimulus-congruence $\geq$ 80%) in the *basic* task of the IBL dataset[@IBL2021].

Across sessions, we found that stimulus-congruent perceptual choices became more frequent ($\beta$ = $`r M_STAT.lmer_Stimulus_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_Stimulus_training$coefficients[2,2]`$, T($`r M_STAT.lmer_Stimulus_training$coefficients[2,3]`$) = $`r M_STAT.lmer_Stimulus_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$) and TDs were progressively shortened ($\beta$ = $`r M_STAT.lmer_RT_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_RT_training$coefficients[2,2]`$, T($`r M_STAT.lmer_RT_training$coefficients[2,3]`$) = $`r M_STAT.lmer_RT_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$).
Crucially, the frequency of history-congruent perceptual choices also increased during training ($\beta$ = $`r M_STAT.lmer_History_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_History_training$coefficients[2,2]`$, T($`r M_STAT.lmer_History_training$coefficients[2,3]`$) = $`r M_STAT.lmer_History_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_History_training$coefficients[2,5]`$; Supplemental Figure S8).  
Within individual session, longer task exposure was associated with an increase in history-congruence ($\beta$ = $`r M_STAT.training_History$coefficients[2,1]`$ ± $`r M_STAT.training_History$coefficients[2,2]`$, z = $`r M_STAT.training_History$coefficients[2,3]`$, p = $`r M_STAT.training_History$coefficients[2,4]`$) and a decrease in TDs ($\beta$ = $`r M_STAT.training_RT$coefficients[2,1]`$ ± $`r M_STAT.training_RT$coefficients[2,2]`$, T($`r M_STAT.training_RT$coefficients[2,3]`$) = $`r M_STAT.training_RT$coefficients[2,4]`$, p = $`r M_STAT.training_RT$coefficients[2,5]`$). In sum, these findings strongly argue against the proposition that mice show biases toward perceptual history due to an unspecific response strategy.

\newpage
## Supplemental Figure S1
```{r Supplemental_Figure_S1}

p_Accuracy <-
  ggplot() + geom_jitter(
    data = Behav,
    mapping = aes(x = as.factor(study_id), y = Accuracy, color = as.factor(study_id)),
    alpha = 0.1,
    width = 0.5,
    size = 0.05
  ) + geom_errorbar(
data = Study_Behav,
mapping = aes(
ymin = mean_Accuracy - error_Accuracy,
ymax = mean_Accuracy + error_Accuracy,
x = as.factor(study_id),
color = as.factor(study_id)
),
width = 0.75,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = "Experiments",  y = "Stimulus-congruence (%)", subtitle = "A")

p_study_Accuracy <-
ggplot(Behav, aes(
x = Accuracy,
y = study_id,
group = study_id,
fill = study_id
)) + geom_density_ridges(scale = 6, alpha = 1, rel_min_height = 0, size = 0.25) + theme_ridges(font_size = 8,
grid = FALSE,
center_axis_labels = TRUE) + xlim(25, 100) + labs(x = NULL,  y =
"Experiments", subtitle = NULL) + theme_classic(base_size = 6) + theme(legend.position =
"none", axis.text.y = element_blank()) + geom_vline(xintercept = 50, color = "black", linetype = 2, size = 0.25) +  scale_x_continuous(name= NULL, breaks = c(50,100), limits=c(30, 100))


p_History <-
  ggplot() + geom_jitter(
    data = Behav,
    mapping = aes(x = as.factor(study_id), y = History, color = as.factor(study_id)),
    alpha = 0.1,
    width = 0.15,
    size = 0.05
  ) + geom_errorbar(
data = Study_Behav,
mapping = aes(
ymin = mean_History - error_History,
ymax = mean_History + error_History,
x = as.factor(study_id),
color = as.factor(study_id)
),
width = 0.75,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = "Experiments",  y = "History-congruence (%)", subtitle = "B")

p_study_History <-
ggplot(Behav, aes(
x = History,
y = study_id,
group = study_id,
fill = study_id
)) + geom_density_ridges(scale = 6, alpha = 1, rel_min_height = 0, size = 0.25) + theme_ridges(font_size = 8,
grid = FALSE,
center_axis_labels = TRUE) + xlim(25, 100) + labs(x =  NULL ,  y =
"Experiments", subtitle = NULL) + theme_classic(base_size = 6) + theme(legend.position =
"none", axis.text.y = element_blank()) + geom_vline(xintercept = 50, color = "black", linetype = 2, size = 0.25) +  scale_x_continuous(name= NULL, breaks = c(50,100), limits=c(30, 100))

d1 <- effect("Accuracy", Global_History_Accuracy, xlevels= list(Accuracy=seq(0, 100, 5)))
d1 <-as.data.frame(d1)
d1[,c(2,4,5)] <- 50 + d1[,c(2,4,5)]

Global_History_vs_Accuracy <- ggplot() + geom_line(data = d1, aes(x = Accuracy, y = fit), size = 0.2) + geom_ribbon(data = d1, aes(x = Accuracy, ymin =  lower, ymax = upper), color = "white", alpha = 0.2) +
  geom_point(data = Behav,  aes(x = Accuracy, y = History, color = study_id), shape = 20, size = 0.05, alpha = 0.3) +
  theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=8),
        axis.title.y = element_text(size=8), axis.text.y  = element_text(size=8)) +
  theme_classic(base_size = 6) + theme(legend.position =
"none") +  labs(x = "Stimulus-congruence (%)",  y =
"History-congruence (%)", subtitle = "C") + xlim(40, 100) + ylim(40,80) + geom_hline(
yintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) + geom_vline(
xintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
)

Model_History_vs_Accuracy <- plot_model(true_Global_History_Accuracy, type = "est", axis.lim = c(-0.05, 2.5), show.values = FALSE, show.p = FALSE, vline.color = "white", dot.size = 1, line.size = 0.1, transform = NULL)
Model_History_vs_Accuracy <- Model_History_vs_Accuracy + 
  theme_classic(base_size = 6) + labs(x = " ",  y =
"History-congruence ~ Stimulus-congruence (%)", subtitle = "E", title = NULL)  + scale_x_discrete(breaks=c("Accuracy"),
        labels=c(expression(paste(beta, "(S)"))))  + ylim(-0.22, 0.005) + geom_vline(
xintercept = 0,
linetype = "dashed",
color = "black",
size = 0.25
) +
  theme_classic(base_size = 6) 

M_STAT.Global_History_Accuracy <- cor.test(M_Behav$History,M_Behav$Accuracy)

M_Global_History_vs_Accuracy <- ggplot(data = M_Behav,  aes(x = Accuracy, y = History)) +
  geom_point(shape = 20, size = 0.05, alpha = 0.3) +
  theme(axis.title.x = element_text(size=8), axis.text.x  = element_text(size=8),
        axis.title.y = element_text(size=8), axis.text.y  = element_text(size=8)) +
  theme_classic(base_size = 6) + theme(legend.position =
"none") +  labs(x = "Stimulus-congruence (%)",  y =
"History-congruence (%)", subtitle = "D") + geom_hline(
yintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) + geom_vline(
xintercept = 50,
linetype = "dashed",
color = "black",
size = 0.25
) + geom_smooth(method = "lm", se =TRUE, color = "black", size = 0.2)

lay <- rbind(c(1, 2, 3, 4), c(5,5,6, 6))
grid.arrange(
p_Accuracy, p_study_Accuracy, p_History, p_study_History, 
Global_History_vs_Accuracy, M_Global_History_vs_Accuracy,
layout_matrix = lay,
heights = c(2,2),
widths = c(0.25,0.1,0.25, 0.1))
```

**Supplemental Figure S1. Stimulus- and history-congruence.** 

A. Stimulus-congruent choices in humans amounted to `r mean(Behav$Accuracy, na.rm = TRUE)`% ± `r sd(Behav$Accuracy, na.rm = TRUE)/sqrt(length(Behav$Accuracy))`% of trials and were highly consistent across the experiments selected from the Confidence Database. 

B. History-congruent choices in humans amounted to  `r mean(Behav$History, na.rm = TRUE)`% ± `r sd(Behav$History, na.rm = TRUE)/sqrt(length(Behav$History))`% of trials. In analogy to stimulus-congruence, the prevalence of history-congruence was highly consistent across the experiments selected from the Confidence Database. `r 100*sum(Study_Behav$p < 0.05 & Study_Behav$estimate > 0, na.rm = TRUE)/sum(!is.na(Study_Behav$study_id))`% of experiments showed significant (p < 0.05) biases toward preceding choices, whereas `r sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)` of the `r nrow(Study_Behav)` of the included experiments showed significant repelling biases.

C. In humans, we found an enhanced impact of perceptual history in participants who were less sensitive to external sensory information (T($`r STAT.Global_History_Accuracy$coefficients[2,3]`$) = $`r STAT.Global_History_Accuracy$coefficients[2,4]`$, p = $`r STAT.Global_History_Accuracy$coefficients[2,5]`$), suggesting that perception results from the competition of external with internal information. 

D. In analogy to humans, mice that were less sensitive to external sensory information showed stronger biases toward perceptual history (T(`r M_STAT.Global_History_Accuracy$parameter`) = `r M_STAT.Global_History_Accuracy$statistic`, p = $`r M_STAT.Global_History_Accuracy$p.value`$, Pearson correlation). 

\newpage
## Supplemental Figure S2
```{r Supplemental_Figure_S2}

# STAT.Phase_History <- summary(lm(C_history ~ levels, data = Phase_Bins))
# STAT.Phase_Stimulus <- summary(lm(C_stimulus ~ levels, data = Phase_Bins))
M_Summary_acf_2 <- read.csv("./Results/M_Summary_acf_session_corrected_normalized_3SD_new_preproc.csv") # M1

Summary_acf_2 <- read.csv("./Results/Summary_acf_study_corrected_normalized_3SD_HardEasy_External2.csv") ## extended_version: hardeasy + external / MV1

##
## Human Data
## 

acf_to_plot = rbind(c("diff_acf_Difficulty", "diff_acf_External"), 
                c("./Results/Summary_acf_study_Difficulty2_corrected_normalized_3SD.csv", "./Results/Summary_acf_study_External2_corrected_normalized_3SD.csv"),
                c("A           Difficulty (Humans)", "B           External stimulation (Humans)"))

for (plot_idx in c(1:ncol(acf_to_plot))){
 
##
## Acf Plot
##
Plot_Acf_Data =  Summary_acf_2[Summary_acf_2$Variable == acf_to_plot[1,plot_idx],]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_HardEasy", "HardEasy", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_External", "External", Plot_Acf_Data$Variable)

y_min = -0.05
p_acf <- ggplot() +
  geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean
    ), color = "#4DAF4A",linetype = "dotted") + 
    
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean), color = "#4DAF4A",
    alpha = 0.8,
    size = 0.5,
    fill = "white"
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.02),
          fill = "#4DAF4A",
      color = "#4DAF4A",
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error
    ),  color = "#4DAF4A", width = 0.5, alpha = 0.5, size = 0.5) +
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = NULL,  y =
paste("Autocorr coef."), subtitle = acf_to_plot[3,plot_idx]) + ylim(-0.05, 0.08) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

assign(paste("plot", acf_to_plot[1,plot_idx], sep = "_"), p_acf)  


Summary_acf_Corrected <- read.csv(acf_to_plot[2,plot_idx])
Summary_acf_Corrected$Trial <- Summary_acf_Corrected$Trial - 1
##
## Acf Plot
##
Plot_Acf_Data =  Summary_acf_Corrected[Summary_acf_Corrected$Variable == "diff_acf_Stimulus" | Summary_acf_Corrected$Variable == "diff_acf_History" ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

y_min = -0.005
p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.005,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") + 
   
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocorr. coef."), subtitle = " ", color = "Congruence to", shape = "p value") + ylim(-0.01, 0.03)  + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

assign(paste("plot_controlled", acf_to_plot[1,plot_idx], sep = "_"), p_acf)
 assign(paste("Summary_acf_Corrected", acf_to_plot[1,plot_idx], sep = "_"), Summary_acf_Corrected)
}

##
## murine Data
## 

acf_to_plot = rbind(c("diff_acf_Difficulty", "diff_acf_External"), 
                c("./Results/M_Summary_acf_session_Difficulty_corrected_normalized_3SD_new_preproc.csv", "./Results/M_Summary_acf_session_External_corrected_normalized_3SD_new_preproc.csv"),
                c("C           Difficulty (Mice)", "D           External stimulation (Mice)"))

for (plot_idx in c(1:ncol(acf_to_plot))){
 
##
## Acf Plot
##
Plot_Acf_Data =  M_Summary_acf_2[M_Summary_acf_2$Variable == acf_to_plot[1,plot_idx],]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_HardEasy", "HardEasy", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_External", "External", Plot_Acf_Data$Variable)

ylim = -0.01
p_acf <- ggplot() +
  geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean
    ),  color = "#4DAF4A", linetype = "dotted") + 
    
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      shape = p
    ),  color = "#4DAF4A",
    alpha = 0.8,
    size = 0.5,
    fill = "white"
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.01
  ),  fill = "#4DAF4A",
      color ="#4DAF4A",
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error
    ),  color = "#4DAF4A", width = 0.5, alpha = 0.5, size = 0.5) +
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = NULL,  y =
paste("Autocorr. coef."), subtitle = acf_to_plot[3,plot_idx]) + ylim(-0.02, 0.03) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

assign(paste("mouse_plot", acf_to_plot[1,plot_idx], sep = "_"), p_acf)  


M_Summary_acf_Corrected <- read.csv(acf_to_plot[2,plot_idx])
M_Summary_acf_Corrected$Trial <- M_Summary_acf_Corrected$Trial - 1
##
## Acf Plot
##
Plot_Acf_Data =  M_Summary_acf_Corrected[M_Summary_acf_Corrected$Variable == "diff_acf_Stimulus" | M_Summary_acf_Corrected$Variable == "diff_acf_History" ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

p_acf <- ggplot() +
  geom_point(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +  geom_point(data = Plot_Acf_Data[Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.005,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Plot_Acf_Data,
    aes(
      x = Trial,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.5, alpha = 0.5, size = 0.5) + geom_line(data = Plot_Acf_Data,
    aes(
      x = Trial,
      y = Mean,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), linetype = "dotted") +

  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Lag (Trials)",  y =
paste("Autocor. coef."), subtitle = " ", color = "Congruence to", shape = "p value") + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none", legend.box = "horizontal") +  scale_x_continuous(breaks = seq(1,25,by = 4), labels = seq(1,25,by = 4), limits=c(0, 25.5))

 assign(paste("mouse_plot_controlled", acf_to_plot[1,plot_idx], sep = "_"), p_acf)
 assign(paste("M_Summary_acf_Corrected", acf_to_plot[1,plot_idx], sep = "_"), M_Summary_acf_Corrected)
}

##
## Exceed Plot Humans
##

gathercol <- colnames(ID_Summary_acf[,c(3,4,5)])

ID_Summary_acf_long_control  <-
gather(ID_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

p_exceed_acf_Difficulty <- ggplot(ID_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())

gathercol <- colnames(ID_Summary_acf[,c(3,4,6)])

ID_Summary_acf_long_control  <-
gather(ID_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

p_exceed_acf_External <- ggplot(ID_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())


##
## Exceed Plot murines
##
gathercol <- colnames(ID_M_Summary_acf[,c(2,3,4)])

ID_M_Summary_acf_long_control  <-
gather(ID_M_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

mouse_p_exceed_acf_Difficulty <- ggplot(ID_M_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())

gathercol <- colnames(ID_M_Summary_acf[,c(2,3,5)])

ID_M_Summary_acf_long_control  <-
gather(ID_M_Summary_acf,
"Variable",
"Lag",
gathercol,
factor_key = TRUE)

mouse_p_exceed_acf_External <- ggplot(ID_M_Summary_acf_long_control, aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white",  position = "dodge", binwidth = 0.5, bins = 100, alpha = 1, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = NULL) +
    scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) #+ facet_wrap(~Variable, nrow = 1) + theme(strip.background = element_blank(), strip.text = element_blank())

lay <- rbind(c(1,3), c(2,4), c(9,10), c(5,7), c(6,8), c(11,12))
grid.arrange(
plot_diff_acf_Difficulty, plot_controlled_diff_acf_Difficulty, 
plot_diff_acf_External, plot_controlled_diff_acf_External,
mouse_plot_diff_acf_Difficulty, mouse_plot_controlled_diff_acf_Difficulty, 
mouse_plot_diff_acf_External, mouse_plot_controlled_diff_acf_External,
p_exceed_acf_Difficulty, p_exceed_acf_External,
mouse_p_exceed_acf_Difficulty, mouse_p_exceed_acf_External,
layout_matrix = lay,
heights = c(0.9,1.25,0.75,0.9,1.25,0.75),
widths = c(1,1))
```

**Supplemental Figure S2. Controlling for task difficulty and external stimulation.** 

In this study, we found highly significant autocorrelations of stimulus- and history-congruence in humans as well as in mice, while controlling for task difficulty and the sequence of external stimulation. Here, we confirm that the autocorrelations of stimulus- and history-congruence were not a trivial consequence of the experimental design or the addition of tast difficulty and external stimulation as control variables in the computation of group-level autocorrelations. 

A. In humans, task difficulty (in green) showed a significant autocorrelation starting at the `r min(which(Summary_acf_2[Summary_acf_2$Variable == "diff_acf_Difficulty",]$p == "< 0.05"))`th trial (upper panel, dots at the bottom indicate intercepts $\neq$ 0 in trial-wise linear mixed effects modeling at p < 0.05). 
When controlling for task difficulty only, linear mixed effects modeling indicated a significant autocorrelation of stimulus-congruence (in red) for the first `r min(which(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (middle panel). 
`r (sum(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus" & Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window remained significantly autocorrelated.
The autocorrelation of history-congruence (in blue) remained significant for the first `r min(which(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(Summary_acf_Corrected_diff_acf_Difficulty[Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History" & Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% significantly autocorrelated trials within the displayed time window).
At the level of individual participants, the autocorrelation of task difficulty exceeded the respective autocorrelation of randomly permuted within a lag of $`r mean(ID_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)`$ ± $`r sd(ID_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)/nrow(ID_Summary_acf)`$ trials (lower panel).

B. In humans, the sequence of external stimulation (i.e., which of the two binary outcomes was supported by the presented stimuli; depicted in green) was negatively autocorrelated for `r min(which(Summary_acf_2[Summary_acf_2$Variable == "diff_acf_External",]$p == "> 0.05")) - 1` trial. 
When controlling for the autocorrelation of external stimulation only, stimulus-congruence remained significantly autocorrelated for `r min(which(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus" & Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window; lower panel)
and history-congruence remained significantly autocorrelated for `r min(which(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(Summary_acf_Corrected_diff_acf_External[Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History" & Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window).
At the level of individual participants, the autocorrelation of external stimulation exceeded the respective autocorrelation of randomly permuted within a lag of $`r mean(ID_Summary_acf$lag_significant_External, na.rm = TRUE)`$ ± $`r sd(ID_Summary_acf$lag_significant_External, na.rm = TRUE)/nrow(ID_Summary_acf)`$ consecutive trials (lower panel).

C. In mice, task difficulty showed a significant autocorrelated for the first `r min(which(M_Summary_acf_2[M_Summary_acf_2$Variable == "diff_acf_Difficulty",]$p == "> 0.05")) - 1` consecutive trials (upper panel). 
When controlling only for task difficulty only, linear mixed effects modeling indicated a significant autocorrelation of stimulus-congruence for the first `r min(which(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (middle panel). 
In total, `r (sum(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_Stimulus" & M_Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window remained significantly autocorrelated.
The autocorrelation of history-congruence remained significant for the first `r min(which(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials, with `r (sum(M_Summary_acf_Corrected_diff_acf_Difficulty[M_Summary_acf_Corrected_diff_acf_Difficulty$Variable == "diff_acf_History" & M_Summary_acf_Corrected_diff_acf_Difficulty$Trial <= 25,]$p == "< 0.05")/25)*100`% significantly autocorrelated trials within the displayed time window. 
At the level of individual mice, autocorrelation coefficients for difficulty were elevated above randomly permuted data within a lag of `r mean(ID_M_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)` ± `r sd(ID_M_Summary_acf$lag_significant_Difficulty, na.rm = TRUE)/nrow(ID_M_Summary_acf)` consecutive trials (lower panel).

D. In mice, the sequence of external stimulation (i.e., which of the two binary outcomes was supported by the presented stimuli) was negatively autocorrelated for `r min(which(M_Summary_acf_2[M_Summary_acf_2$Variable == "diff_acf_External",]$p == "> 0.05")) - 1` consecutive trials (upper panel). 
When controlling only for the autocorrelation of external stimulation, stimulus-congruence remained significantly autocorrelated for `r min(which(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_Stimulus" & M_Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window; middle)
and history-congruence remained significantly autocorrelated for `r min(which(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (`r (sum(M_Summary_acf_Corrected_diff_acf_External[M_Summary_acf_Corrected_diff_acf_External$Variable == "diff_acf_History" & M_Summary_acf_Corrected_diff_acf_External$Trial <= 25,]$p == "< 0.05")/25)*100`% of trials within the displayed time window).
At the level of individual mice, autocorrelation coefficients for external stimulation were elevated above randomly permuted data within a lag of $`r mean(ID_M_Summary_acf$lag_significant_External, na.rm = TRUE)`$ ± $`r sd(ID_M_Summary_acf$lag_significant_External, na.rm = TRUE)/nrow(ID_M_Summary_acf)`$ consecutive trials (lower panel).

\newpage
## Supplemental Figure S3
```{r Supplemental_Figure_S3}

y_min = -0.02
p_trialwise_logreg <- ggplot() +
  geom_point(
    data = Group_Tw_LogReg_long,
    aes(
      x = -lag,
      y = Mean,
      fill = Variable,
      color = Variable
    ),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  geom_errorbar(
    data = Group_Tw_LogReg_long,
    aes(
      x = -lag,
      ymin = Mean - Error,
      ymax = Mean + Error,
      color = Variable
    ),
    position = position_dodge(width = 0.5),
    width = 0.5,
    alpha = 0.5,
    size = 0.5
  ) +
  geom_point(
    data = Group_Tw_LogReg_long[Group_Tw_LogReg_long$p_value < 0.05, ],
    aes(
      x = -lag,
      y = y_min,
      fill = Variable,
      color = Variable
    ),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  
  scale_color_brewer(palette = "Set1", direction = -1) +
  
  geom_line(
    data = Group_Tw_LogReg_long,
    aes(x = -lag,
        y = Mean,
        color = Variable),
    position = position_dodge(width = 0.5),
    linetype = "dotted"
  ) +
  
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
    
  ) +  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(
    x = "Lag (Trials)",
    y = "Regression coefficient: Congruence",
    subtitle = "A",
    color = "Regression weight"
  ) + theme(legend.position = "top", legend.box = "horizontal")  +  scale_x_continuous(
    breaks = seq(-25, 0, by = 1),
    labels = seq(-25, 0, by = 1),
    limits = c(-25.5, 0)
  )

mouse_p_trialwise_logreg <- ggplot() +
  geom_point(
    data = M_Group_Tw_LogReg_long,
    aes(
      x = -lag,
      y = Mean,
      fill = Variable,
      color = Variable
    ),
    alpha = 0.8,
    size = 0.5,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  
  geom_errorbar(
    data = M_Group_Tw_LogReg_long,
    aes(
      x = -lag,
      ymin = Mean - Error,
      ymax = Mean + Error,
      color = Variable
    ),
    position = position_dodge(width = 0.5),
    width = 0.5,
    alpha = 0.5,
    size = 0.5
  ) + scale_color_brewer(palette = "Set1", direction = -1) +
  
  geom_line(
    data = M_Group_Tw_LogReg_long,
    aes(x = -lag,
        y = Mean,
        color = Variable),
    position = position_dodge(width = 0.5),
    linetype = "dotted"
  ) +
  
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
    
  ) +  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(
    x = "Lag (Trials)",
    y = "Regression coefficient: Congruence",
    subtitle = "B",
    color = "Congruence to"
  ) + theme(legend.position = "none", legend.box = "horizontal")  +  scale_x_continuous(
    breaks = seq(-25, 0, by = 1),
    labels = seq(-25, 0, by = 1),
    limits = c(-25.5, 0)
  )

lay <- rbind(c(1), c(2))
grid.arrange(
  p_trialwise_logreg,
  mouse_p_trialwise_logreg,
  layout_matrix = lay,
  heights = c(1, 1),
  widths = c(1)
)
```

**Supplemental Figure S3. Reproducing group-level autocorrelations using logistic regression.**

A. As an alternative to group-level autocorrelation coefficients, we used trial-wise logistic regression to quantify serial dependencies in stimulus- and history-congruence. This analysis predicted stimulus- and history-congruence at the index trial (trial $t = 0$, vertical line) based on stimulus- and history-congruence at the `r n_back` preceding trials. Mirroring the shape of the group-level autocorrelations, trial-wise regression coefficients (depicted as mean ± SEM, dots mark trials with regression weights significantly greater than zero at p < 0.05) increased toward the index trial $t = 0$ for the human data.

B. Following our results in human data, regression coefficients that predicted history-congruence at the index trial (trial t = 0, vertical line) increased exponentially for trials closer to the index trial in mice. In contrast to history-congruence, stimulus-congruence showed a negative regression weight (or autocorrelation coefficient; Figure 3B) at trial -2. This was due to the experimental design (see also the autocorrelations of difficulty and external stimulation in Supplemental Figure S2C and D): When mice made errors at easy trials (contrast $\geq$ 50%), the upcoming stimulus was shown at the same spatial location and at high contrast. This increased the probability of stimulus-congruent perceptual choices after stimulus-incongruent perceptual choices at easy trials, thereby creating a negative regression weight (or autocorrelation coefficient) of stimulus-congruence at trial -2. 

\newpage
## Supplemental Figure S4

```{r Supplemental_Figure_S4}

p_AIC <-
  ggplot() + geom_jitter(
    data = LogAggr,
    mapping = aes(x = as.factor(study_id), y = diff_AIC, color = study_id),
    alpha = 0.1,
    width = 0.15,
    size = 0.05
  ) + geom_errorbar(
data = Study_LogAggr,
mapping = aes(
ymin = mean_diff_AIC - error_diff_AIC,
ymax = mean_diff_AIC + error_diff_AIC,
x = as.factor(study_id),
color = study_id
),
width = 0.75,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none", axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = "Experiments",  y = "rel. AIC (Stim. only - Stim. + Hist.)", subtitle = "A") + ylim(-60, 60)

p_study_AIC <-
ggplot(LogAggr[!is.na(LogAggr$diff_AIC),], aes(
x = diff_AIC,
y = study_id,
group = study_id,
fill = study_id
)) + geom_density_ridges(scale = 6, alpha = 1, rel_min_height = 0, size = 0.25) + theme_ridges(font_size = 8,
grid = FALSE,
center_axis_labels = TRUE) + labs(x = NULL,  y =
"Experiments", subtitle = NULL) + theme_classic(base_size = 6) + theme(legend.position =
"none", axis.text.y = element_blank()) + geom_vline(xintercept = 0, color = "black", linetype = 2, size = 0.25)  + xlim(-10,50)

gathercol <- colnames(LogAggr[, c(6:7)])

LogAggr_long  <-
gather(LogAggr[, c(3,6:7)],
"Variable",
"beta",
gathercol,
factor_key = TRUE)

LogAggr_long$Variable <-
gsub("beta_History", "History", LogAggr_long$Variable)
LogAggr_long$Variable  <-
gsub("beta_Stimulus", "Stimulus",LogAggr_long$Variable)
LogAggr_long$Variable <- as.factor(LogAggr_long$Variable)

p_distribution_logit <- ggplot(LogAggr_long, aes(x= beta, color = Variable, fill = Variable)) + 
   geom_density(colour = "white", alpha=.6, bw = 0.10, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(-2.5 ,5) + ylim(0,0.8) + labs(x = "Logistic regression: Perceptual choice ~ Stimulus + History",  y = "Density", fill = expression(paste(beta, " for")), subtitle = "B") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
    geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "right")


Summary_M_LogAggr <- data.frame(
mean_diff_AIC = mean(M_LogAggr$diff_AIC, na.rm = TRUE),
error_diff_AIC = sd(M_LogAggr$diff_AIC, na.rm = TRUE)/sqrt(nrow(M_LogAggr))
)

mouse_p_AIC <-
  ggplot() + geom_jitter(
    data = M_LogAggr,
    mapping = aes(x = subject_id, y = -diff_AIC),
    alpha = 0.15,
    width = 0.05,
    size = 0.1
  ) + geom_errorbar(
data = Summary_M_LogAggr,
mapping = aes(
ymin = -mean_diff_AIC - error_diff_AIC,
ymax = -mean_diff_AIC + error_diff_AIC,
x = mean(M_LogAggr$subject_id)
),
width = 7.5,
position = position_dodge(width = 0.5),
size = 0.5
) +
theme_classic(base_size = 6) +
    geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme(legend.position = "none") + labs(x = "Mice",  y = "rel. AIC (Stim. only - Stim. + Hist.)", subtitle = "C")+ ylim(-500, 500)


gathercol <- colnames(M_LogAggr[, c(4:5)])

M_LogAggr_long  <-
gather(M_LogAggr[, c(1,4:5)],
"Variable",
"beta",
gathercol,
factor_key = TRUE)

M_LogAggr_long$Variable <-
gsub("beta_History", "History", M_LogAggr_long$Variable)

M_LogAggr_long$Variable  <-
gsub("beta_Stimulus", "Stimulus", M_LogAggr_long$Variable)

M_LogAggr_long$Variable <- as.factor(M_LogAggr_long$Variable)

mouse_p_distribution_logit <- ggplot(M_LogAggr_long, aes(x= beta, color = Variable, fill = Variable)) + 
   geom_density(colour = "white", alpha=.6, bw = 0.30, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(-2.5 ,10) + labs(x = "Logistic regression: Perceptual choice ~ Stimulus + History",  y = "Density", fill = expression(paste(beta, " for")), subtitle = "D") +
    scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
    geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none")

lay <- rbind(c(1,1, 2), c(3,3, 3), c(4,5, 5))
grid.arrange(
p_AIC, p_study_AIC, p_distribution_logit, mouse_p_AIC, mouse_p_distribution_logit, 
layout_matrix = lay,
heights = c(1,1,1),
widths = c(0.5, 0.5,0.25))
```

**Supplemental Figure S4. History-congruence in logistic regression.**

A. To ensure that perceptual history played a significant role in perception despite the ongoing stream of external information, we tested whether human perceptual decision-making was better explained by the combination of external and internal information or, alternatively, by external information alone. To this end, we compared AIC between logistic regression models that predicted trial-wise perceptual responses either by both current external sensory information and the preceding percept, or by external sensory information alone (values above zero indicate a superiority of the full model). With high consistency across the experiments selected from the Confidence Database, this model-comparison confirmed that perceptual history contributed significantly to perception (difference in AIC = `r mean(LogAggr$diff_AIC, na.rm = TRUE)`  ± `r sd(LogAggr$diff_AIC, na.rm = TRUE)/sqrt(length(LogAggr$diff_AIC))`, T($`r STAT.LogReg$coefficients[1,3]`$) = $`r STAT.LogReg$coefficients[1,4]`$, p = $`r STAT.LogReg$coefficients[1,5]`$).

B. Participant-wise regression coefficients amount to `r mean(LogAggr$beta_History, na.rm = TRUE)` ± `r sd(LogAggr$beta_History, na.rm = TRUE)/sqrt(length(LogAggr$beta_History))` for the effect of perceptual history and `r mean(LogAggr$beta_Stimulus, na.rm = TRUE)`  ± `r sd(LogAggr$beta_Stimulus, na.rm = TRUE)/sqrt(length(LogAggr$beta_Stimulus))` for external sensory stimulation. 

C. In mice, an AIC-based model comparison indicated that perception was better explained by logistic regression models that predicted trial-wise perceptual responses based on both current external sensory information and the preceding percept (difference in AIC = `r -mean(M_LogAggr$diff_AIC, na.rm = TRUE)`  ± `r sd(M_LogAggr$diff_AIC, na.rm = TRUE)/sqrt(length(M_LogAggr$diff_AIC))`, T($`r M_STAT.LogReg$parameter`$) = $`r M_STAT.LogReg$statistic`$, p = $`r M_STAT.LogReg$p.value`$).

D. In mice, individual regression coefficients amounted to `r mean(M_LogAggr$beta_History, na.rm = TRUE)` ± `r sd(M_LogAggr$beta_History, na.rm = TRUE)/sqrt(length(M_LogAggr$beta_History))` for the effect of perceptual history and `r mean(M_LogAggr$beta_Stimulus, na.rm = TRUE)`  ± `r sd(M_LogAggr$beta_Stimulus, na.rm = TRUE)/sqrt(length(M_LogAggr$beta_Stimulus))` for external sensory stimulation. 

\newpage
## Supplemental Figure S5

```{r Supplemental_Figure_S5, cache = TRUE}
##
## Visualize correction procedure
##
if (run_visualize_bias_sim){
source("./Functions/visualize_bias_correction.R",
local = knitr::knit_global())

n_trials = 100
levels = seq(0.6, 0.9, 0.1)
n_participants = 1000

Bias_Sim = data.frame()
for (level_idx in c(1:length(levels))) {
print(level_idx)
for (subj_idx in c(1:n_participants)) {
add_Bias_Sim <- visualize_bias_correction(n_trials, levels[level_idx])
add_Bias_Sim$subj_idx = subj_idx

Bias_Sim = rbind(Bias_Sim, add_Bias_Sim)
}
}

Bias_Sim$corrected <- Bias_Sim$acf - Bias_Sim$random


Sum_Bias_Sim <-  ddply(
Bias_Sim,
.(trial, bias_level),
summarise,

mean = mean(corrected, na.rm = TRUE),
error = sd(corrected, na.rm = TRUE) / sqrt(length(corrected))
)

Sum_Bias_Sim <- Sum_Bias_Sim[Sum_Bias_Sim$trial != 1,]
Sum_Bias_Sim$trial = Sum_Bias_Sim$trial-1
} else {Sum_Bias_Sim <- read.csv("./Results/Sum_Bias_Sim.csv")}

Sum_Bias_Sim$bias_level = paste("General response bias: ", as.character(Sum_Bias_Sim$bias_level), "%")

p_Sum_Bias_Sim <- ggplot() +
geom_errorbar(
data = Sum_Bias_Sim,
aes(
x = trial,
ymin = mean - error,
ymax = mean + error
),
width = 0.5,
alpha = 0.5,
size = 0.5,
color = "#377EB8"
) +
geom_line(
data = Sum_Bias_Sim,
aes(x = trial,
y = mean),
color = "#377EB8",
linetype = "dotted"
) +

geom_hline(
yintercept = 0,
linetype = "dashed",
color = "black",
size = 0.25
) +
theme_classic(base_size = 6) + labs(
x = "Lag (Trials)",
y =
paste("Autocorrelation coefficient: History congruence"),
subtitle = NULL
) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(
breaks = seq(1, 25, by = 4),
labels = seq(1, 25, by = 4),
limits = c(0, 25)
) + ylim(-0.01, 0.035) + facet_wrap( ~ bias_level)
p_Sum_Bias_Sim
```

**Supplemental Figure S5. Correcting for general response biases.**

Here, we ask whether the autocorrelation of history-congruence (as shown in Figure 2-3C) may be driven by general response biases (i.e., a general propensity to choose one of the two possible outcomes more frequently than the alternative). To this end, we generated sequences of `r max(Sum_Bias_Sim$trial) + 1` perceptual choices with general response biases ranging from 60 to 90% for 1000 simulated participants each. We then computed the autocorrelation of history-congruence for these simulated data. Crucially, we used the correction procedure that is applied to the autocorrelation curves shown in this manuscript: All reported autocorrelation coefficients are computed relative to the average autocorrelation coefficients obtained for `r n_permutations` iterations of randomly permuted trial sequences. The above simulation show that this correction procedure removes any potential contribution of general response biases to the autocorrelation of history-congruence. This indicates that the autocorrelation of history-congruence (as shown in Figure 2-3C) is not driven by general response biases that were present in the empirical data at a level of `r mean(Behav$Bias, na.rm = TRUE)`% ± `r sd(Behav$Bias, na.rm = TRUE)/sqrt(length(Behav$Bias))`% in humans and `r mean(M_Behav$Bias, na.rm = TRUE)`% ± `r sd(M_Behav$Bias, na.rm = TRUE)/sqrt(length(M_Behav$Bias))`% in mice. 

\newpage
## Supplemental Figure S6

```{r Supplemental_Figure_S6}
p_pm_estimates_lower_lapse <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("lower_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.06,
    size = 0.5
  ) + 
  labs(x = "Lower lapses",  y =
         "Density", linetype = "", subtitle = "C") +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_upper_lapse <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("higher_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.06,
    size = 0.5
  )  + 
  labs(x = "Higher lapses",  y =
         "Density", linetype = "", subtitle = "D") +
  theme_classic(base_size = 6) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)


p_pm_estimates_bias <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("bias"), ], aes(x = -Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.25,
    size = 0.5
  )  + 
  labs(x = "Bias",  y =
         "Density", linetype = "", subtitle = "B") +
  theme_classic(base_size = 6) + 
  xlim(-3, 3) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_threshold <-
  ggplot(P_Behav_long[P_Behav_long$Variable %in% c("threshold"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.6,
    size = 0.15
  ) + 
  labs(x = "Threshold",  y =
         "Density", linetype = "", subtitle = "E") +
  theme_classic(base_size = 6) + xlim(0, 10) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_simulation <-
  ggplot(data = Sim_PM_group, 
         aes(x = Stimulus, y = mean_y_prob, ymax = mean_y_prob + error_y_prob, ymin = mean_y_prob - error_y_prob, color = Type, fill = Type)) +
  geom_ribbon(
    color = "white",
    alpha = 0.25,
    size = 0.1
  ) +  geom_line(size = 0.1) +
  labs(x = "Stimulus",  y =
         "P (y = 1)", linetype = "", subtitle = "A") +
  theme_classic(base_size = 6) +
  xlim(-5, 5) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "left") + facet_wrap( ~ Conditioned, ncol = 1) 

lay <- rbind(c(5, 1, 2, 3, 4))
grid.arrange(
  p_pm_estimates_bias,
  p_pm_estimates_lower_lapse,
  p_pm_estimates_upper_lapse,
  p_pm_estimates_threshold,
  p_pm_simulation,
  layout_matrix = lay,
  heights = c(1),
  widths = c(3, 1, 1, 1, 1)
)




```

**Supplemental Figure S6. Full and history-conditioned psychometric functions across modes in humans.** 
 
A. Here, we show average psychometric functions for the full dataset (upper panel) and conditioned on perceptual history ($y_{t-1} = 1$ and $y_{t-1} = 0$; middle and lower panel) across modes (green line) and for internal mode (blue line) and external mode (red line) separately. 

B. Across the full dataset, biases $\mu$ were distributed around zero ($\beta_0$ = $`r STAT.PM_zero_bias_b$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_b$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_b$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_b$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_b$coefficients[1,5]`$; upper panel), 
with larger absolute biases $|\mu|$ for internal as compared to external mode ($\beta_0$ = $`r STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r STAT.PM_diff_bias$coefficients[1,2]`$, T($`r STAT.PM_diff_bias$coefficients[1,3]`$) = $`r STAT.PM_diff_bias$coefficients[1,4]`$, p = $`r STAT.PM_diff_bias$coefficients[1,5]`$; controlling for differences in lapses and thresholds).
When conditioned on perceptual history, we observed negative biases for $y_{t-1} = 0$ ($\beta_0$ = $`r STAT.PM_zero_bias_0$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_0$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_0$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_0$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_0$coefficients[1,5]`$; middle panel) 
and positive biases for $y_{t-1} = 1$ ($\beta_0$ = $`r STAT.PM_zero_bias_1$coefficients[1,1]`$ ± $`r STAT.PM_zero_bias_1$coefficients[1,2]`$, T($`r STAT.PM_zero_bias_1$coefficients[1,3]`$) = $`r STAT.PM_zero_bias_1$coefficients[1,4]`$, p = $`r STAT.PM_zero_bias_1$coefficients[1,5]`$; lower panel).

C. Lapse rates were higher in internal mode as compared to external mode ($\beta_0$ = $`r STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse$coefficients[1,5]`$; controlling for differences in biases and thresholds; see upper panel and subplot D). 
Importantly, the between-mode difference in lapses depended on perceptual history: We found no significant difference in lower lapses $\gamma$ for $y_{t-1} = 0$ ($\beta_0$ = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_lower$coefficients[1,5]`$; middle panel), 
but a significant difference for $y_{t-1} = 1$ ($\beta_0$ = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_lower$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_lower$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_lower$coefficients[1,5]`$; lower panel).

D. Conversely, higher lapses $\delta$ were significantly increased for $y_{t-1} = 0$ ($\beta_0$ = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_0_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_0_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_0_higher$coefficients[1,5]`$; middle panel), 
but not for $y_{t-1} = 1$ ($\beta_0$ = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,1]`$ ± $`r STAT.PM_diff_lapse_1_higher$coefficients[1,2]`$, T($`r STAT.PM_diff_lapse_1_higher$coefficients[1,3]`$) = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,4]`$, p = $`r STAT.PM_diff_lapse_1_higher$coefficients[1,5]`$; lower panel).

E. The thresholds $t$ were larger in internal as compared to external mode ($\beta_0$ = $`r STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold$coefficients[1,2]`$, T($`r STAT.PM_diff_threshold$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold$coefficients[1,5]`$; controlling for differences in biases and lapses) and were not modulated by perceptual history ($\beta_0$ = $`r STAT.PM_diff_threshold_0_1$coefficients[1,1]`$ ± $`r STAT.PM_diff_threshold_0_1$coefficients[1,2]`$, T($`r   STAT.PM_diff_threshold_0_1$coefficients[1,3]`$) = $`r STAT.PM_diff_threshold_0_1$coefficients[1,4]`$, p = $`r STAT.PM_diff_threshold_0_1$coefficients[1,5]`$).

\newpage
## Supplemental Figure S7

```{r Supplemental_Figure_S7}
p_pm_estimates_lower_lapse_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("lower_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.03,
    size = 0.5
  ) + 
  labs(x = "Lower lapses",  y =
         "Density", linetype = "", subtitle = "C") +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_upper_lapse_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("higher_lapse"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.03,
    size = 0.5
  )  + 
  labs(x = "Higher lapses",  y =
         "Density", linetype = "", subtitle = "D") +
  theme_classic(base_size = 6) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = "none") + scale_fill_brewer(palette = "Set1", direction = -1) + facet_wrap( ~ Conditioned, ncol = 1)


p_pm_estimates_bias_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("bias"), ], aes(x = -Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.05,
    size = 0.5
  )  + 
  labs(x = "Bias",  y =
         "Density", linetype = "", subtitle = "B") +
  theme_classic(base_size = 6) + 
  xlim(-0.5, 0.5) +
  theme_classic(base_size = 6) +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_estimates_threshold_mouse <-
  ggplot(P_M_Behav_long[P_M_Behav_long$Variable %in% c("threshold"), ], aes(x = Estimate, color = Type, fill = Type)) +
  geom_density(
    colour = "white",
    alpha = 0.3,
    bw = 0.1,
    size = 0.15
  ) + 
  labs(x = "Threshold",  y =
         "Density", linetype = "", subtitle = "E") +
  theme_classic(base_size = 6) + xlim(0, 1) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") + facet_wrap( ~ Conditioned, ncol = 1)

p_pm_simulation_mouse <-
  ggplot(data = Sim_PM_M_group, 
         aes(x = Stimulus, y = mean_y_prob, ymax = mean_y_prob + error_y_prob, ymin = mean_y_prob - error_y_prob, color = Type, fill = Type)) +
  geom_ribbon(
    color = "white",
    alpha = 0.25,
    size = 0.1
  ) +  geom_line(size = 0.1) +
  labs(x = "Stimulus",  y =
         "P (y = 1)", linetype = "", subtitle = "A") +
  theme_classic(base_size = 6) +
  xlim(-0.5, 0.5) +
  theme_classic(base_size = 6)  +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + theme(legend.position = "left") + facet_wrap( ~ Conditioned, ncol = 1) 

lay <- rbind(c(5, 1, 2, 3, 4))
grid.arrange(
  p_pm_estimates_bias_mouse,
  p_pm_estimates_lower_lapse_mouse,
  p_pm_estimates_upper_lapse_mouse,
  p_pm_estimates_threshold_mouse,
  p_pm_simulation_mouse,
  layout_matrix = lay,
  heights = c(1),
  widths = c(2, 1, 1, 1, 1)
)
```

**Supplemental Figure S7. Full and history-conditioned psychometric functions across modes in mice.** 
 
A. Here, we show average psychometric functions for the full IBL dataset (upper panel) and conditioned on perceptual history ($y_{t-1} = 1$ and $y_{t-1} = 0$; middle and lower panel) across modes (green line) and for internal mode (blue line) and external mode (red line) separately. 

B. Across the full dataset, biases $\mu$ were distributed around zero (T(`r M_STAT.PM_zero_bias_b$parameter`) = `r M_STAT.PM_zero_bias_b$statistic`, p = $`r M_STAT.PM_zero_bias_b$p.value`$; upper panel), 
with larger absolute biases $|\mu|$ for internal as compared to external mode ($\beta_0$ = $`r M_STAT.PM_diff_bias$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_bias$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_bias$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_bias$coefficients[1,4]`$; controlling for differences in lapses and thresholds).
When conditioned on perceptual history, we observed negative biases for $y_{t-1} = 0$ (T(`r M_STAT.PM_zero_bias_0$parameter`) = `r -M_STAT.PM_zero_bias_0$statistic`, p = $`r M_STAT.PM_zero_bias_0$p.value`$; middle panel) 
and positive biases for $y_{t-1} = 1$ (T(`r M_STAT.PM_zero_bias_1$parameter`) = `r -M_STAT.PM_zero_bias_1$statistic`, p = $`r M_STAT.PM_zero_bias_1$p.value`$; lower panel).

C. Lapse rates were higher in internal as compared to external mode ($\beta_0$ = $`r M_STAT.PM_diff_lapse$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_lapse$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_lapse$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_lapse$coefficients[1,4]`$; controlling for differences in biases and thresholds; upper panel, see subplot D). 
For $y_{t-1} = 1$, the difference between internal and external mode was more pronounced for lower lapses $\gamma$ (T(`r M_STAT.PM_diff_lapse_0_higher_vs_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_higher_vs_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher_vs_lower$p.value`$) as compared to higher lapses $\delta$ (see subplot D). 
In mice, lower lapses $\gamma$ were significantly elevated during internal mode irrespective of the preceding perceptual choice 
(middle panel: lower lapses $\gamma$ for $y_{t-1} = 0$; T(`r M_STAT.PM_diff_lapse_0_lower$parameter`) = `r M_STAT.PM_diff_lapse_0_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_0_lower$p.value`$,
lower panel: lower lapses $\gamma$ for $y_{t-1} = 1$; T(`r M_STAT.PM_diff_lapse_1_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_lower$p.value`$).

D. For $y_{t-1} = 0$, the difference between internal and external mode was more pronounced for higher lapses $\delta$ (T(`r M_STAT.PM_diff_lapse_1_higher_vs_lower$parameter`) = `r M_STAT.PM_diff_lapse_1_higher_vs_lower$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher_vs_lower$p.value`$, see subplot C). Higher lapses were significantly elevated during internal mode irrespective of the preceding perceptual choice 
(middle panel: higher lapses $\delta$ for $y_{t-1} = 0$; T(`r M_STAT.PM_diff_lapse_0_higher$parameter`) = `r M_STAT.PM_diff_lapse_0_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_0_higher$p.value`$
lower panel: higher lapses $\delta$ for $y_{t-1} = 1$; T(`r M_STAT.PM_diff_lapse_1_higher$parameter`) = `r M_STAT.PM_diff_lapse_1_higher$statistic`, p = $`r M_STAT.PM_diff_lapse_1_higher$p.value`$;
).

E. Thresholds $t$ were higher in internal as compared to external mode ($\beta_0$ = $`r M_STAT.PM_diff_threshold$coefficients[1,1]`$ ± $`r M_STAT.PM_diff_threshold$coefficients[1,2]`$, T = $`r M_STAT.PM_diff_threshold$coefficients[1,3]`$, p = $`r M_STAT.PM_diff_threshold$coefficients[1,4]`$; controlling for differences in biases and lapses) and were not modulated by perceptual history (T(`r M_STAT.PM_diff_threshold_0_1$parameter`) = `r M_STAT.PM_diff_threshold_0_1$statistic`, p = $`r M_STAT.PM_diff_threshold_0_1$p.value`$).

\newpage
## Supplemental Figure S8

```{r Supplemental_Figure_S8, cache = TRUE}
p_training_mouse <- ggplot() +
  geom_point(data = M_Behav_training_long[M_Behav_training_long$Variable != "RT",],
    aes(
      x = session_id,
      y = Frequency,
      fill = Variable,
      color = Variable),
    alpha = 0.1,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Summary_M_Behav_training_long[Summary_M_Behav_training_long$Variable != "RT",],
    aes(
      x =  session_id,
      ymin = Mean - Error,
      ymax = Mean + Error,
      fill = Variable,
      color = Variable), position = position_dodge(width = 0.5), width = 0.75, alpha = 1, size = 0.5) +
   
  geom_hline(
    yintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(x = "Session",  y = "Frequency", subtitle = NULL, color = "Congruence to") + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "top") + ylim(40, 100) + xlim(0,65) + facet_wrap(~Variable) + theme(strip.text.x = element_blank())

p_training_mouse_RT <- ggplot() +
  geom_point(data = M_Behav_training_long[M_Behav_training_long$Variable == "RT",],
    aes(
      x = session_id,
      y = Frequency),
    color = "#4DAF4A",
    alpha = 0.1,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) + 
  geom_errorbar(data = Summary_M_Behav_training_long[Summary_M_Behav_training_long$Variable == "RT",],
    aes(
      x =  session_id,
      ymin = Mean - Error,
      ymax = Mean + Error), color = "#4DAF4A", position = position_dodge(width = 0.5), width = 0.75, alpha = 1, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Session",  y = "TD (ms)", subtitle = NULL, color = "Congruence to") + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "top") + xlim(0,65) + ylim(1000,4000) + theme(strip.text.x = element_blank())

lay <- rbind(c(1,2))
grid.arrange(
p_training_mouse,p_training_mouse_RT,
layout_matrix = lay,
heights = c(1),
widths = c(2, 1))
```

**Supplemental Figure S8. History-/stimulus-congruence and TDs during training of the basic task.**

Here, we depict the progression of history- and stimulus-congruence (depicted in blue and red, respectively; left panel) as well as TDs (in green; right panel) across training sessions in mice that achieved proficiency (i.e., stimulus-congruence $\geq$ 80%) in the *basic* task of the IBL dataset.
We found that both history-congruent perceptual choices ($\beta$ = $`r M_STAT.lmer_History_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_History_training$coefficients[2,2]`$, T($`r M_STAT.lmer_History_training$coefficients[2,3]`$) = $`r M_STAT.lmer_History_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_History_training$coefficients[2,5]`$) 
and stimulus-congruent perceptual choices ($\beta$ = $`r M_STAT.lmer_Stimulus_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_Stimulus_training$coefficients[2,2]`$, T($`r M_STAT.lmer_Stimulus_training$coefficients[2,3]`$) = $`r M_STAT.lmer_Stimulus_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$) became more frequent with training. 
As in humans, mice showed shorter TDs with increased exposure to the task ($\beta$ = $`r M_STAT.lmer_RT_training$coefficients[2,1]`$ ± $`r M_STAT.lmer_RT_training$coefficients[2,2]`$, T($`r M_STAT.lmer_RT_training$coefficients[2,3]`$) = $`r M_STAT.lmer_RT_training$coefficients[2,4]`$, p = $`r M_STAT.lmer_Stimulus_training$coefficients[2,5]`$).

\newpage
## Supplemental Figure S9

```{r Supplemental_Figure_S9, cache = TRUE}
p_distribution_Sum_Subj_Model_AIC <-
  ggplot(Sum_Subj_Model_AIC,
         aes(
           x = round_any(AIC, 17.5),
           y = ..density..,
           color = type,
           fill = type
         )) +
  geom_histogram(position = "dodge", alpha = 0.5) +
  theme_classic(base_size = 6) + xlim(-1, 250) +
  labs(x = "AIC (subject-level)", y = "Density", fill = "Model", subtitle = "B") +
  scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +
  theme(legend.position = "none") + facet_wrap( ~ species, ncol = 2)

p_distribution_Sum_Model_AIC <-
  ggplot(Sum_AIC_Models[Sum_AIC_Models$type != "Model X: Only one amplitude", ],
         aes(
           y = type,
           x = AIC,
           color = type,
           fill = type
         )) +
  geom_col(alpha = 0.5, width = 0.1, linewidth = 0.1) +
  theme_classic(base_size = 6) + 
  scale_y_discrete(labels=c(" M1", " M2", " M3", " M4", " M5"))+
  labs(x = "AIC (group-level)", y = "Model", color = "Model", fill = "Model", subtitle = "A") +
  scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +
  theme(legend.position = "top") + facet_wrap( ~ species, ncol = 2)

lay <- rbind(c(1), c(2))
grid.arrange(
p_distribution_Sum_Model_AIC,
p_distribution_Sum_Subj_Model_AIC,
layout_matrix = lay,
heights = c(0.75,1),
widths = c(1))
```
**Supplemental Figure S9. Comparison of the bimodal inference model against reduced control models.** 

A. Group-level AIC. The bimodal inference model (M1) achieved the lowest AIC across the full model space ($AIC_1$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in mice). Model M2 ($AIC_2$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in mice) and Model M3 ($AIC_3$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in mice) incorporated only oscillations of either likelihood or prior precision. Model M4 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model M5 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information. 

B. Subject-level AIC. Here, we show the distribution of AIC values at the subject-level. AIC for the bimodal inference model tended to be smaller than AIC for the comparator models (statistical comparison to the second-best model M2 in humans: $\beta$ = $`r -STAT.M1_M2$coefficients[2,1]`$ ± $`r STAT.M1_M2$coefficients[2,2]`$, T($`r STAT.M1_M2$coefficients[2,3]`$) = $`r -STAT.M1_M2$coefficients[2,4]`$, p = $`r STAT.M1_M2$coefficients[2,5]`$; mice:  T($`r M_STAT.M1_M2$parameter`$) = `r M_STAT.M1_M2$statistic`, p = $`r M_STAT.M1_M2$p.value`$).

\newpage
## Supplemental Figure S10

```{r prepare_Supplemental_Figure_S10_11_12_13, cache = TRUE}

if (!load_summary_data) {
  
  ##
## run control simulation
##

if (run_control_simulation) {
  
n_participants = nrow(Optim)
n = runif(n_participants, 300, 700)#500

# outcomes
outcomes = c(0,1)

source("./Functions/simulation_glaze_osc_human_zeta.R", local = knitr::knit_global())

type_list = c("glaze_no_osc", "glaze_only_p_osc", "glaze_only_l_osc", "glaze_osc_no_H")

Control_Sim <- data.frame()
for (type_idx in c(4)){
for (subj_idx in c(1:n_participants)){

  if ((subj_idx %% 10) == 0){print(subj_idx)}
  
  random_steps = rnorm(n = n[subj_idx], mean = 0, sd = 0)
  
  if (type_list[type_idx] == "glaze_no_osc") {
  Control_Sim_add <-
  simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  Optim$p1[subj_idx],
  Optim$p2[subj_idx],
  0,
  0,
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )}
  else if (type_list[type_idx] == "glaze_only_l_osc") {
  Control_Sim_add <-
   simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  Optim$p1[subj_idx],
  Optim$p2[subj_idx],
  0,
  Optim$p4[subj_idx],
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )}
  else if (type_list[type_idx] == "glaze_only_p_osc") {
  Control_Sim_add <-
   simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  Optim$p1[subj_idx],
  Optim$p2[subj_idx],
  Optim$p3[subj_idx],
  0,
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )}
  else if (type_list[type_idx] == "glaze_osc_no_H") {
  Control_Sim_add <-
  simulation_glaze_osc_human_zeta(
  outcomes,
  n[subj_idx],
  0.5,
  Optim$p2[subj_idx],
  Optim$p3[subj_idx],
  Optim$p4[subj_idx],
  Optim$p5[subj_idx],
  Optim$p6[subj_idx],
  Optim$p7[subj_idx],
  random_steps,
  sliding_window,
  n_permutations
  )
  } else {
  }
  
  Control_Sim_add$diff_acf_Stimulus <- exclude_3SD(Control_Sim_add$acf_Stimulus - Control_Sim_add$random_acf_Stimulus) 
  Control_Sim_add$diff_acf_History <- exclude_3SD(Control_Sim_add$acf_History - Control_Sim_add$random_acf_History) 
  Control_Sim_add$subject_id = subj_idx
  Control_Sim_add$Trial = c(1:n[subj_idx])
  Control_Sim_add$H = Optim$p1[subj_idx]
  Control_Sim_add$prec = Optim$p2[subj_idx]
  Control_Sim_add$amp = Optim$p3[subj_idx]
  Control_Sim_add$amp_LLR = Optim$p4[subj_idx]
  Control_Sim_add$frequency = Optim$p5[subj_idx]
  Control_Sim_add$phase =  Optim$p6[subj_idx]
  Control_Sim_add$zeta =  Optim$p7[subj_idx]
  Control_Sim_add$random_steps = random_steps
  Control_Sim_add$type = type_list[type_idx]
  
  Control_Sim = rbind(Control_Sim, Control_Sim_add)
}
}

} else {
Control_Sim <- read.csv(paste(root, "Control_Sim_param_variable_n_human_zeta_corr_smaller.csv", sep = ""))
#Control_Sim <- read.csv(paste(root, "Control_Sim_param_variable_n_human.csv", sep = ""))
type_list = unique(Control_Sim$type)

}

Control_Sim_Behav <- 
  ddply(
    Control_Sim, 
    .(subject_id,type),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100, 
    # Min_History = min(History_slider, na.rm = TRUE)*100,
    # Max_History = max(History_slider, na.rm = TRUE)*100,
    Accuracy = sum(Accuracy, na.rm = TRUE)/length(Accuracy)*100
    # Min_Accuracy = min(Accuracy_slider, na.rm = TRUE)*100,
    # Max_Accuracy = max(Accuracy_slider, na.rm = TRUE)*100
  )

Control_Sim_Behav_diff <- 
   ddply(
    Control_Sim[!is.na(Control_Sim$Accuracy),], 
    .(subject_id, Accuracy, type),
    summarise,
    History = sum(History, na.rm = TRUE)/length(History)*100
  )

Control_Sim_Behav$null_History = Control_Sim_Behav$History - 50

if (compute_simulation_control_group_acf) {
source("./Functions/f_compute_sim_group_acf.R", local = knitr::knit_global())

acf_to_test = c('diff_acf_Stimulus', 'diff_acf_History')
max_trial = 99
Control_Sim_Summary_acf <- f_compute_sim_group_acf(Control_Sim, acf_to_test, max_trial, type_list)
} else {
#Control_Sim_Summary_acf <-  read.csv("./Results/Control_Sim_Summary_acf_param_variable_n_human_zeta.csv")
Control_Sim_Summary_acf <-  read.csv("./Results/Control_Sim_Summary_acf_param_variable_n_human_zeta_corr.csv")
}
Control_Sim_Summary_acf$Trial <- Control_Sim_Summary_acf$Trial - 1


Control_ID_Sim_Summary_acf <- ddply(
    Control_Sim, 
    .(subject_id,type),
    summarise,
    lag_significant_Stimulus = min(which(diff(which(c(0, exceed_acf_Stimulus,0) < n_permutations/2)) != 1)) - 1,
    lag_significant_History = min(which(diff(which(c(0, exceed_acf_History,0) < n_permutations/2)) != 1)) - 1
  )
Control_ID_Sim_Summary_acf$lag_significant_Stimulus[Control_ID_Sim_Summary_acf$lag_significant_Stimulus == Inf] = NA
Control_ID_Sim_Summary_acf$lag_significant_History[Control_ID_Sim_Summary_acf$lag_significant_History == Inf] = NA

if (compute_power_spectra_simulation) {
source("./Functions/f_compute_power_spectra_sim_control.R", local = knitr::knit_global())

sliders = c("Accuracy_slider", "History_slider")  
Control_Sim_Power_Spectra <- f_compute_power_spectra_sim_control(Control_Sim, sliders, type_list)  
} else {
  Control_Sim_Power_Spectra <- read.csv(paste(root, "Control_Sim_power_spectra_smoothed_param_variable_n_human_zeta_corr.csv", sep = ""))
}

Control_Sim_Power_Spectra$r_freq = round(Control_Sim_Power_Spectra$freq, digits = 4)

step_size = 0.01
bins <- seq(from = min(Control_Sim_Power_Spectra$freq), to = max(Control_Sim_Power_Spectra$freq), by = step_size)
names <- round(bins, digits = 2)
names = names[1:length(names) - 1]

Control_Sim_Power_Spectra$bin_freq <- cut(Control_Sim_Power_Spectra$freq, breaks = bins, labels = names)
Control_Sim_Power_Spectra$bin_freq <- as.numeric(Control_Sim_Power_Spectra$bin_freq) * step_size
Control_Sim_Power_Spectra$bin_freq[is.na(Control_Sim_Power_Spectra$bin_freq)] = max(Control_Sim_Power_Spectra$freq)

Control_Sim_Power_Spectra$Coherence <- Control_Sim_Power_Spectra$Coherence * 100



gathercol = colnames(Control_Sim_Power_Spectra[, c(2,3)])
Control_Sim_Power_Spectra_long  <-
gather(Control_Sim_Power_Spectra[, c(1,2,3, 6,7,8)],
"Variable",
"Power",
gathercol,
factor_key = TRUE)

Control_Sim_Power_Frequency <-  ddply(
      Control_Sim_Power_Spectra_long,
      .(r_freq, Variable, type),
      summarise,
      mean_power = mean(exclude_3SD((Power)), na.rm = TRUE),
      ci_power = qnorm(0.975) * sd(exclude_3SD(Power), na.rm = TRUE)/sqrt(length(exclude_3SD(Power)))
 )

Control_Sim_Coherence_Phase_Frequency <- ddply(
      Control_Sim_Power_Spectra,
      .(bin_freq,type),
      summarise,
    
      mean_coherence = mean(exclude_3SD((Coherence)), na.rm = TRUE),
      ci_coherence = qnorm(0.975) * sd(exclude_3SD(Coherence), na.rm = TRUE)/sqrt(length(exclude_3SD(Coherence))),
      
      mode_phase = getmode(exclude_3SD((abs(Phase)))),
      mean_phase = getmode(exclude_3SD((abs(Phase)))),
      ci_phase = qnorm(0.975) * sd(exclude_3SD(abs(Phase)), na.rm = TRUE)/sqrt(length(exclude_3SD(Phase)))
 )

##
## mode of coherence and phase
##
Control_Sim_Summary_Power_Spectra <- 
   ddply(
 Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$freq > 0.01 & Control_Sim_Power_Spectra$freq < 0.1,],
       .(subject_id, type),
       summarise,
       mean_coherence = mean(exclude_3SD((
 Coherence))),
       mode_phase = getmode(exclude_3SD((
 abs(Phase))))
   )

##
## Simulated modes
##
Control_Sim_Mode_gather <-  ddply(
  Control_Sim,
  .(subject_id),
  summarise,
  
  directed_mode = round((round(Accuracy_slider, digits = 1) - round(History_slider, digits = 1))*100, digits = 0),
  scaled_directed_mode = scale(directed_mode),
  strength_mode = abs(scaled_directed_mode)
)

Control_Sim$directed_mode = Control_Sim_Mode_gather$directed_mode
Control_Sim$scaled_directed_mode = Control_Sim_Mode_gather$scaled_directed_mode
Control_Sim$strength_mode = Control_Sim_Mode_gather$strength_mode

Control_Sim_ID_mode <-  ddply(
  Control_Sim,
  .(subject_id, type),
  summarise,
  strength_mode = mean(strength_mode, na.rm = TRUE),
  directed_mode = mean(directed_mode, na.rm = TRUE)
)


##
## Simulated confidence
##

Control_Sim$Confidence <- abs(Control_Sim$mu) 
Control_Sim$scaled_Confidence <- scale(Control_Sim$Confidence)
Control_Sim$clear_Confidence <- Control_Sim$Confidence
Control_Sim[Control_Sim$clear_Confidence > median(Control_Sim$clear_Confidence, na.rm = TRUE) + 3*median(Control_Sim$clear_Confidence, na.rm = TRUE),]$clear_Confidence <- NA

Control_Sim_Post_Perceptual_Modes = ddply(
 Control_Sim[!is.na(Control_Sim$History) & !is.na(Control_Sim$Accuracy) & !is.na(Control_Sim$clear_Confidence),], 
  .(directed_mode, type),
  summarise,
  average_Confidence = mean(clear_Confidence, na.rm = TRUE),
  se_Confidence = sd(clear_Confidence, na.rm = TRUE)/sqrt(length(clear_Confidence)),
  n = length(mu),
  n_percent = (sum(!is.na(mu))/nrow(Control_Sim))*100
)

##
## Confidence vs mode: Prepare plot and stats
##
Control_Sim_Confidence_Behav <- 
  ddply(
    Control_Sim[!is.na(Control_Sim$History) & !is.na(Control_Sim$Accuracy) & !is.na(Control_Sim$clear_Confidence),], 
    .(subject_id, type),
    summarise,
    diff_Confidence_History = mean(clear_Confidence[History == 1], na.rm = TRUE) -  mean(clear_Confidence[History == 0], na.rm = TRUE),
    diff_Confidence_Stimulus = mean(clear_Confidence[Accuracy == 1], na.rm = TRUE) -  mean(clear_Confidence[Accuracy == 0], na.rm = TRUE)
    )


gathercol = colnames(Control_Sim_Confidence_Behav[, c(3,4)])
Control_Sim_Confidence_Behav_long  <-
  gather(Control_Sim_Confidence_Behav,
         "Variable",
         "diff",
         gathercol,
         factor_key = TRUE)

Control_Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_History", "History", Control_Sim_Confidence_Behav_long$Variable)
Control_Sim_Confidence_Behav_long$Variable <-
  gsub("diff_Confidence_Stimulus", "Stimulus", Control_Sim_Confidence_Behav_long$Variable)

# Sim_diff_Confidence <- lmer(diff ~ Variable + (1|subject_id), data = Sim_Confidence_Behav_long)
# Sim_STAT.diff_Confidence <- summary(Sim_diff_Confidence)

## exclude low performance mice
Control_Summary_Sim_Confidence_Behav <- 
  ddply(
    Control_Sim_Confidence_Behav_long, 
    .(Variable),
    summarise,
    Mean = mean(diff, na.rm = TRUE),
    Error = sd(diff, na.rm = TRUE)/sqrt(length(diff)))

# Sim_lmer_Confidence_Accuracy_History <- lmer(clear_Confidence ~ Accuracy + History + (1|subject_id), data = Sim)
# Sim_STAT.lmer_Confidence_Accuracy_History <- summary(Sim_lmer_Confidence_Accuracy_History)



#
## Plots
##
for (type_idx in c(1:length(type_list))){

gathercol <- colnames(Control_Sim_Behav[,c(3,4)])

Control_Sim_Behav_long  <-
  gather(Control_Sim_Behav[, c(1,2,3,4)],
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Control_Sim_Behav_long$Frequency <-
  gsub("Accuracy", "Stimulus", Control_Sim_Behav_long$Frequency)

Control_sim_p_distribution_Behav <- ggplot(Control_Sim_Behav_long[Control_Sim_Behav_long$type == type_list[type_idx],], 
                                  aes(x = as.numeric(Frequency), color = Variable, fill = Variable)) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency (%)",  y = "Density", subtitle = "A") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_brewer(palette = "Set1", direction = -1) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "none") 

##
## Probability of History-congruence depending on perceptual accuracy
##

gathercol <- colnames(Control_Sim_Behav_diff[4])

Control_Sim_Behav_diff_long  <-
  gather(Control_Sim_Behav_diff,
         "Variable",
         "Frequency",
         gathercol,
         factor_key = TRUE)

Control_Sim_Behav_diff_long$Accuracy <- as.character(Control_Sim_Behav_diff_long$Accuracy) 
Control_Sim_Behav_diff_long$Accuracy <-
  gsub("0", "error", Control_Sim_Behav_diff_long$Accuracy)
Control_Sim_Behav_diff_long$Accuracy <-
  gsub("1", "correct", Control_Sim_Behav_diff_long$Accuracy)

Control_sim_p_distribution_Behav_diff <- ggplot(Control_Sim_Behav_diff_long[Control_Sim_Behav_diff_long$type == type_list[type_idx],], aes(x = Frequency, color = -Accuracy, fill = Accuracy)) + 
  # geom_histogram(aes(y=..density..), colour="white", fill="black", position="identity", binwidth = 0.25, bins = 100, alpha = 0.3) + 
  geom_density(colour = "white", alpha=.4, bw = 0.75, size = 0.5) +
  theme_classic(base_size = 6) +  xlim(25 , 100) + labs(x = "Frequency of History-congruence (%)",  y = "Density", subtitle = NULL, fill = "") +
  scale_color_brewer(palette = "Set1", direction = -1) + scale_fill_manual(values = c("#4292C6", "#084594")) + 
  geom_vline(
    xintercept = 50,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + theme(legend.position = "bottom")


##
## Acf Plot
##
Plot_Acf_Data =  Control_Sim_Summary_acf[(Control_Sim_Summary_acf$Variable == "diff_acf_Stimulus" | Control_Sim_Summary_acf$Variable == "diff_acf_History") ,]

Plot_Acf_Data$Variable <- gsub("diff_acf_Stimulus", "Stimulus", Plot_Acf_Data$Variable)
Plot_Acf_Data$Variable <- gsub("diff_acf_History", "History", Plot_Acf_Data$Variable)

# Plot_Acf_Data$type <- gsub("glaze_no_osc", "B. No oscillation", Plot_Acf_Data$type)
# Plot_Acf_Data$type <- gsub("glaze_only_l_osc", "D. No oscillation of the prior", Plot_Acf_Data$type)
# Plot_Acf_Data$type <- gsub("glaze_only_p_osc", "C. No oscillation of the likelihood", Plot_Acf_Data$type)
# Plot_Acf_Data$type <- gsub("glaze_osc_no_H", "A. No accumulation of evidence", Plot_Acf_Data$type)


Control_sim_p_acf <- ggplot() +
  geom_point(
  data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx],],
  aes(
  x = Trial,
  y = Mean,
  fill = Variable,
  color = Variable
  ),
  alpha = 0.8,
  size = 1,
  fill = "white",
  position = position_dodge(width = 0.5)
  ) + geom_point(data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx] & Plot_Acf_Data$p == "< 0.05",],
    aes(
      x = Trial,
      y = -0.01,
      fill = Variable,
      color = Variable),
    alpha = 0.8,
    size = 0.1,
    fill = "white",
    position = position_dodge(width = 0.5)
  ) +
  geom_errorbar(
  data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx],],
  aes(
  x = Trial,
  ymin = Mean - Error,
  ymax = Mean + Error,
  fill = Variable,
  color = Variable
  ),
  position = position_dodge(width = 0.5),
  width = 0.5,
  alpha = 0.5,
  size = 0.5
  ) + geom_line(
  data = Plot_Acf_Data[Plot_Acf_Data$type == type_list[type_idx],],
  aes(
  x = Trial,
  y = Mean,
  fill = Variable,
  color = Variable
  ),
  position = position_dodge(width = 0.5),
  linetype = "dotted"
  ) +
  
  geom_hline(
  yintercept = 0,
  linetype = "dashed",
  color = "black",
  size = 0.25
  ) +
  theme_classic(base_size = 6) + labs(
  x = "Lag (Trials)",
  y =
  paste("Autocorrelation coefficient"),
  subtitle = "B",
  color = "Congruence to",
  shape = "p value"
  ) + scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = c(0.35, 0.9), legend.box = "horizontal") + ylim(-0.01, 0.03) + scale_x_continuous(
  breaks = seq(1, 25, by = 4),
  labels = seq(1, 25, by = 4),
  limits = c(-1, 25)
  ) 


gathercol <- colnames(Control_ID_Sim_Summary_acf[,c(3,4)])

Control_ID_Sim_Summary_acf_long  <-
  gather(Control_ID_Sim_Summary_acf,
         "Variable",
         "Lag",
         gathercol,
         factor_key = TRUE)

Control_sim_p_exceed_acf <- ggplot(Control_ID_Sim_Summary_acf_long[Control_ID_Sim_Summary_acf_long$type == type_list[type_idx],], aes(x = Lag, fill = Variable)) + 
  geom_histogram(aes(y=..density..), color = "white", position="identity", binwidth = 1, bins = 100, alpha = 0.4, size = 0.5) + 
  #geom_density(alpha=.4, color = "white", bw = 1) +
  theme_classic(base_size = 6) +  xlim(-1 ,12) + labs(x = "Positive autocorrelation (Trials)",  y = "Density", subtitle = "C") +
  scale_color_brewer(palette = "Set1") + scale_fill_brewer(palette = "Set1")  + theme(legend.position = "none") + scale_x_continuous(breaks = seq(0,10,by=1), limits=c(-1, 11)) + facet_wrap(~Variable, nrow = 2) + theme(strip.background = element_blank(), strip.text = element_blank())

Control_Sim_p_power_frequency <-
  ggplot(data = Control_Sim_Power_Frequency[Control_Sim_Power_Frequency$type == type_list[type_idx] & Control_Sim_Power_Frequency$r_freq > 0.01 & Control_Sim_Power_Frequency$r_freq < 0.09,],
         mapping = aes(x = (r_freq), y = (mean_power), ymin = (mean_power - ci_power), ymax = (mean_power - ci_power), color = Variable, group = Variable, fill = Variable)) + 
  geom_point(size = 0.5, alpha = 0.1) + 
  theme_classic(base_size = 6) +
  theme(legend.position = "none") + stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,
                                                level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.15) + xlim(0.01, 0.09) + ylim(0, 0.2) + scale_color_brewer(palette = "Set1", direction = 1) + scale_fill_brewer(palette = "Set1", direction = 1) +  labs(x = "Frequency",  y = "Spectral Density", subtitle = "D") 

Control_Sim_p_density_phase <-
  ggplot(Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$type == type_list[type_idx] & Control_Sim_Power_Spectra$r_freq > 0.01 & Control_Sim_Power_Spectra$r_freq < 0.09,], aes(
    x = abs(Phase)
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + 
  labs(x = "Phase",  y = "Density", subtitle = "E") + theme_classic(base_size = 6) + 
  theme(legend.position = "none") +
  geom_vline(xintercept = pi, linetype = "dashed",
             color = "black",
             size = 0.25) #+ xlim(-0.2, 3.4)

Control_Sim_p_density_coherence <-
  ggplot(Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$type == type_list[type_idx] & Control_Sim_Power_Spectra$bin_freq < 0.09,], aes(
    x = Coherence
  ))  + geom_density(alpha=.2, color = "white", fill="darkorchid4", bw = 0.01) + 
  labs(x = "Squared coherence (%)",  y = "Density", subtitle = "F") + theme_classic(base_size = 6) + theme(legend.position = "none") + 
  geom_vline(xintercept = mean(Control_Sim_Power_Spectra[Control_Sim_Power_Frequency$r_freq > 0.01 & Control_Sim_Power_Frequency$r_freq < 0.1,]$Coherence, na.rm = TRUE), linetype = "dashed",color = "black", size = 0.25) + xlim(-1, 26)

Control_sim_p_Confidence_mode <- ggplot(data = Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type  == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5,], aes(
  x = directed_mode,
  y = average_Confidence,
  ymin = average_Confidence - se_Confidence,
  ymax = average_Confidence + se_Confidence)) +
  geom_point(
    alpha = 0.5,
    size = 0.1,
    fill = "darkorchid4",
    position = position_dodge(width = 0)
  ) + 
  geom_errorbar(
    width = 5, alpha = 0.5, size = 0.5, color = "darkorchid4") + 
  geom_line(linetype = "dotted", color = "darkorchid4") +
  theme_classic(base_size = 6) + 
  labs(x = "Int.< Mode (%) > Ext.",  y = "Simulated posterior certainty", subtitle = "H")  + 
  scale_color_brewer(palette = "Set1", direction = -1) + theme(legend.position = "none") +  
  scale_x_continuous(breaks = seq(-100,100,by = 20), labels = seq(-100,100,by = 20), limits=c(-100, 100)) + 
  stat_smooth(position = "identity", method = "loess", se = TRUE, n = 180, fullrange = TRUE,level = 0.95, na.rm = TRUE, alpha = 0.2, size = 0.1, color = "darkorchid4", fill = "darkorchid4", linetype = "dashed") + 
  geom_hline(
    yintercept = min(Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type  == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +
    geom_vline(
    xintercept = Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5 & Control_Sim_Post_Perceptual_Modes$average_Confidence == min(Control_Sim_Post_Perceptual_Modes[Control_Sim_Post_Perceptual_Modes$type  == type_list[type_idx] & Control_Sim_Post_Perceptual_Modes$n_percent > 0.5,]$average_Confidence),]$directed_mode,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) + ylim(0.15, 0.45) 

Control_sim_p_Confidence <- ggplot() +
  geom_point(data = Control_Sim_Confidence_Behav_long[Control_Sim_Confidence_Behav_long$type == type_list[type_idx],],
             aes(
               x = Variable,
               y = diff,
               color = Variable),
             alpha = 0.01,
             size = 0.01,
             fill = "white",
             position = position_jitter(width = 0.1)
  )  +
  geom_errorbar(data = Control_Summary_Sim_Confidence_Behav[Control_Summary_Sim_Confidence_Behav$type == type_list[type_idx],],
                aes(
                  x = Variable,
                  ymin = Mean - Error,
                  ymax = Mean + Error,
                  color = Variable), position = position_dodge(width = 0.5), width = 0.125, alpha = 0.8, size = 0.5) +
  theme_classic(base_size = 6) + labs(x = "Congruency",  y = "diff posterior certainty (zscore)", subtitle = "G") + scale_color_brewer(palette = "Set1", direction = -1) +   geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    size = 0.25
  ) +  theme(legend.position = "none") + ylim(-0.1, 0.25) 

##
## store plots
##
assign(paste("Control_sim_p_distribution_Behav", type_list[type_idx], sep = "_"), Control_sim_p_distribution_Behav)  
assign(paste("Control_sim_p_acf", type_list[type_idx], sep = "_"), Control_sim_p_acf )  
assign(paste("Control_sim_p_exceed_acf", type_list[type_idx], sep = "_"), Control_sim_p_exceed_acf)  
assign(paste("Control_Sim_p_power_frequency", type_list[type_idx], sep = "_"), Control_Sim_p_power_frequency)  
assign(paste("Control_Sim_p_density_phase", type_list[type_idx], sep = "_"), Control_Sim_p_density_phase)  
assign(paste("Control_Sim_p_density_coherence", type_list[type_idx], sep = "_"), Control_Sim_p_density_coherence)  
assign(paste("Control_sim_p_Confidence_mode", type_list[type_idx], sep = "_"), Control_sim_p_Confidence_mode)  
assign(paste("Control_sim_p_distribution_Behav_diff", type_list[type_idx], sep = "_"), Control_sim_p_distribution_Behav_diff)  
assign(paste("Control_sim_p_Confidence", type_list[type_idx], sep = "_"), Control_sim_p_Confidence)  

##
## do and store stats
##

## History-Effect
Control_Sim_STAT.Global_History_Accuracy = t.test(Control_Sim_Behav[Control_Sim_Behav$type == type_list[type_idx],]$null_History)

compare_participants = which(Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 1,]$subject_id %in% Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 0,]$subject_id)

Control_Sim_STAT.diff_History_Accuracy = t.test(Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 0 & Control_Sim_Behav_diff$subject_id %in% compare_participants,]$History, Control_Sim_Behav_diff[Control_Sim_Behav_diff$type == type_list[type_idx] & Control_Sim_Behav_diff$Accuracy == 1 & Control_Sim_Behav_diff$subject_id %in% compare_participants,]$History, paired = TRUE)

assign(paste("Control_Sim_STAT.Global_History_Accuracy", type_list[type_idx], sep = "_"), Control_Sim_STAT.Global_History_Accuracy) 
assign(paste("Control_Sim_STAT.diff_History_Accuracy", type_list[type_idx], sep = "_"), Control_Sim_STAT.diff_History_Accuracy) 

## exceedance of autocorrelations
Control_Sim_STAT.exc_autocorrelationt_Stimulus = t.test(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[type_idx],]$lag_significant_Stimulus, 
       ID_Sim_Summary_acf$lag_significant_Stimulus, paired = TRUE)
Control_Sim_STAT.exc_autocorrelationt_History = t.test(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[type_idx],]$lag_significant_History, 
       ID_Sim_Summary_acf$lag_significant_History, paired = TRUE)

assign(paste("Control_Sim_STAT.exc_autocorrelationt_Stimulus", type_list[type_idx], sep = "_"), Control_Sim_STAT.exc_autocorrelationt_Stimulus) 
assign(paste("Control_Sim_STAT.exc_autocorrelationt_History", type_list[type_idx], sep = "_"), Control_Sim_STAT.exc_autocorrelationt_History) 

## STATS Power vs Frequency (1/f noise)
Control_Sim_lmer_power_freq_Stimulus <- lmer(log(Power_Stimulus) ~ log(freq) + (1|subject_id), data = Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$freq > 0.01 & Control_Sim_Power_Spectra$freq < 0.1 & is.finite(log(Control_Sim_Power_Spectra$Power_Stimulus)) & Control_Sim_Power_Spectra$type == type_list[type_idx],])
Control_Sim_STAT.lmer_power_freq_Stimulus <- summary(Control_Sim_lmer_power_freq_Stimulus)

Control_Sim_lmer_power_freq_History <- lmer(log(Power_History) ~ log(freq) + (1|subject_id), data = Control_Sim_Power_Spectra[Control_Sim_Power_Spectra$freq > 0.01 & Control_Sim_Power_Spectra$freq < 0.1 & is.finite(log(Control_Sim_Power_Spectra$Power_History)) & Control_Sim_Power_Spectra$type == type_list[type_idx],])
Control_Sim_STAT.lmer_power_freq_History <- summary(Control_Sim_lmer_power_freq_History)

assign(paste("Control_Sim_STAT.lmer_power_freq_Stimulus", type_list[type_idx], sep = "_"), Control_Sim_STAT.lmer_power_freq_Stimulus) 
assign(paste("Control_Sim_STAT.lmer_power_freq_History", type_list[type_idx], sep = "_"), Control_Sim_STAT.lmer_power_freq_History) 

## Difference in coherence
Control_Sim_STAT.coherence = t.test(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[type_idx],]$mean_coherence, 
       Sim_Summary_Power_Spectra$mean_coherence, paired = TRUE)
assign(paste("Control_Sim_STAT.coherence", type_list[type_idx], sep = "_"), Control_Sim_STAT.coherence) 

## Effects of History and Accuracy on Confidence
Control_Sim_lmer_Confidence_Accuracy_History <- lmer(clear_Confidence ~ Accuracy + History + (1|subject_id), data = Control_Sim[Control_Sim$type == type_list[type_idx],])
Control_Sim_STAT.lmer_Confidence_Accuracy_History <- summary(Control_Sim_lmer_Confidence_Accuracy_History)

assign(paste("Control_Sim_STAT.lmer_Confidence_Accuracy_History", type_list[type_idx], sep = "_"), Control_Sim_STAT.lmer_Confidence_Accuracy_History)

## History vs Accuracy
Control_Sim_slider_History_vs_Accuracy <- lmer(History_slider ~ Accuracy_slider + (1|subject_id), data = Control_Sim[Control_Sim$type == type_list[type_idx],])
Control_Sim_STAT.slider_History_vs_Accuracy <- summary(Control_Sim_slider_History_vs_Accuracy)

assign(paste("Control_Sim_STAT.slider_History_vs_Accuracy", type_list[type_idx], sep = "_"), Control_Sim_STAT.slider_History_vs_Accuracy)

## Confidence vs mode
Control_Sim_Confidence_vs_mode <- lmer(clear_Confidence ~ poly(directed_mode, 2) + (1|subject_id), data = Control_Sim[Control_Sim$type == type_list[type_idx],])
Control_Sim_STAT.Confidence_vs_mode <- summary(Control_Sim_Confidence_vs_mode)
assign(paste("Control_Sim_STAT.Confidence_vs_mode", type_list[type_idx], sep = "_"),Control_Sim_STAT.Confidence_vs_mode)

# lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
# p <- grid.arrange(
# Control_sim_p_distribution_Behav, Control_sim_p_acf, Control_sim_p_exceed_acf,
# Control_Sim_p_power_frequency, Control_Sim_p_density_phase, Control_Sim_p_density_coherence,
# Control_sim_p_Confidence_mode, Control_sim_p_distribution_Behav_diff, Control_sim_p_Confidence,
# layout_matrix = lay,
# heights = c(0.5, 0.5, 0.4,0.4),
# widths = c(0.3,0.2, 0.225,0.3))
}
               
    
  
  if (save_summary_data) {
save(
Control_Sim_Behav, Control_ID_Sim_Summary_acf, Control_Sim_Summary_Power_Spectra, type_list, file = "./Summary_Data/Control_Sim_global.Rdata")

      save( 
Control_sim_p_acf_glaze_osc_no_H,                               
Control_sim_p_Confidence_glaze_osc_no_H,                         
Control_sim_p_Confidence_mode_glaze_osc_no_H,                    
Control_Sim_p_density_coherence_glaze_osc_no_H,                  
Control_Sim_p_density_phase_glaze_osc_no_H,                      
Control_sim_p_distribution_Behav_diff_glaze_osc_no_H,            
Control_sim_p_distribution_Behav_glaze_osc_no_H,                 
Control_sim_p_exceed_acf_glaze_osc_no_H,                         
Control_Sim_p_power_frequency_glaze_osc_no_H,                    
Control_Sim_STAT.coherence_glaze_osc_no_H,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H,              
Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H,   
Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H,
Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H,  
file = "./Summary_Data/Control_Sim_osc_no_H.Rdata")
      
            save( 
Control_sim_p_acf_glaze_no_osc,                               
Control_sim_p_Confidence_glaze_no_osc,                         
Control_sim_p_Confidence_mode_glaze_no_osc,                    
Control_Sim_p_density_coherence_glaze_no_osc,                  
Control_Sim_p_density_phase_glaze_no_osc,                      
Control_sim_p_distribution_Behav_diff_glaze_no_osc,            
Control_sim_p_distribution_Behav_glaze_no_osc,                 
Control_sim_p_exceed_acf_glaze_no_osc,                         
Control_Sim_p_power_frequency_glaze_no_osc,                    
Control_Sim_STAT.coherence_glaze_no_osc,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc,              
Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc,   
Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc,
Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc,  
file = "./Summary_Data/Control_Sim_no_osc.Rdata")
            
                        save( 
Control_sim_p_acf_glaze_only_l_osc,                               
Control_sim_p_Confidence_glaze_only_l_osc,                         
Control_sim_p_Confidence_mode_glaze_only_l_osc,                    
Control_Sim_p_density_coherence_glaze_only_l_osc,                  
Control_Sim_p_density_phase_glaze_only_l_osc,                      
Control_sim_p_distribution_Behav_diff_glaze_only_l_osc,            
Control_sim_p_distribution_Behav_glaze_only_l_osc,                 
Control_sim_p_exceed_acf_glaze_only_l_osc,                         
Control_Sim_p_power_frequency_glaze_only_l_osc,                    
Control_Sim_STAT.coherence_glaze_only_l_osc,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc,              
Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc,   
Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc,
Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc,  
file = "./Summary_Data/Control_Sim_only_l_osc.Rdata")
                        
save( 
Control_sim_p_acf_glaze_only_p_osc,                               
Control_sim_p_Confidence_glaze_only_p_osc,                         
Control_sim_p_Confidence_mode_glaze_only_p_osc,                    
Control_Sim_p_density_coherence_glaze_only_p_osc,                  
Control_Sim_p_density_phase_glaze_only_p_osc,                      
Control_sim_p_distribution_Behav_diff_glaze_only_p_osc,            
Control_sim_p_distribution_Behav_glaze_only_p_osc,                 
Control_sim_p_exceed_acf_glaze_only_p_osc,                         
Control_Sim_p_power_frequency_glaze_only_p_osc,                    
Control_Sim_STAT.coherence_glaze_only_p_osc,                       
Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc,              
Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc,           
Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc,    
Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc,   
Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc,         
Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc,
Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc,         
Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc,        
Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc,  
file = "./Summary_Data/Control_Sim_only_p_osc.Rdata")
  }
  
} else {
load("./Summary_Data/Control_Sim_global.Rdata")
load("./Summary_Data/Control_Sim_no_osc.Rdata")
load("./Summary_Data/Control_Sim_osc_no_H.Rdata")
load("./Summary_Data/Control_Sim_only_l_osc.Rdata")
load("./Summary_Data/Control_Sim_only_p_osc.Rdata")
}


```

```{r Supplemental_Figure_S10, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_only_l_osc, Control_sim_p_acf_glaze_only_l_osc, Control_sim_p_exceed_acf_glaze_only_l_osc,
Control_Sim_p_power_frequency_glaze_only_l_osc, Control_Sim_p_density_phase_glaze_only_l_osc, Control_Sim_p_density_coherence_glaze_only_l_osc,
Control_sim_p_Confidence_mode_glaze_only_l_osc, Control_sim_p_distribution_Behav_diff_glaze_only_l_osc, Control_sim_p_Confidence_glaze_only_l_osc,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S10. Reduced Control Model M2: Only oscillation of the likelihood.** When simulating data for the *likelihood-oscillation-only model*, we removed the oscillation from the prior term by setting the amplitude $a_{\psi}$ to zero. Simulated data thus depended only on the participant-wise estimates for hazard rate $H$, amplitude $a_{LLR}$, frequency $f$, phase $p$ and inverse decision temperature $\zeta$.    

A. Similar to the full model M1 (Figure 1F and Figure 4), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[3],]$History))`% of trials (in blue). As in the full model, the likelihood-oscillation-only model showed a significant bias toward perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_l_osc$p.value`$; upper panel). 
Similarly, history-congruent choices were more frequent at error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_l_osc$p.value`$; lower panel).

B. In the likelihood-oscillation-only model, we observed that the autocorrelation coefficients for history-congruence were reduced below the autocorrelation coefficients of stimulus-congruence. This is an approximately five-fold reduction relative to the empirical results observed in humans (Figure 2B), where the autocorrelation of history-congruence was above the autocorrelation of stimulus-congruence. Moreover, in the reduced model shown here, the number of consecutive trials that showed significant autocorrelation of history-congruence was reduced to 11.

C. In the likelihood-oscillation-only model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data did not differ with respect to stimulus-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_l_osc$p.value`$), 
but decreased with respect to history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[3],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_l_osc$p.value`$) relative to the full model.

D. In the likelihood-oscillation-only model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_l_osc$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_l_osc$coefficients[2,5]`$).

E. In the likelihood-oscillation-only model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). In contrast to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were positively correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_l_osc$coefficients[2,5]`$).

F. In the likelihood-oscillation-only model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_only_l_osc$parameter`) = `r Control_Sim_STAT.coherence_glaze_only_l_osc$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_only_l_osc$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[3],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[3],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[3],]$mean_coherence)`%.

G. Similar to the full bimodal inference model, confidence simulated from the likelihood-oscillation-only model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[2,5]`$)
and history-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_l_osc$coefficients[3,5]`$). 

H. In the likelihood-oscillation-only model, the positive quadratic relationship between the mode of perceptual processing and confidence was markedly reduced in comparison to the full model ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_l_osc$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Supplemental Figure S11

```{r Supplemental_Figure_S11, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_only_p_osc, Control_sim_p_acf_glaze_only_p_osc, Control_sim_p_exceed_acf_glaze_only_p_osc,
Control_Sim_p_power_frequency_glaze_only_p_osc, Control_Sim_p_density_phase_glaze_only_p_osc, Control_Sim_p_density_coherence_glaze_only_p_osc,
Control_sim_p_Confidence_mode_glaze_only_p_osc, Control_sim_p_distribution_Behav_diff_glaze_only_p_osc, Control_sim_p_Confidence_glaze_only_p_osc,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S11. Reduced Control Model M3: Only oscillation of the prior.** When simulating data for the *prior-oscillation-only model*, we removed the oscillation from the prior term by setting the amplitude $a_{LLR}$ to zero. Simulated data thus depended only on the participant-wise estimates for hazard rate $H$, amplitude $a_{\psi}$, frequency $f$, phase $p$ and inverse decision temperature $\zeta$.  

A. Similar to the full model (Figure 1F and Figure 4), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[2],]$History))`% of trials (in blue). As in the full model, the prior-oscillation-only showed a significant bias toward perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_only_p_osc$p.value`$; upper panel). 
Similarly, history-congruent choices were more frequent at error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_only_p_osc$p.value`$; lower panel).

B. In the prior-oscillation-only model, we did not observe any significant positive autocorrelation of stimulus-congruence , whereas the autocorrelation of history-congruence was preserved.

C. In the prior-oscillation-only model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data did was decreased with respect to stimulus-congruence relative to the full model (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_only_p_osc$p.value`$), 
but did not differ from the full model with respect to history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[2],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_only_p_osc$p.value`$).

D. In the prior-oscillation-only model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_only_p_osc$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_only_p_osc$coefficients[2,5]`$).

E. In the prior-oscillation-only model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). Similar to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were anti-correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_only_p_osc$coefficients[2,5]`$).

F. In the prior-oscillation-only model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_only_p_osc$parameter`) = `r Control_Sim_STAT.coherence_glaze_only_p_osc$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_only_p_osc$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[2],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[2],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[2],]$mean_coherence)`%.

G. Similar to the full bimodal inference model, confidence simulated from the prior-oscillation-only model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[2,5]`$)
and history-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_only_p_osc$coefficients[3,5]`$). 

H. In contrast to the full bimodal inference model, the prior-oscillation-only model did not yield a positive quadratic relationship between the mode of perceptual processing and confidence ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_only_p_osc$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Supplemental Figure S12

```{r Supplemental_Figure_S12, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_no_osc, Control_sim_p_acf_glaze_no_osc, Control_sim_p_exceed_acf_glaze_no_osc,
Control_Sim_p_power_frequency_glaze_no_osc, Control_Sim_p_density_phase_glaze_no_osc, Control_Sim_p_density_coherence_glaze_no_osc,
Control_sim_p_Confidence_mode_glaze_no_osc, Control_sim_p_distribution_Behav_diff_glaze_no_osc, Control_sim_p_Confidence_glaze_no_osc,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S12. Reduced Control Model M4: Normative evidence accumulation.** When simulating data for the *normative-evidence-accumulation model*, we removed the oscillation from the likelihood and prior terms by setting the amplitudes $a_{LLR}$ and $a_{\psi}$ to zero. Simulated data thus depended only on the participant-wise estimates for hazard rate $H$ and inverse decision temperature $\zeta$.    

A. Similar to the full model (Figure 1F and Figure 4), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[1],]$History))`% of trials (in blue). As in the full model, the no-oscillation model showed a significant bias toward perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_no_osc$p.value`$; upper panel). 
Similarly, history-congruent choices were more frequent at error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_no_osc$p.value`$; lower panel).

B. In the normative-evidence-accumulation model, we did not find significant autocorrelations for stimulus-congruence. Likewise, we did not observe any autocorrelation of history-congruence beyond the first three consecutive trials.

C. In the normative-evidence-accumulation model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data decreased with respect to both stimulus-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_no_osc$p.value`$) 
and history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[1],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_no_osc$p.value`$) relative to the full model.

D. In the normative-evidence-accumulation model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_no_osc$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_no_osc$coefficients[2,5]`$).

E. In the normative-evidence-accumulation model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). In contrast to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were positively correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_no_osc$coefficients[2,5]`$).

F. In the normative-evidence-accumulation model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_no_osc$parameter`) = `r Control_Sim_STAT.coherence_glaze_no_osc$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_no_osc$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[1],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[1],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[1],]$mean_coherence)`%.

G. Similar to the full bimodal inference model, confidence simulated from the no-oscillation model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[2,5]`$)
and history-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_no_osc$coefficients[3,5]`$). 

H. In the normative-evidence-accumulation model, the positive quadratic relationship between the mode of perceptual processing and confidence was markedly reduced in comparison to the full model ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_no_osc$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Supplemental Figure S13

```{r Supplemental_Figure_13, cache = TRUE}
lay <- rbind(c(1,2,2,3), c(8,2,2,3), c(4,5, 9,7), c(4,6,9,7))
p <- grid.arrange(
Control_sim_p_distribution_Behav_glaze_osc_no_H, Control_sim_p_acf_glaze_osc_no_H, Control_sim_p_exceed_acf_glaze_osc_no_H,
Control_Sim_p_power_frequency_glaze_osc_no_H, Control_Sim_p_density_phase_glaze_osc_no_H, Control_Sim_p_density_coherence_glaze_osc_no_H,
Control_sim_p_Confidence_mode_glaze_osc_no_H, Control_sim_p_distribution_Behav_diff_glaze_osc_no_H, Control_sim_p_Confidence_glaze_osc_no_H,
layout_matrix = lay,
heights = c(0.5, 0.5, 0.4,0.4),
widths = c(0.3,0.2, 0.225,0.3))
```

**Supplemental Figure S13. Reduced Control Model M5: No accumulation of information across trials.** When simulating data for the *no-evidence-accumulation model*, we removed the accumulation of information across trials by setting the Hazard rate $H$ to 0.5. Simulated data thus depended only on the participant-wise estimates for the amplitudes $a_{LLR/\psi}$, frequency $f$, phase $p$ and inverse decision temperature $\zeta$.    

A. Similar to the full model (Figure 1F and Figure 4), simulated perceptual choices were stimulus-congruent in `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$Accuracy, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$Accuracy, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$Accuracy))`% of trials (in red). History-congruent amounted to `r mean(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$History, na.rm = TRUE)`% ± `r sd(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$History, na.rm = TRUE)/sqrt(length(Control_Sim_Behav[Control_Sim_Behav$type == type_list[4],]$History))`% of trials (in blue). In contrast to the full model, the no-accumulation model showed a significant bias against perceptual history T(`r Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.Global_History_Accuracy_glaze_osc_no_H$p.value`$; upper panel). 
In contrast to the full model, there was no difference in the frequency of history-congruent choices between correct and error trials (T(`r Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.diff_History_Accuracy_glaze_osc_no_H$p.value`$; lower panel).

B. In the no-evidence-accumulation model, we found no significant autocorrelation of history-congruence beyond the first trial, whereas the autocorrelation of stimulus-congruence was preserved.

C. In the no-evidence-accumulation model, the number of consecutive trials at which true autocorrelation coefficients exceeded the autocorrelation coefficients for randomly permuted data increased with respect to stimulus-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_Stimulus, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_Stimulus, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_Stimulus_glaze_osc_no_H$p.value`$) 
and decreased with respect to history-congruence (`r mean(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_History, na.rm = TRUE)` ± `r sd(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],]$lag_significant_History, na.rm = TRUE)/nrow(Control_ID_Sim_Summary_acf[Control_ID_Sim_Summary_acf$type == type_list[4],])` trials; T(`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.exc_autocorrelationt_History_glaze_osc_no_H$p.value`$) relative to the full model.

D. In the no-evidence-accumulation model, the smoothed probabilities of stimulus- and history-congruence (sliding windows of ±`r sliding_window/2` trials) fluctuated as a scale-invariant process with a 1/f power law, i.e., at power densities that were inversely proportional to the frequency (power ~ 1/$f^\beta$; stimulus-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_Stimulus_glaze_osc_no_H$coefficients[2,5]`$; 
history-congruence: $\beta$ = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_power_freq_History_glaze_osc_no_H$coefficients[2,5]`$).

E. In the no-evidence-accumulation model, the distribution of phase shift between fluctuations in simulated stimulus- and history-congruence peaked at half a cycle ($\pi$ denoted by dotted line). In contrast to the full model, the dynamic probabilities of simulated stimulus- and history-congruence were not significantly anti-correlated ($\beta$ = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.slider_History_vs_Accuracy_glaze_osc_no_H$coefficients[2,5]`$).

F. In the no-evidence-accumulation model, the average squared coherence between fluctuations in simulated stimulus- and history-congruence (black dotted line) was reduced in comparison to the full model (T(`r Control_Sim_STAT.coherence_glaze_osc_no_H$parameter`) = `r Control_Sim_STAT.coherence_glaze_osc_no_H$statistic`, p = $`r Control_Sim_STAT.coherence_glaze_osc_no_H$p.value`$)
and amounted to `r mean(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[4],]$mean_coherence, na.rm = TRUE)` ± `r sd(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[4],]$mean_coherence, na.rm = TRUE)/length(Control_Sim_Summary_Power_Spectra[Control_Sim_Summary_Power_Spectra$type == type_list[4],]$mean_coherence)`%.

G. Similar to the full bimodal inference model, confidence simulated from the no-evidence-accumulation model was enhanced for stimulus-congruent choices ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[2,5]`$).
In contrast to the full bimodal inference model, history-congruent choices were not characterized by enhanced confidence ($\beta$ = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,1]`$ ± $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,2]`$, T($`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,3]`$) = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,4]`$, p = $`r Control_Sim_STAT.lmer_Confidence_Accuracy_History_glaze_osc_no_H$coefficients[3,5]`$). 

H. In the no-evidence-accumulation model, the positive quadratic relationship between the mode of perceptual processing and confidence was markedly reduced in comparison to the full model ($\beta_2$ = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,1]`$ ± $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,2]`$, T($`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,3]`$) = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,4]`$, p = $`r Control_Sim_STAT.Confidence_vs_mode_glaze_osc_no_H$coefficients[3,5]`$). The horizontal and vertical dotted lines indicate minimum posterior certainty and the associated mode, respectively.

\newpage
## Supplemental Figure S14
```{r Supplemental_Figure_S14, cache = TRUE}
##
## Visualize alternating and repetitive biases 
##
n_participants = 1000
if (run_visualize_alt_sim){
source("./Functions/visualize_rep_alt_sim_correction.R",
local = knitr::knit_global())

blocks = rep(c(0.9, 0.5), 10)


Rep_Sim = data.frame()
for (subj_idx in c(1:n_participants)) {
n_trials_per_block = round(runif(length(blocks), min = 15, max = 30))
add_Rep_Sim <- visualize_rep_alt_sim_correction(blocks, n_trials_per_block)
add_Rep_Sim$subj_idx = subj_idx

Rep_Sim = rbind(Rep_Sim, add_Rep_Sim)
}
Rep_Sim$corrected <- Rep_Sim$acf - Rep_Sim$random
Rep_Sim$type = "Attraction"


blocks = rep(c(0.1, 0.5), 10)
n_participants = 1000
Alt_Sim = data.frame()
for (subj_idx in c(1:n_participants)) {
n_trials_per_block = round(runif(length(blocks), min = 15, max = 30))
add_Alt_Sim <- visualize_rep_alt_sim_correction(blocks, n_trials_per_block)
add_Alt_Sim$subj_idx = subj_idx

Alt_Sim = rbind(Alt_Sim, add_Alt_Sim)
}
Alt_Sim$corrected <- Alt_Sim$acf - Alt_Sim$random
Alt_Sim$type = "Repulsion"

Rep_Alt_Sim = rbind(Rep_Sim, Alt_Sim)


Sum_Rep_Alt_Sim <-  ddply(
Rep_Alt_Sim,
.(trial, type),
summarise,
mean = mean(corrected, na.rm = TRUE),
error = sd(corrected, na.rm = TRUE) / sqrt(length(corrected))
)

Sum_Rep_Alt_Sim <- Sum_Rep_Alt_Sim[Sum_Rep_Alt_Sim$trial != 1,]
Sum_Rep_Alt_Sim$trial = Sum_Rep_Alt_Sim$trial-1
} else {Sum_Rep_Alt_Sim <- read.csv("./Results/Sum_Rep_Alt_Sim.csv")}


Sum_Rep_Alt_Sim$type = gsub("Attraction", "Repeating", Sum_Rep_Alt_Sim$type)
Sum_Rep_Alt_Sim$type = gsub("Repulsion", "Alternating", Sum_Rep_Alt_Sim$type)
p_Sum_Rep_Alt_Sim <- ggplot() +
geom_errorbar(
data = Sum_Rep_Alt_Sim,
aes(
x = trial,
ymin = mean - error,
ymax = mean + error,
color = type
),
width = 0.5,
alpha = 0.5,
size = 0.5,
) +
geom_line(
data = Sum_Rep_Alt_Sim,
aes(x = trial,
y = mean,
color = type),
linetype = "dotted"
) +
geom_hline(
yintercept = 0,
linetype = "dashed",
color = "black",
size = 0.25
) +
theme_classic(base_size = 6) + labs(
x = "Lag (Trials)",
y =
paste("Autocorrelation coefficient: History congruence"),
subtitle = NULL,
color = "Regime"
) + xlim(0, 25.5) + scale_color_brewer(palette = "Set1", direction = 1) + theme(legend.position = c(0.5, 0.9), legend.box = "horizontal") +  scale_x_continuous(
breaks = seq(1, 25, by = 4),
labels = seq(1, 25, by = 4),
limits = c(0, 25)
) #+ ylim(-0.01, 0.035) 
p_Sum_Rep_Alt_Sim
```

**Supplemental Figure S14. Autocorrelation of history-congruence of alternating and repeating biases.** Here, we simulate the autocorrelation of history-congruence in $`r n_participants`$ synthetic participants. In the repeating regime (blue), history-congruence fluctuated between 50% and 80% (blue) in interleaved blocks (10 blocks per condition with a random duration between 15 and 30 trials). In the alternation regime (red), history-congruence fluctuated between 50% and 20%. The resulting autocorrelation curves for history-congruence overlap, indicating that our analysis is able to accommodate both repeating and alternating biases. 

\newpage
## Supplemental Table T1

```{r Supplemental_Table_T1, cache = TRUE}

library(kableExtra)
Tables <- Experiments[Study_Behav[!is.na(Study_Behav$study_id),]$study_id, c(3,4,5)]
row.names(Tables) <- NULL
kbl(Tables, format = "latex", longtable = T, booktabs = T) %>%
kable_styling(font_size = 7, latex_options = c("repeat_header"))
```

\newpage
## Supplemental Table T2
```{r Supplemental_Table_T2, cache = TRUE}

library(kableExtra)
Parameters <- data.frame(
  Parameters = c("$\\alpha$", "$H$", "$amp_{LLR}$", "$amp_{\\psi}$", "$f$", "$p$"),
  Interpretation = c("Sensitivity to sensory information",
                     "Expected probability of a switch in the cause of sensory information (Hazard)",
                     "Amplitude of fluctuations in likelihood precision ($omega_{LLR}$)",
                     "Amplitude of fluctuations in prior precision ($omega_{\\psi}$)",
                     "Frequency of $omega_{LLR}$ and $omega_{\\psi}$",
                     "Phase ($p$ for $omega_{LLR}$; $p + \\pi$  for $omega_{\\psi}$)"))
row.names(Parameters) <- NULL
kbl(Parameters, format = "latex", longtable = T, booktabs = T) %>%
kable_styling(font_size = 7, latex_options = c("repeat_header"))
```

\newpage
# Response to Reviewers

## Reviewer 1

**This was an interesting and thought-provoking submission. I note that it is a revision: I am therefore supposing that the authors have already responded to one round of reviewer comments and that you are potentially interested in publishing this work. In brief, I think there are many elements of this report that warrant publication; however, there are some parts that are less compelling and could be deferred to a subsequent paper. The paper is far too long and would benefit greatly from being streamlined. Furthermore, some of the modeling is overengineered and is difficult to follow. I have tried to suggest how the authors might improve the presentation of their work in my comments to authors.**

**I enjoyed reading this long but thought-provoking report of fluctuations in the sensitivity to sensory evidence in perceptual decision-making tasks. There were some parts of this report that were compelling and interesting. Other parts were less convincing and difficult to understand. Overall, this paper is far too long. An analogy that might help here is that a dinner guest is very entertaining for the first hour or so - and then overstays their welcome; until you start wishing they would leave. Another analogy, which came to mind, was that the modeling—and its interpretation—was a bit autistic (i.e., lots of fascinating if questionable detail with a lack of central coherence).**

**I think that both issues could be resolved by shortening the paper and removing (or, at least, greatly simplifying) the final simulation studies of metacognition. I try to unpack this suggestion in the following.**

We would like to thank Prof. Friston for the very insightful and helpful comments on our manuscript. We fully agree that our ideas about the computational function of between-mode fluctuations and the associated simulations may be presented in a more accessible form in a standalone paper. As we outlined in more detail below, we have streamlined our findings and rewrote the paper and reduced it's length by shortening the sections on computational modeling. We have also followed Prof. Friston's suggestion to interpret the effects of mode on RT and confidence in the context of predictive processing definitions of attention, namely the allocation of precision between prior and likelihood. 

**Major points:**

**As I understand it, you have used publicly available data on perceptual decision-making to demonstrate slow fluctuations in the tendency to predicate perceptual decisions on the stimuli and on the history of recent decisions. You find scale-free fluctuations in this tendency — that are anti-correlated — and interpret this as fluctuations in the precision afforded sensory evidence, relative to prior beliefs. This interpretation is based upon a model of serial dependencies (parameterised with a hazard function).**

**The stimulus and history (i.e., likelihood and prior) sensitivities are anti-correlated and both show scale free behavior. This is reproduced in men and mice. You then proceed to model this with periodic fluctuations in the precisions or weights applied to the likelihood and prior that are in anti-phase - and then estimate the parameters of the ensuing model. Finally, you then simulate the learning of the hazard parameter — and something called metacognition - to show that periodic fluctuations improve estimates of metacognition (based upon a Rescorla-Wagner model of learning). You motivate this by suggesting that the fluctuations in sensitivity are somehow necessary to elude circular inference and provide better estimates of precision.**

**Note that I am reading the parameters omega_LLR and omega_psi as the precision of the likelihood and prior, where the precision of the likelihood is called sensory precision. This contrasts with your use of sensory precision, which seems to be attributed to a metacognitive construct M.**

**As noted above, all of this is fascinating but there are too many moving parts that do not fit together comfortably. I will list a few examples:**

### Comment 1

**If, empirically, the fluctuations in sensitivity are scale-free with a 1/f power law, why did you elect to model fluctuations in precision as a periodic function with one unique timescale (i.e., f).?**

The reason for choosing a unique timescale $f$ was to enable our model to depict the dominant timescale at which prior and likelihood precision fluctuate in their impact on perceptual decision-making, giving rise to between-mode fluctuations. We think that identifying this timescale is important for planning future experiments targeted at between-mode fluctuations and their manipulation by causal interventions (e.g., pharmacology or TMS). The posterior value for $f$ lies at approximately 0.11 1/$N_{trials}$ in both humans and mice. The value of $f$ approximately matches the transition probabilities between *engaged* and *disengaged* states in work assessing fluctuations in perceptual decision-making using Hidden Markov models (stay probabilities ranged between 0.94 and 0.98, see Ashwood et al., Nature Neuroscience 2022). Simulating from our model (Figure 4) replicates the 1/f feature of the empirical data. Please note that the individual trial is the smallest unit of *measurement* for these fluctuations, such that our analysis is limited by definition to frequencies below 1 (1/$N_{trials}$). 

We now provide a rationale for choosing one value for $f$ - identifying the dominant timescale for fluctuations in mode - in the results section:

- To allow for *bimodal inference*, i.e., alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$. These anti-phase sine functions (defined by amplitudes $a_{LLR/\psi}$, frequency $f$ and phase $p$) determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014a]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)

- (...) The parameter $f$ captures the dominant time scale at which likelihood (amplitude humans: $a_{LLR}$ = $`r mean(Optim_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $a_{LLR}$ = $`r mean(Optim_M_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_M_Behav)`$ and prior precision (amplitude humans: $a_{\psi}$ = $`r mean(Optim_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $a_{\psi}$ = $`r mean(Optim_M_Behav$amp_prior, na.rm = TRUE)` ± `r sd(Optim_M_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_M_Behav)`$ were shifted against each other and was estimated at $`r mean(Optim_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$frequency, na.rm = TRUE)/nrow(Optim_Behav)`$ 1/$N_{trials}$ and $`r mean(Optim_M_Behav$frequency, na.rm = TRUE)` ± `r sd(Optim_M_Behav$frequency, na.rm = TRUE)/nrow(Optim_M_Behav)`$ 1/$N_{trials}$ in mice.

- Table 2 (see response to Comment 22 by Reviewer 1) contains an additional definition for all model parameters, including $f$. 

### Comment 2

**At present, the estimates of meta-cognition (M) play the role of accumulated estimates of (sensory or prior) precision. Why are these not used in your model of perceptual decisions in Equation 2.**

In our model, the parameter $\alpha$ controls the encoding precision by governing the transformation from sensory stimuli to the log likelihood ratio (LLR) via the equations (13-16) (the LLR ends up closer to zero when $\alpha$ is low). Our simulations on the adaptive benefits of bimodal inference rest on the assumption that $\alpha$ may change unpredictably. The construct $M$ is a belief about $\alpha$ that may be useful for, e.g., communicating the precision of sensory encoding to other cognitive domains or agents. To our mind, $\alpha$ is a feature of low-level sensory encoding that cannot be modulated by top-down beliefs such as $M$. This is why we did not include $M$ in equation (2). Please note that we have removed this section following Comment 9 of Reviewer 1. 

### Comment 3

**Why do you assume that non-specific increases in attention and arousal will increase reaction times? If one has very precise prior beliefs (and is not attending to stimuli), would you not expect a decrease in reaction time?**

Thanks a lot for pointing this out (see also the Comment below and Comment 6 by Reviewer 3). Both high prior and high likelihood precision lead to higher absolute values of the log posterior ratio (reflecting certainty encoded by the decision variable), and thus faster RTs. This is reflected empirically by RTs in humans (Figure 2) and to a lesser degree in mice (Figure 3): RTs tended to be shorter for stronger biases toward both external and internal mode. Our model, which incorporates (i), the accumulation of information across trials, and (ii), fluctuations in the likelihood precision relative to the prior precision, recapitulates this feature of the data, which is lost or greatly attenuated when eliminating process (i) and/or (ii) (see model comparison and simulations below). Our data thus confirm the hypothesis that both high prior and likelihood precision lead to faster RTs. 

In the previous version of the manuscript, we had included the relation between mode and RTs and confidence primarily as a defensive analysis against the proposition that what we call between-mode fluctuations is not a perceptual phenomenon, but occurs downstream of perception. One may imagine that fluctuations in perceptual performance are not influenced by periods of relative increases in prior precision (which decrease performance in fully randomized designs), but by periods when participants do not attend to the task at all, i.e., neither to sensory information nor to prior precision. We propose that the analyses of RTs and confidence can give some insight into whether such alternative mechanisms may be at play, as we would assume longer RTs and lower confidence if participants failed to attend to the task at all.

That being said, we realize that, due to the potential non-linearity in their relation to arousal (see also Comment 6 by Reviewer 3), RTs and confidence cannot provide a definitive map of where fluctuations in mode are situated in relation to arousal. Such a delineation may potentially be provided by tracking of pupil size, response behavior or by neural data (e.g., noise correlations of fluctuations in LFP). These data are not available for the studies in the Confidence Database, but were very recently published for the IBL database (after this manuscript was submitted). While we believe that this is beyond the scope of this manuscript, we will assess the relation of pupil diameter, motor behavior (turning of the response wheel) and LFPs to between-mode fluctuations in a future publication using the IBL dataset.

In light of the considerations above and our response to Comment 6 by Reviewer 3, we now refer to attention in the predictive processing sense. We use the term "task engagement" instead of "on-task attention" to refer to situation in which participants may not attend to the task at all, e.g. due to low arousal or fatigue, and discuss these as alternative explanations for between-mode fluctuations. We have made three sets of changes to our manuscript: 

First, we present our results on the relation of mode to RT and Confidence in a more descriptive way, and do not use it as a strong defensive analysis against arousal: 

- The above results point to systematic fluctuations in the *decision variable*[@Kepecs2008] that determines perceptual choices, causing enhanced sensitivity to external stimulus information during external mode and increased biases toward preceding choices during internal mode. As such, fluctuations in mode should influence downstream aspects of behavior and cognition that operate on the perceptual decision variable[@Kepecs2008]. To test this hypothesis with respect to motor behavior and metacognition, we asked how bimodal inference relates to response times (RTs) and confidence reports. (....)

- (...) In sum, the above results indicate that reporting behavior and metacognition do not map linearly onto the mode of sensory processing. Rather, they suggest that slow fluctuations in the respective impact of external and internal information are most likely to affect perception at an early level of sensory analysis[@St.John-Saaltink2016; @Cicchini2021]. Such low-level processing may thus integrate perceptual history with external inputs into a decision variable[@Kepecs2008] that influences not only perceptual choices, but also the speed and confidence at which they are made.

- In what follows, we probe alternative explanations for between-mode fluctuations, test for the existence of modes in mice, and propose a predictive processing model that explains fluctuations in mode by ongoing shifts in the precision afforded to external sensory information relative to internal predictions driven by perceptual history.

Second, we have re-written our discussion of the quadratic relationship of mode to RTs and Confidence, focusing on predictive processing models of attention: 
- As a functional explanation for bimodal inference, we propose that perception temporarily disengages from internal predictions to form stable inferences about the statistical properties of the sensory environment. Between-mode fluctuations may thus elude circular inferences that occur when both the causes and the encoding of sensory stimuli are volatile[@Jardri2017; @Weilnhammer2021a]. By the same token, we suggest that fluctuations in mode occur at the level of perceptual processing[@St.John-Saaltink2016; @Cicchini2017; @Braun2018; @Cicchini2021], and are not a passive phenomenon that is primarily driven by factors situated up- or downstream of sensory analysis.

- How does attention relate to between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. From this perspective, fluctuations between external and internal mode can be understood as ongoing shifts in the attention afforded to either external sensory information (regulated via likelihood precision) or internal predictions (regulated via prior precision). When the precision of either likelihood or prior increases, posterior precision increases, which leads to faster RTs and higher confidence. Therefore, when defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may provide a plausible explanation for the quadratic relationship of mode to RTs and confidence (Figure 2H and J; Figure 3I; Figure 3I).

Third, we have added a more general discussion of our findings in the light of fluctuations in task engagement: 

- Outside of the predictive processing field, attention is often understood in the context of task engagement[@Matthews2010], which varies according to the availability of cognitive resources that are modulated by factors such as tonic arousal, familiarity with the task, or fatigue[@Matthews2010]. Our results suggest that internal mode processing cannot be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, choices during internal mode were not random or globally biased, but driven by perceptual history (Supplemental Figures S6-7). Moreover, our computational model identified the dominant timescale of between-mode fluctuations at 0.11 1/$N_{trials}$, which may be compatible with fluctuations in arousal[@McGinley2015], but is faster than to be expected for the development of task familiarity or fatigue.

- However, in interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects on task performance[@Beerendonk2023]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. 

- Of note, high phasic arousal has been shown to suppress multi-domain biases in decision-making in humans and mice[@Gee2014; @Gee2017; @Gee2020], including the biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. The increase in response speed and history congruence over time (Supplemental Section 9.4) may argue against insufficient training as an alternative explanation for internal mode processing, but may also taken as a sign of waning arousal. The multiple mechanistic mappings to RTs and confidence therefore warrant more direct measures of arousal (such as pupil size[@Gee2014; @McGinley2015b; @Urai2017; @Gee2020; @Gee2022; @Beerendonk2023], motor behavior[@McGinley2015b; @Gee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.

### Comment 4

**In the predictive processing literature, attention is thought to correspond to fluctuations in sensory and prior precision. Why did you then consider attention as some additional or unrelated confound?**

This point is closely related to the comment above. We realize that, in the predictive processing field, attention is equated with the precision of probability distributions that contribute to the perceptual decision, such that an observer can attend strongly to sensory information (high likelihood precision) or to internal predictions derived from the sequence of preceding percepts (high prior precision). Therefore, when following the above predictive processing definition, fluctuations in attention can be equated with fluctuations in mode. 

However, we feel that outside of the predictive processing field, attention is not always conceived in that way, such that low attention is often considered as low engagement with the task, i.e., relating to low likelihood and low prior precision in the predictive processing sense. It is against these caveats that our control analysis (based on psychometric functions, training effects, RTs and confidence) defend our conclusions on bimodal inference. 

We now provide a more nuanced interpretation of our findings of RTs and confidence in relation to attention, with a specific focus on predictive processing and precision. We hope that our responses to the comment above resolve the points raised in this comment.

### Comment 5

**What licences the assumption that "agents depend upon internal confidence signals" in the absence of feedback?** 

In the absence of feedback, observers can only rely on internal estimates of performance to guide updates to their model of the reliability of their sensory apparatus (inferences about $M$). Previous work (e.g. Guggenmos et al., Elife 2106, https://doi.org/10.7554/eLife.13388) has shown that confidence signals can provide signals that drive perceptual learning in the absence of feedback. This has motivated our model simulation on the adaptive benefits of bimodal inference for metacognition, where the learning signal $\epsilon_M$ (i.e., the difference between the choice and its probability) drives inferences about $M$. Please note that we have removed this section following Comment 9 of Reviewer 1. 

### Comment 6

**And what licences the assumption that internal confidence feedback corresponds to "the absolute of the posterior log ratio" (did you mean the log of the posterior ratio)?**

We mean the absolute of the log of the posterior ratio. Following first order models (see e.g., Fleming & Daw, Self-evaluation of decision-making: A general Bayesian framework for metacognitive computation, Psychol. Rev. 2017, https://doi.org/10.1037/rev0000045), the perceptual decision and the confidence report rely on the posterior. The distance of the log of the posterior ratio $L_t$ from zero becomes a measure of decision-certainty or confidence.

### Comment 7

**I got a bit lost here when you say that "the precision of sensory coding M a function of u_t. This is largely because I couldn't find a definition of u_t.**

We apologize for this lack of clarity. In the model simulations on the adaptive benefits of bimodal inference, we generated stimuli $s_t$ from a Bernoulli-distribution with p = q = 0.5. The value of $u_t$ was then defined via equation (13), following our modeling of the human data. Please note that we have removed this section following Comment 9 of Reviewer 1. 

### Comment 8

**What licences an application of Rescorla-Wagner to learning the parameters (as in Equation 11) and, learning sensory precision as described by M_T (Equation 13). Are you moving from a Bayesian framework to a reinforcement learning framework?**

We would like to thank the reviewer for pointing out this inconsistency. We have chosen the Rescorla-Wagner learning rule for simplicity: In our model, the speed of learning about $H$ and $M$ varied according to the current mode of perceptual processing and a constant learning rate. Allowing the learning rate itself to vary as a function of preceding experiences would add an additional level of complexity that we sought to omit in this analysis. However, we fully agree that choosing a Bayesian framework (e.g., a three-level HGF) would indeed be more consistent. Please note that we have removed this section following Comment 9 of Reviewer 1. 

### Comment 9

**I am sure you have answers to these questions - but with each new question the reader is left more and more skeptical that there is a coherent story behind your analyses. It would have been more convincing had you just committed to a Bayesian filter and made your points using one update scheme, under ideal Bayesian observer assumptions.**

**Unlike your piecemeal scheme, things like the hierarchical Gaussian filter estimates the sensory and prior decisions explicitly and these estimates underwrite posterior inference. In your scheme, the sensory precision M appears to have no influence on perceptual inference (which is why, presumably you call it metacognition). The problem with this is that your motivation for systematic fluctuations in precision is weakened. This is because improved metacognition does not improve perception — it only improves the perception of perception.**

**In light of the above, can I suggest that you remove Section 5.8 and use your model in the preceding section to endorse your hypothesis along the following lines:**

**"In summary, we hypothesized that subjects have certain hyperpriors that are apt for accommodating fluctuations in the predictability of their environment; i.e., people believe that their world is inherently volatile. This means that to be Bayes optimal it is necessary to periodically re-evaluate posterior beliefs about model parameters. One way to do this is to periodically suspend the precision of prior beliefs and increase the precision afforded to sensory evidence that updates (Bayesian) beliefs about model parameters. The empirical evidence above suggests that the timescale of this periodic scheduling of evidence accumulation may be scale-invariant. This means that there may exist a timescale of periodic fluctuations in precision over every window or length of perceptual decision-making. In what follows, we model perceptual decisions under a generative model (based upon a hazard function to model historical or serial dependencies) with, a periodic fluctuation in the precision of sensory evidence relative to prior beliefs at a particular timescale. Remarkably—using Bayesian model comparison—we find that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. Furthermore, we were able to quantify the dominant timescale of periodic fluctuations; appropriate for these kinds of paradigm."**

**Note, again, I am reading your omega_LLR and omega_psi as precisions and that the periodic modulation is the hyperprior that you are characterizing—and have discovered.**

We would like to thank Prof. Friston for these very helpful and precise suggestions. In brief, we now provide a quantitative assessment of model space based on AIC (i) and have followed the suggestion of omitting section 5.8 (ii).

In addition to the qualitative assessment of our models in the initial version of our manuscript, we have conducted a formal model comparison (i). Following the model comparisons in other sections of the manuscript, we based the model comparison on AIC. We furthermore show that the winning *bimodal inference model* predicts out-of-training variables (RT and confidence) and use simulations from posterior model parameters to show that, in contrast to reduced models, the bimodal inference model neither over- nor underfits the empirical data. We have added a description of our model comparison to the Method section:

- We validated the bimodal inference model in three steps: a formal model comparison to reduced models based on AIC (Figure 1F-G; Supplemental Figure S9), the prediction of within-training (stimulus- and history-congruence) as well as out-of-training variables (RT and confidence), and a qualitative reproduction of the empirical data from model simulations based on estimated parameters (Figure 4). 

- **Model comparison**. We assessed the following model space based on AIC:

- The full *bimodal inference model* (M1; Figure 1F) incorporates the influence of sensory information according to the parameter $\alpha$ (likelihood); the integration of evidence across trials according to the parameter $H$ (prior); anti-phase oscillations in between likelihood and prior precision according to $\omega_{LLR}$ and $\omega_{\psi}$ with parameters $a_{LLR}$ (amplitude likelihood fluctuation), $a_{\psi}$ (amplitude prior fluctuation), $f$ (frequency) and $p$ (phase). 

- The *likelihood-oscillation-only model* (M2; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to parameter $H$ (prior); oscillations in likelihood precision according to $\omega_{LLR}$ with parameters $a_{LLR}$ (amplitude likelihood fluctuation), $f$ (frequency) and $p$ (phase).

- The *prior-oscillation-only model* (M3; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to parameter $H$ (prior); oscillations in the prior precision according to $\omega_{\psi}$ with parameters $a_{\psi}$ (amplitude prior fluctuation), $f$ (frequency) and $p$ (phase). Please note that all models M1-3 lead to shifts in the relative precision of likelihood and prior. 

- The *normative-evidence-accumulation* (M4; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood); the integration of evidence across trials according to parameter $H$ (prior), There are no additional oscillations. Model M4 thus corresponds to the model proposed by Glaze et al. and captures normative evidence accumulation in unpredictable environments using a Bayesian update scheme[@Glaze2015]. The comparison against M4 tests the null hypothesis that fluctuations in mode emerge from a normative Bayesian model without the ad-hoc addition of oscillations as in models M1-3.

- The *no-evidence-accumulation* (M5; Figure 1G) incorporates the influence of sensory information according to parameter $\alpha$ (likelihood). The model lacks integration of evidence across trials (flat prior) and oscillations. The comparison against M5 tests the null hypothesis that observers do not use prior information derived from serial dependency in perception. 

- **Prediction of within-training and out-of-training variables**. To validate our model, we correlated individual posterior parameter estimates with the respective conventional variables. As a sanity check, we tested (i), whether the estimated hazard rate $H$ correlated negatively with the frequency of history-congruent choices and, (ii), whether the estimated sensitivity to sensory information $\alpha$ correlated positively with the frequency of stimulus-congruent choices. In addition, we tested whether the posterior decision certainty (i.e.. the absolute of the log posterior ratio) correlated negatively with RTs and positively with confidence. This allowed us to assess whether our model could explain aspects of the data it was not fitted to (i.e., RTs and confidence). 

- **Simulations**. Finally, we used simulations (Figure 4, Supplemental Figures S10-13) to show that all model components, including the anti-phase oscillations governed by $a_{\psi}$, $a_{LLR}$, $f$ and $p$, were necessary for our model to reproduce the characteristics of the empirical data. This enabled us to assess over- or under-fitting in the bimodal inference model and all reduced models M2-M5. We used the posterior model parameters observed for humans ($H$, $\alpha$, $a_{\psi}$, $a_{LLR}$, $f$, $p$ and $\zeta$) to define individual parameters for simulation in `r nrow(Optim_Behav)` simulated participants (i.e., equivalent to the number of human participants). For each participant, the number of simulated trials was drawn at random between 300 to 700. Inputs $s$ were drawn at random for each trial, such that the sequence of inputs to the simulation did not contain any systematic seriality. Noisy observations $u$ were generated by applying the posterior parameter $\alpha$ to inputs $s$, thus generating stimulus-congruent choices in $`r mean(Sim_Behav$Accuracy, na.rm = TRUE)`$ ± $`r sd(Sim_Behav$Accuracy, na.rm = TRUE)/nrow(Sim_Behav)`$% of trials. Choices were simulated based on the trial-wise choice probabilities $y_{p}$ obtained from our model. Simulated data were analyzed in analogy to the human and mouse data. As a substitute of subjective confidence, we computed the absolute of the trial-wise log posterior ratio $|L|$ (i.e., the posterior decision certainty).

We have also added a graphical depiction of the model space to Figure 1 (subpanels F and G): 

![Figure 1 F-G](./Material/model_space.png)

- F. The bimodal inference model (M1) explains fluctuations between externally- and externally-biased modes (left panel) by two interacting factors: a normative accumulation of evidence according to parameters $H$ (middle panel), and anti-phase oscillations in the precision terms $\omega_{LLR}$ and $\omega_{\psi}$ (right panel). 

- G. The control models M2-M5 were constructed by successively removing the anti-phase oscillations and the integration of information from the bimodal inference model. Please note that the normative-evidence-accumulation-model (M4) corresponds to the model proposed by Glaze et al.[@Glaze2015]. In the no-evidence-accumulation model (M5), perceptual decisions depend only on likelihood information (flat priors). 

The formal model comparison yielded clear evidence for a superiority of the bimodal inference model, in particular over the normative Bayesian model of evidence accumulation. The model successfully predicted both within-training variables (as a sanity-check) and out-of-training variables. Simulations from posterior model parameters closely followed the empirical data (Figure 4), which was not the case for reduced models (Supplemental Figures S10-13). We summarize these findings in the Results section and have added a Supplemental Figure S9 to show the distribution of observer-level AIC at the session-level (see below):

- We used a maximum likelihood procedure to fit the bimodal inference model (M1, Figure 1F) to the behavioral data from the Confidence database[@Rahnev2020] and the IBL database[@IBL2021], optimizing the parameters $\alpha$, $H$, $amp_{LLR}$, $amp_{\psi}$, $f$, $p$, and $\zeta$ (see Methods for details and Supplemental Table T2 for a summary of the parameters of the bimodal inference model). We validated our model in three steps: First, to show that bimodal inference does not emerge spontaneously in normative Bayesian models of evidence accumulation, but requires the ad-hoc addition of anti-phase oscillations in prior and likelihood precision, we compared the bimodal inference model to four control models (M2-5, Figure 1G). In these models, we successively removed the anti-phase oscillations (M2-M4) and the integration of information across trials (M5) from the bimodal inference model and performed a model comparison based on AIC. 

- Model M2 ($AIC_2$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in mice) and Model M3 ($AIC_3$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in mice) incorporated only oscillations of either likelihood or prior precision. Model M4 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model M5 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information (Figure 1G).

- The bimodal inference model achieved the lowest AIC across the full model space ($AIC_1$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in mice) and was clearly superior to the normative Bayesian model of evidence accumulation ($\delta_{AIC}$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice; Supplemental Figure S9).

- As a second validation of the bimodal inference model, we tested whether the posterior model predicted within-training and out-of-training variables. The bimodal inference model characterizes each subject by a sensitivity parameter $\alpha$ (humans: $\alpha$ = $`r mean(Optim_Behav$Precision, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$Precision, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $\alpha$ = $`r mean(Optim_M_Behav$Precision, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$Precision, na.rm = TRUE)/nrow(Optim_M_Behav)`$) that captures how strongly perception is driven by the available sensory information, and a hazard rate parameter $H$ (humans: $H$ = $`r mean(Optim_Behav$Hazard, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$Hazard, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $H$ = $`r mean(Optim_M_Behav$Hazard, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$Hazard, na.rm = TRUE)/nrow(Optim_M_Behav)`$) that controls how heavily perception is biased by perceptual history. The parameter $f$ captures the dominant time scale at which likelihood (amplitude humans: $a_{LLR}$ = $`r mean(Optim_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $a_{LLR}$ = $`r mean(Optim_M_Behav$amp_LLR, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_LLR, na.rm = TRUE)/nrow(Optim_M_Behav)`$) and prior precision (amplitude humans: $a_{\psi}$ = $`r mean(Optim_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_Behav)`$; mice: $a_{\psi}$ = $`r mean(Optim_M_Behav$amp_prior, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$amp_prior, na.rm = TRUE)/nrow(Optim_M_Behav)`$) fluctuated and was estimated at $`r mean(Optim_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_Behav$frequency, na.rm = TRUE)/nrow(Optim_Behav)`$ 1/$N_{trials}$ and $`r mean(Optim_M_Behav$frequency, na.rm = TRUE)`$ ± $`r sd(Optim_M_Behav$frequency, na.rm = TRUE)/nrow(Optim_M_Behav)`$ 1/$N_{trials}$ in mice.

- As a sanity check for model fit, we tested whether the frequency of stimulus- and history-congruent trials in the Confidence database[@Rahnev2020] and IBL database[@IBL2021] correlate with the estimated parameters $\alpha$ and $H$, respectively. As expected, the estimated sensitivity toward stimulus information $\alpha$ was positively correlated with the frequency of stimulus-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r STAT.Optim_Accuracy_Precision$coefficients[2,5]`$; mice: $\beta$ = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,1]`$ ± $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,2]`$, T($`r M_STAT.Optim_Accuracy_Precision$coefficients[2,3]`$) = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,4]`$, p = $`r M_STAT.Optim_Accuracy_Precision$coefficients[2,5]`$). Likewise, $H$ was negatively correlated with the frequency of history-congruent perceptual choices (humans: $\beta$ = $`r STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r STAT.Optim_History_Hazard$coefficients[2,5]`$; mice:  $\beta$ = $`r M_STAT.Optim_History_Hazard$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_Hazard$coefficients[2,2]`$, T($`r M_STAT.Optim_History_Hazard$coefficients[2,3]`$) = $`r M_STAT.Optim_History_Hazard$coefficients[2,4]`$, p = $`r M_STAT.Optim_History_Hazard$coefficients[2,5]`$).

- Our behavioral analyses reveal that humans and mice show significant effects of perceptual history that impaired performance in randomized psychophysical experiments[@Kiyonaga2017; @Urai2017; @Braun2018; @Abrahamyan2016; @Bergen2019] (Figure 2A and 3A). We therefore expected that humans and mice underestimated the true hazard rate $\hat{H}$ of the experimental environments (Confidence database[@Rahnev2020]: $\hat{H}_{Humans}$ = `r mean((100 - Behav$Stimulus_History)/100, na.rm = TRUE)` ± `r sd((100 - Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(Behav)`); IBL database[@IBL2021]: $\hat{H}_{Mice}$ = `r mean((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)` ± `r sd((100 - M_Behav$Stimulus_History)/100, na.rm = TRUE)/nrow(M_Behav)`). Indeed, when fitting the bimodal inference model to the trial-wise perceptual choices, we found that the estimated (i.e., subjective) hazard rate $H$ was lower than $\hat{H}$ for both humans ($\beta$ = $`r STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r STAT.Optim_Underestimation$coefficients[1,5]`$) and mice ($\beta$ = $`r M_STAT.Optim_Underestimation$coefficients[1,1]`$ ± $`r M_STAT.Optim_Underestimation$coefficients[1,2]`$, T($`r M_STAT.Optim_Underestimation$coefficients[1,3]`$) = $`r M_STAT.Optim_Underestimation$coefficients[1,4]`$, p = $`r M_STAT.Optim_Underestimation$coefficients[1,5]`$).

- To further probe the validity of the bimodal inference model, we tested whether posterior model quantities could explain aspects of the behavioral data that the model was not fitted to. We predicted that the posterior decision variable $L_t$ not only encodes perceptual choices (i.e., the variable used for model estimation), but should also predict the speed of response and subjective confidence[@Kepecs2008; @Braun2018]. Indeed, the estimated trial-wise posterior decision certainty $|L_t|$ correlated negatively with RTs in humans ($\beta$ = $`r STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_RT_Certainty$coefficients[2,5]`$) and TDs mice ($\beta$ = $`r M_STAT.Optim_RT_Certainty$coefficients[2,1]`$ ± $`r M_STAT.Optim_RT_Certainty$coefficients[2,2]`$, T($`r M_STAT.Optim_RT_Certainty$coefficients[2,3]`$) = $`r M_STAT.Optim_RT_Certainty$coefficients[2,4]`$, p = $`r M_STAT.Optim_RT_Certainty$coefficients[2,5]`$). Likewise, subjective confidence reports were positively correlated with the estimated posterior decision certainty in humans ($\beta$ = $`r STAT.Optim_Confidence_Certainty$coefficients[2,1]`$ ± $`r STAT.Optim_Confidence_Certainty$coefficients[2,2]`$, T($`r STAT.Optim_Confidence_Certainty$coefficients[2,3]`$) = $`r STAT.Optim_Confidence_Certainty$coefficients[2,4]`$, p = $`r STAT.Optim_Confidence_Certainty$coefficients[2,5]`$). 

- The dynamic accumulation of information inherent to our model entails that biases toward perceptual history are stronger when the posterior decision certainty at the preceding trial is high[@Glaze2015; @Braun2018; @Bergen2019]. Due to the link between posterior decision certainty and confidence, we reasoned that confident perceptual choices should be more likely to induce history-congruent perception at the subsequent trial[@Braun2018; @Bergen2019]. Indeed, logistic regression indicated that history-congruence was predicted by the posterior decision certainty $|L_{t-1}|$  (humans: $\beta$ = $`r STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_mu_minus_1$coefficients[2,4]`$; mice: $\beta$ = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,1]`$ ± $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,2]`$, z = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,3]`$, p = $`r M_STAT.Optim_History_mu_minus_1$coefficients[2,4]`$) and subjective confidence (humans: $\beta$ = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,1]`$ ± $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,2]`$, z = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,3]`$, p = $`r STAT.Optim_History_Confidence_minus_1$coefficients[2,4]`$) at the preceding trial.

- As a third validation of the bimodal inference model, we used the posterior model parameters to simulate synthetic perceptual choices and repeated the behavioral analyses conducted for the empirical data. Simulations from the bimodal inference model closely replicated our empirical results: Simulated perceptual decisions resulted from a competition of perceptual history with incoming sensory signals (Figure 4A). Stimulus- and history-congruence were significantly autocorrelated (Figure 4B-C), fluctuating in anti-phase as a scale-invariant process with a 1/f power law (Figure 4D-F). Simulated posterior certainty[@Kepecs2008; @Urai2017; @Braun2018] (i.e., the absolute of the log posterior ratio $|L_t|$) showed a quadratic relationship to the mode of sensory processing (Figure 4H), mirroring the relation of RTs and confidence reports to external and internal biases in perception (Figure 2G-H and Figure 3G-H). Crucially, the overlap between empirical and simulated data broke down when we removed the anti-phase oscillations or the accumulation of evidence over time from the bimodal inference model (Supplemental Figure S10-13).

- In sum, computational modeling suggested that between-mode fluctuations are best explained by two interlinked processes (Figure 1E): (i), the dynamic accumulation of information across successive trials mandated by normative Bayesians model of evidence accumulation and, (ii), ongoing anti-phase oscillations in the impact of external and internal information.

```{r Supplemental_Figure_S9_print, cache = TRUE}
lay <- rbind(c(1), c(2))
grid.arrange(
p_distribution_Sum_Model_AIC,
p_distribution_Sum_Subj_Model_AIC,
layout_matrix = lay,
heights = c(0.75,1),
widths = c(1))
```

- **Supplemental Figure S9**. Comparison of the bimodal inference model against reduced control models. 

- A. Group-level AIC. The bimodal inference model (M1) achieved the lowest AIC across the full model space ($AIC_1$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in mice). Model M2 ($AIC_2$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in mice) and Model M3 ($AIC_3$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in mice) incorporated only oscillations of either likelihood or prior precision. Model M4 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model M5 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information. 

- B. Subject-level AIC. Here, we show the distribution of AIC values at the subject-level. AIC for the bimodal inference model tended to be smaller than AIC for the comparator models (statistical comparison to the second-best model M2 in humans: $\beta$ = $`r -STAT.M1_M2$coefficients[2,1]`$ ± $`r STAT.M1_M2$coefficients[2,2]`$, T($`r STAT.M1_M2$coefficients[2,3]`$) = $`r -STAT.M1_M2$coefficients[2,4]`$, p = $`r STAT.M1_M2$coefficients[2,5]`$; mice:  T(`r M_STAT.M1_M2$parameter`) = `r M_STAT.M1_M2$statistic`, p = $`r M_STAT.M1_M2$p.value`$).

In light of our response to Comments 1 - 9 of Reviewer 1, we agree that a complete and extensive investigation of the relation between bimodal inference, learning about changes in the environment and the relation to metacognition may be beyond the scope of the current manuscript: Both Reviewer 1 and 3 (see below) have shared that the manuscript is too long and should be streamlined. Yet evaluating the full model space (e.g., comparing update rules for inferences about $H$ and $M$, testing for an influence of beliefs about $M$ on learning about $H$ etc.) would make the manuscript even longer. We are therefore happy to follow Prof. Fristons suggestions to omit section 5.8. We have changed the manuscript in the following ways:

When introducing $\omega_{LLR}$, we identify it as the precision afforded to the likelihood, referring to the Bayesian framework, and refer to fluctuations in mode as a hyperprior.

- To allow for *bimodal inference*, i.e., alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$. These anti-phase sine functions (defined by amplitudes $a_{LLR/\psi}$, frequency $f$ and phase $p$) determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014a]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$.

We have deleted the section 5.8 and added a summary of our modeling approach to the discussion, closely following the text recommended by Prof. Friston: 

- (...) Yet relying too strongly on serial dependencies may come at a cost: When accumulating over time, internal predictions may eventually override external information, leading to circular and false inferences about the state of the environment[@Jardri2017]. Akin to the wake-sleep-algorithm in machine learning[@Bengio2015], bimodal inference may help to determine whether errors result from external input or from internally-stored predictions: During internal mode, sensory processing is more strongly constrained by predictive processes that auto-encode the agent's environment. Conversely, during external mode, the network is driven predominantly by sensory inputs[@Honey2017]. Between-mode fluctuations may thus generate an unambiguous error signal that aligns internal predictions with the current state of the environment in iterative test-update-cycles[@Bengio2015]. On a broader scale, between-mode fluctuations may thus regulate the balance between feedforward versus feedback contributions to perception and thereby play a adaptive role in metacognition and reality monitoring[@Dijkstra2021].

- We hypothesized that observers have certain hyperpriors that are apt for accommodating fluctuations in the predictability of their environment, i.e., people believe that their world is inherently volatile. To be Bayes optimal, it is therefore necessary to periodically re-evaluate posterior beliefs about the parameters that define an internal generative model of the external sensory environment. One way to do this is to periodically suspend the precision of prior beliefs and increase the precision afforded to sensory evidence, thus updating Bayesian beliefs about model parameters.

- The empirical evidence above suggests that the timescale of this periodic scheduling of evidence accumulation may be scale-invariant. This means that there may exist a timescale of periodic fluctuations in precision over every window or length of perceptual decision-making. Bimodal inference predicts perceptual decisions under a generative model (based upon a hazard function to model serial dependencies between subsequent trials) with periodic fluctuations in the precision of sensory evidence relative to prior beliefs at a particular timescale. Remarkably, a systematic model comparison based on AIC indicated that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. This ad-hoc addition of oscillations to a normative Bayesian model of evidence accumulation[@Glaze2015] allowed us to quantify the dominant timescale of periodic fluctuations mode at approximately 0.11 1/$N_{trials}$ in humans and mice that is appropriate for these kinds of paradigms.

Following the deletion of section 5.8, we have adapted the last sentence of the abstract and the last paragraph of the introduction: 

- (...) We propose that between-mode fluctuations generate unambiguous error signals that enable optimal inference in volatile environments.

- When less sensitive to external stimulus information, humans and mice did not behave more randomly, but showed stronger serial dependencies in their perceptual choices[@fischer_serial_2014; @Liberman2014; @Abrahamyan2016; @Cicchini2014; @Cicchini2017; @Fritsche2020; @Urai2017; @Akrami2018; @Braun2018; @Bergen2019; @Urai2019; @Hsu2020]. These serial dependencies may be understood as driven by internal predictions that reflect the autocorrelation of natural environments[@Dong1995] and bias perception toward preceding experiences[@Burr2014; @Braun2018; @Bergen2019]. Computational modeling indicated that ongoing changes in perceptual performance may be driven by systematic fluctuations between externally- and internally-oriented modes of sensory analysis. We suggest that such *bimodal inference* may help to build stable internal representations of the sensory environment despite an ongoing stream of sensory information.

### Comment 11

**This begs the question as to whether you want to pursue the 1/f story. You refer to this as "noise". However, there is no noise in this setup. I think what you meant was that the fluctuations are scale free, because they evinced a power law. I am sure that there are scale free aspects of these kinds of hyperpriors; however, in the context of your paradigm I wonder whether you should just ignore the scale free aspect and focus on your estimated temporal scale implicit in f. This means you don't have to hand wave about self-organized criticality in the discussion and focus upon your hypothesis.**

We would like to thank the reviewer for this suggestion. We agree that the discussion of self-organized criticality is far from the data. We have omitted this section from the discussion. With respect to $f$, we have adapted the manuscript to make clear that it captures the dominant timescale of fluctuations in mode:

- (...) This implements a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)

-  (...) The parameter $f$ captured the dominant time scale at which likelihood and prior precision were shifted against each other and was estimated at 0.11 1/$N_{trials}$ in both humans and mice.

- Remarkably, a systematic model comparison based on AIC indicated that a model with fluctuating precisions has much greater evidence, relative to a model in the absence of fluctuating precisions. The ad-hoc addition of oscillations to a normative Bayesian model of evidence accumulation[@Glaze2015] allowed us to quantify the dominant timescale of periodic fluctuations mode at approximately 0.11 1/$N_{trials}$ in humans and mice that is appropriate for these kinds of paradigms.

### Comment 12

**A final move—to make the paper more focused and digestible—would be to put a lot of your defensive analyses (e.g. about general arousal et cetera) in supplementary material. You have to be careful not to exhaust the reader by putting up a lot of auxiliary material before the important messages in your report.**

We have followed this suggestion and move the following sections to the Supplement: section 5.3 (Internal and external modes of processing facilitate response behavior and enhance confidence in human perceptual decision-making), section 5.4 (Fluctuations between internal and external mode modulate perceptual performance beyond the effect of general response biases), section 5.5 (Internal mode is characterized by lower thresholds as well as by history-dependent changes in biases and lapses). We have also moved secondary statistics to the figure legends and to the Supplement.  

**Minor points**

### Comment 13

**I cannot resist suggesting that you change your title to "Bimodal Inference in Mice and Men"**

We would like to thank the Reviewer for this suggestion and agree that this would indeed sound great. However, we are worried that changing the title to *mice and men* would not be as gender-neutral as *humans and mice*. We would therefore propose to stay with the current title. If we are mistaken and *mice and men* can be considered gender-neutral (we are non-native speakers), we would be happy to change the title. 

### Comment 14

**Please replace "infra-slow fluctuations" with "slow fluctuations". Slow has some colloquial meaning in fMRI studies but not in any scale free context.**

Done.

### Comment 15

**Please replace "simulated data" with "simulations" in the abstract. Finally, please replace "robust learning and metacognition in volatile environments" with "enable optimal inference and learning in volatile environments."**

Done. Since we have followed the suggestion to delete section 5.8, we have rephrased the last paragraph of the abstract into:

- (...) We propose that between-mode fluctuations generate unambiguous error signals that enable optimal inference in volatile environments.

### Comment 16

**Line 50, please replace "about the degree of noise inherent in encoding of sensory information" with "the precision of sensory information relative to prior (Bayesian) beliefs."**

Done. 

### Comment 17

**Line 125: please replace "a source of error" with "a source of bias"**

Done.

### Comment 18

**Line 141: please replace "one 1/f noise" with a scale-invariant process with a 1/f power law" (here and throughout) this is not "noise" it is a particular kind of fluctuation.**

Done.

### Comment 19

**Line 178, when you say that the fluctuations may arise due to "changes in level of tonic arousal or on-task attention", I think you need to qualify this. In predictive processing, on-task attention is exactly the modulation of sensory precision, relative to prior precision that you are characterizing here. Tonic arousal may be another thing may or may not confound your current results.**

Thank you very much for pointing this out. We have adapted the discussion to make the distinction between attention in the predictive processing sense and the broader issue of task engagement (reflecting fluctuations in arousal, fatigue etc.) clearer (see also our responses above): 

- As a functional explanation for bimodal inference, we propose that perception temporarily disengages from internal predictions to form stable inferences about the statistical properties of the sensory environment. Between-mode fluctuations may thus elude circular inferences that occur when both the causes and the encoding of sensory stimuli are volatile[@Jardri2017; @Weilnhammer2021a]). By the same token, we suggest that fluctuations in mode occur at the level of perceptual processing[@St.John-Saaltink2016; @Cicchini2017; @Braun2018; @Cicchini2021], and are not a passive phenomenon that is primarily driven by factors situated up- or downstream of sensory analysis.

- How does attention relate to between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. As outlined above, between-mode fluctuations can be understood as ongoing shifts in the precision afforded to likelihood (*external mode*) and prior (*internal mode*), respectively. When the precision afforded to prior or likelihood increases, posterior precision increases, which leads to faster RTs and higher confidence. When defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may thus provide a plausible explanation for the quadratic relationship between mode and RTs and confidence (Figure 2H and J; Figure 3I; Figure 3I).

- Outside of the predictive processing field, attention is often understood in the context of task engagement[@Matthews2010], which varies according to the availability of cognitive resources that are modulated by factors such as tonic arousal, familiarity with the task, or fatigue[@Matthews2010]. Our results suggest that internal mode processing cannot be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, choices during internal mode were not random or globally biased, but driven by perceptual history (Supplemental Figures S6-7). Moreover, our computational model identified the dominant timescale of between-mode fluctuations at 0.11 1/$N_{trials}$, which may be compatible with fluctuations in arousal[@McGinley2015], but is faster than to be expected for the development of task familiarity or fatigue. 

- However, in interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects on task performance[@Beerendonk2023]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. 

- Of note, high phasic arousal has been shown to suppress multi-domain biases in decision-making in humans and mice[@Gee2014; @Gee2017; @Gee2020], including the biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. The increase in response speed and history congruence over time (Supplemental Section 9.4) may argue against insufficient training as an alternative explanation for internal mode processing, but may also taken as a sign of waning arousal. The multiple mechanistic mappings to RTs and confidence therefore warrant more direct measures of arousal (such as pupil size[@Gee2014; @McGinley2015b; @Urai2017; @Gee2020; @Gee2022; @Beerendonk2023], motor behavior[@McGinley2015b; @Gee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.

### Comment 20

**When introducing Equation 2, please make it clear that the omega terms stand in for the precisions afforded to the likelihood (omega_LLR) and prior (omega_psi) that constitute the log posterior.**

We have modified the introduction of equation 2 as follows: 

- Following Bayes' theorem, we reasoned that binary perceptual decisions depend on the log posterior ratio $L$ of the two alternative states of the environment that participants learn about via noisy sensory information[@Glaze2015]. We computed the posterior by combining the sensory evidence available at time-point $t$ (i.e., the log likelihood ratio $LLR$) with the prior probability $\psi$, weighted by the respective precision terms $\omega_{LLR}$ and $\omega_{\psi}$:

**You can then motivate Equation 6 and 7 as implementing the hyperprior in which the sensory and prior precisions fluctuate at a particular time scale**.

We would like to thank the reviewer for this suggestion, which we have added to the introduction of equations (6) and (7): 

- To allow for *bimodal inference*, i.e., alternating periods of internally- and externally-biased modes of perceptual processing that occur irrespective of the sequence of preceding experiences, we assumed that likelihood and prior vary in their influence on the perceptual decision according to fluctuations governed by $\omega_{LLR}$ and $\omega_{\psi}$. These anti-phase sine functions (defined by amplitudes $a_{LLR/\psi}$, frequency $f$ and phase $p$) determine the precision afforded to the likelihood and prior[@Feldman2010]. The implicit anti-phase fluctuations are mandated by Bayes-optimal formulations in which inference depends only on the relative values of prior and likelihood precision (i.e., the Kalman gain[@Mathys2014a]). As such, $\omega_{LLR}$ and $\omega_{\psi}$ implement a hyperprior[@Friston2005] in which the likelihood and prior precisions are shifted against each other at a dominant timescale defined by $f$: (...)

### Comment 21

**You can also point out that the implicit anti-phase fluctuations are mandated by Bayes optimal formulations in which it is only the relative values of the prior and sensory precision that matter. Bayesian filters these precisions constitute the Kalman gain. You can find a derivation of why this in treatments of the hierarchical Gaussian filter is by Mathys et al.**

We would like to thank the reviewer for this suggestion. We added this information to the description of our model in the Results section (see comment above).

### Comment 22

**In your first model simulations, I would make it clear in the main text which parameters you are optimizing's; namely (H, alpha, a_likelihood, a_prior f). Perhaps a little table with a brief description of the meaning of these hyper parameters would be useful?**

We now identify the optimized parameters at the outset of the modeling section: 

- (...) We used a maximum likelihood procedure to fit the bimodal inference model (M1, Figure 1F) to the behavioral data from the Confidence database[@Rahnev2020] and the IBL database[@IBL2021], optimizing the parameters $\alpha$, $H$, $amp_{LLR}$, $amp_{\psi}$, $f$, $p$, and $\zeta$ (see Methods for details and Supplemental Table T2 for a summary of the parameters of the bimodal inference model). We validated our model in three steps: (...).

We furthermore included a table summarizing the model parameters in the Supplement. 

```{r Supplemental_Table_T2_print, cache = TRUE}
# library(kableExtra)
# kbl(Parameters, format = "latex", longtable = T, booktabs = T) %>%
# kable_styling(font_size = 7, latex_options = c("repeat_header"))
```

### Comment 23

**Please remove Section 5.8. If you do not, you need to explain why — on line 586 - setting a = 0 is appropriate when a = 0, the log posterior in Equation 2 is zero because the precisions (omegas) are zero (by Equations 6 and 7).**

We have removed the section 5.8. When setting the amplitude parameters to zero, $\omega_{LLR}$ and $\omega_{\psi}$ are constant at 1, creating a unimodal control model that corresponds to the normative Bayesian evidence accumulation model proposed by Glaze et al[@Glaze2015].  

## Reviewer 2

**The authors elucidate whether periodicities in the sensitivity to external information represent an epiphenomenon of limited processing capacity or, alternatively, result from a structured and adaptive mechanism of perceptual inference. Analyzing large datasets of perceptual decision-making in humans and mice, they investigated whether the accuracy of visual perception is constant over time or whether it fluctuates. The authors found significant autocorrelations on the group level and on the level of individual participants, indicating that a stimulus-congruent response in a given trial increased the probability of stimulus-congruent responses in the future. Furthermore, the authors addressed whether observers cycle through periods of enhanced and reduced sensitivity to external information or whether observers rely on internal information in certain phases. This was quantified by whether a response at a given trial was correlated with responses in previous trials. The authors used computational modeling to infer the origin of the different modes (internal vs. external).**

**Evaluation**

**This is a very interesting and well-written manuscript, dealing with an important question. The findings are novel and provide an innovative account of interpreting visual perception. I am not an expert in modeling, so I will restrict my comments to theoretical framework and the experimental approach. I have a few minor questions that I would like the authors to answer or clarify.**

We would like to thank the reviewer for the evaluation of our manuscript. We have added the discussion of potential effects at the motor-level to our discussion. 

**Minor questions**

### Comment 1

**History congruent perception was defined on the basis of response repetitions. Are we really sure that responses are repeated due to some variant of a perceptual decision process (internal or external) or may arise on the motor-level - independent of a perceptual source? For instance, a response primed by residual activation in the motor system may represent a local effect independent from a general response bias. If indeed, a response repetition is initiated by whatever reasons (non-perceptual), wouldn't this imply that the repeated response is per se more related to previous than to current visual information and would hence signal a reduced sensitivity to current external information? The authors are discussing the option of stereotypically repeated responses in the context of alertness. However, a tendency to repeat responses may arise due to other reasons. For instance, may the motor priming effects mentioned possibly explain faster RTs along with a stronger bias when in internal-mode.**

Thanks a lot for pointing this out. In this manuscript, we attempt to characterize the phenomenon of bimodal inference at the level of behavior: The Confidence database consists only of behavioral data. At the time of publishing this paper as a preprint, the IBL database had also released only behavioral data. 

We realize that it is very difficult to preclude all influences from effects that occur at the level of behavior. Not all studies in the confidence database have used a counter-balanced mapping between the perceptual decision and the associated motor-response. In the IBL data, the mapping between the perceptual decision and the associated motor-response is fixed (turning a response wheel left or right depending on the perceived location of a grating). Confidence reports also provide only indirect information on motor- vs. perceptual effects. One may speculate that, if a response was driven by residual activity in the motor system, it may be more likely to be a lapse and be accompanied by reduced confidence. By contrast, we found that confidence was, on average, elevated for history-congruent choices. 

That being said, behavioral analyses alone are insufficient to rule out the contribution of motor-related effects to seriality in choices. This would require analyses of additional types of data, such as video tracking of the motor response or even neural data collected in brain areas directly related to motor behavior. While this analysis is beyond the scope of the present manuscript, we plan to carry out these analyses using the recent data publication of the IBL, that contains, among others, video tracking of the motor response (turning of the response wheel) and neuropixel recording across the whole brain, including premotor and motor cortex[@IBL2023]. 

We have added these considerations to the discussion of potential confounds:

- Residual activation of the motor system may provide another contribution to serial biases in perceptual choices[@Mawase2018]. Such motor-driven priming may lead to errors in randomized psychophysical designs, resembling the phenomenon that we identify as internally-biased processing[@Pomper2023]. Moreover, residual activation of the motor system may lead to faster responses, and thus constitutes an alternative explanation for the quadratic relationship of mode with RTs[@Mawase2018]. The observation of elevated confidence for stronger biases toward internal mode speaks against the proposition that residual activation of the motor system is the primary driver of serial choice biases, since strong motor-driven priming should lead to frequent lapses that are typically associated reduced confidence[@Kepecs2012]. Likewise, perceptual history effects have repeatedly been replicated in experiments with counter-balanced stimulus-response mappings[@Braun2018: @Feigin2021].

- **No-response paradigms, in which perceptual decision are inferred from eye-movements alone, could help to better differentiate perceptual from motor-related effects. Likewise, video-tracking of response behavior and neural recording from motor- and premotor, which has recently been released for the IBL database[@IBL2023], may provide further insight into the relation of motor behavior to the perceptual phenomenon of between-mode fluctuations.**  

## Reviewer 3 

**In this paper the authors propose that during perceptual decisions, humans and mice exhibit regular oscillatory fluctuations  between an "external"  (that places more weight on the perceptual evidence) and an "internal" (that places more weight on historical experiences) mode. In particular, the authors propose a computational scheme in which the influences of history and current stimulus on choice oscillate in anti phase, effectively implementing "bimodal inference". The computational advantages of these scheme as well as its relation to the underlying neurophysiology are discussed.**

**Overall, the authors make a very interesting proposal about what drives slow fluctuations in perceptual performance during randomized two-alternative choice tasks. This proposal relates changes in accuracy with changes in serial choice biases, which is a timely and synthesizing contribution. Furthermore, this proposal is backed by analyses over several human datasets  and a large dataset in mice.** 

**Despite its strong empirical contribution, the paper seems limited by the fact that alternative computational hypotheses are not adequately considered (or at least considered in a systematic way). At the same time, and although the paper is well written, some parts are overly technical.**

We would like to thank the Reviewer for the very helpful comments on our manuscript. We fully agree that the previous version of our manuscript did not consider alternative computational hypotheses in a systematic and adequate way. As we outline in more detail in our point-by-point-responses below, we have addressed this issue by adding a formal model comparison of the bimodal inference model to reduced models, including a normative models of Bayesian evidence accumulation. We have added null-hypothesis-testing for enhanced history-congruence during internal mode. We have also moved a number of defensive analyses to the Supplement. 

**Major comments:**

### Comment 1

**The authors collapse across various datasets in which different tasks were employed. However, some details on the nature of these different tasks and a discussion on the rationale of collapsing behavioral metrics across them is missing. The authors mention that all tasks involved binary perceptual decisions. In some parts of the manuscript the term "false alarms" is mentioned, indicating a detection protocol. Other terms in the methods section (e.g., "set size") might need further clarification. Importantly, it is not clear how reaction times were calculated in the various tasks and whether some experiments involved free response paradigms while others interrogation/ cued paradigms (in which case RTs can be defined as the latency between the response cue and the response).**

We would like to thank the Reviewer for this important point. Regarding the rationale for collapsing across the studies in the Confidence database: Having found strong evidence of apparent between-mode fluctuations in a study on intermittent bistable perception[@Weilnhammer2021a], our goal was to test whether between-mode fluctuations were a general phenomenon in perceptual decision-making. We thus had a lenient threshold for including studies from the Confidence database, i.e., all that addressed the domain of perception in 2AFC tasks. This has the advantage of looking at perceptual 2AFC decisions in general and indicated that history effects and bimodal inference are present in a wide variety of tasks. 

At the same time, collapsing across diverse set of experiments means that the stimuli, the timing and the way that responses were collected differed between them. To account for this variability, we included individual experiments as random factors in linear mixed modeling. We also made sure that perceptual performance is comparable across studies (Supplemental Figure S1A-B). While our analysis of the IBL database was mainly motivated by investigating bimodal inference across species, it also allowed us to replicate our results in a highly standardized task that was collected across many individual subjects[@IBL2021]. We have added the rationale for collapsing across studies with its advantages and disadvantages to the *Open questions and limitations* subsection of the discussion:

- Our results suggest bimodal inference as a pervasive aspect of perceptual decision-making in humans and mice. However, a number of limitations and open questions have to be considered: First, this work sought to understand whether fluctuations between internal and external mode, which we initially observed in an experiment on bistable perception in humans[@Weilnhammer2021a], represent a general phenomenon that occurs across a diverse set of perceptual decision-making tasks. Our analysis of the Confidence database[@Rahnev2020] therefore collapsed across all available experiments on binary perceptual decision-making. Individual experiments differed with respect to the stimuli, the manipulation of difficulty, the timing of trials, and the way responses were collected, but were highly comparable with respect to the central variables of stimulus- and history-congruence (Supplemental Figure S1A-B).

- The variability across experiments, which we considered as random effects in all statistical analyses, enabled us to assess whether bimodal inference represents a general phenomenon in perceptual decision-making, but limited the precision at which we were able to investigate the relation of mode to behavioral variables such as timing, task difficulty, RT or confidence. This issue is partially resolved by our analyses of the IBL database, which replicated our findings in an experiment that was highly standardized with respect to timing, task difficulty, and behavioral read-out[@IBL2021]. It will be an important task for future research to validate our results on bimodal inference in a standardized dataset of comparable volume in humans, which is, to our knowledge, not yet available.

We apologize for the lack of clarity regarding the way we queried the Confidence database. To select a broad variety of experiments on 2AFC perceptual decision-making, we queried the Confidence database for studies from the *perception* category (excluding studies from the categories *cognitive*, *motor*, *memory* and *mixed*) and selected studies with 2AFC responses. Our previous version of the manuscript had mentioned the variable names given to the difficulty variable in the individual experiments (i.e., the name of the column in the individual *.txt* files provided for every experiment in the Confidence database). The term *false alarm* turned up in the discussion of lapses of attention and arousal, which we have re-written in response to Comment 19 by Reviewer 1 and Comment 6 by Reviewer 3. We now provide more information on the experiments and variables selected in the Method section:

- We downloaded the human data from the Confidence database[@Rahnev2020] on 10/21/2020, limiting our analyses to the category *perception*. Within this category, we selected studies in which participants made binary perceptual decisions between two alternatives. We excluded two experiments in which the average perceptual accuracy fell below 50%. After excluding these experiments, our sample consisted of $`r n_trials_human/100000`$ million trials obtained from `r n_participants_human` human participants and `r n_studies_human` individual experiments (Supplemental Table 1). Out of the `r n_studies_human` included experiments, `r sum(Exp_rep$domain == "Vision")` investigated visual, `r sum(Exp_rep$domain == "Auditory")` auditory, `r sum(Exp_rep$domain == "Vestibular self-motion")` proprioceptive, and `r sum(Exp_rep$domain == "Multimodal")` multimodal perception. `r sum(Exp_rep$Decision_type == "Discrimination")` experiments were based on discrimination and `r sum(Exp_rep$Decision_type == "Detection")` on detection, with one investigating both.

- Out of the `r sum(Exp_rep$RT_available == 1)` experiments that provide information on RTs, `r sum(Exp_rep$RT_type == "cued")` cued the response by the onset of a response screen or an additional response cue, whereas `r sum(Exp_rep$RT_type == "free")` allowed participants to respond at any time after stimulus onset. 

- `r sum(Exp_rep$Difficulty_available == 0)` of the `r nrow(Exp_rep)` included experiments used fixed difficulty levels, whereas `r sum(Exp_rep$Difficulty_available == 1)` manipulated difficulty levels within participants. Difficulty was manipulated via noise masks, contrast, luminance, presentation time, or stimulus probability for gabors, dot coherence for random dot kinematograms, difference in elements and set size for comparisons of numerosity, difference in clicks for auditory discrimination, temporal distance for meta-contrast masking, and amount of self-motion for proprioception. We treated task difficulty as a missing variable for the experiments that fixed it at the participant-level, as this precluded the computation of autocorrelation curves.

### Comment 2

**The key premise that when participants do not rely on the external stimulus they rely more on the previous trial needs to be more clearly (and statistically) contrasted against a null hypothesis. For instance, an null hypothesis could be that when participants place a lower weight on the stimulus they simply choose randomly. It is important to specify a null hypothesis such that the key premise does not appear self-evident or circular.**

We would like to thank the reviewer for highlighting this important point. Following this suggestion, we have explicitly tested our main hypothesis ($H1$: periods of reduced stimulus-congruence are periods of enhanced reliance on history-congruence) against the following null hypotheses:

- $H0_1$: Periods of reduced stimulus-congruence are periods of enhanced random choices
- $H0_2$: Periods of reduced stimulus-congruence are periods of enhanced general bias

We present three sets of statistical analyses to test H1 against $H0_{1/2}$:

First, we used logistic regression to predict individual choices. Under H1, one would expect a significant effect of perceptual history in a logistic regression model that predicts individual choices from the external stimulus, perceptual history and general response bias. At the model level, one would expect higher AIC in a model without perceptual history as a predictor of individual choices, indicating that perceptual history influences choices beyond noise ($H0_1$) and general response bias ($H0_2$).

In both humans and mice, we found a significant effect of perceptual history on choices while controlling for bias. When eliminating perceptual history as a predictor of individual choices, we found higher AIC (providing model-level evidence against $H0_1$ and $H0_2$). We complemented this analysis by computing AIC in individual observers (Supplemental Figure S4), and again found higher AIC in models from which perceptual history was eliminated. We have made the following changes to the main manuscript:

Humans:

- Subsection title: Fluctuations between internal and external mode cannot be reduced to general response biases or random choices

- The core assumption of bimodal inference - that ongoing changes in the sensitivity to external information are driven by internal predictions induced via perceptual history - needs to be contrasted against two alternative hypotheses: When making errors, observers may not engage with the task and respond stereotypically, i.e., exhibit stronger general biases toward one of the two potential outcomes, or simply choose randomly. Logistic regression confirmed that perceptual history made a significant contribution to perception ($\beta$ = $`r STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum$coefficients[3,4]`$) over and above the ongoing stream of external sensory information ($\beta$ = $`r STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward ($\beta$ = $`r STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r STAT.full_logit_Sum$coefficients[1,4]`$). When eliminating perceptual history as a predictor of individual choices at individual trials, AIC increased by $\delta_{AIC}$ = $`r -delta_AIC_log_reg_humans`$ (see Supplemental Figure S4A-B for parameter- and model-level inference at the level of individual observers).

Mice:

- In line with humans, mice were biased toward perceptual history in `r mean(M_Behav$History, na.rm = TRUE)`% ± `r sd(M_Behav$History, na.rm = TRUE)/sqrt(length(M_Behav$History))`% of trials (T(`r M_STAT.Global_History_Accuracy$parameter`) = `r M_STAT.Global_History_Accuracy$statistic`, p = $`r M_STAT.Global_History_Accuracy$p.value`$; Figure 4A and Supplemental Figure S1D). Perceptual history effects remained significant ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[3,4]`$) when controlling for external sensory information ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward one of the two potential outcomes ($\beta$ = $`r M_STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r M_STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r M_STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r M_STAT.full_logit_Sum$coefficients[1,4]`$). When eliminating perceptual history as a predictor of individual choices, AIC increased by  $\delta_{AIC}$ = $`r -delta_AIC_log_reg_mice`$, arguing against the notion that choice randomness and general response bias are the only determinants of perceptual performance (see Supplemental Figure S4C-D for parameter- and model-level inference within individual mice).

Second, we analyzed dynamic changes in history- and stimulus-congruence (i.e., smoothed probabilities for stimulus-congruence, history-congruence and general response bias in sliding 10 trial time-windows). Under H1, one would expect a significant negative correlation between the dynamic probability of stimulus- and history-congruence. At the model level, one would expect higher AIC in a model without history-congruence as a predictor of stimulus-congruence, indicating that changes in the probability of history-congruence influence stimulus-congruence beyond noise ($H0_1$) and general response bias ($H0_2$).

In both humans and mice, we found a significant negative correlation between history-congruence and stimulus-congruence while controlling for general response bias. When eliminating the dynamic probability of history-congruence as a predictor of stimulus-congruence, we found higher AIC (providing model-level evidence against $H0_1$ and $H0_2$). The section 5.4 or our original manuscript (*Fluctuations between internal and external mode modulate perceptual performance beyond the effect of general response biases*) complements these control analyses and has been moved to the Supplement to stream-line the manuscript (following the Comment 12 by Reviewer 1 and Comment 9 by Reviewer 3). We have modified the main manuscript in the following way:

Humans:

- Finally, we ensured that fluctuations in stimulus- and history-congruence are linked to each other, while controlling for fluctuations in the strength of general response biases. When perceptual choices were less biased toward external information, participants relied more strongly on internal information acquired from perceptual history (and vice versa, $\beta$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$), controlling for fluctuations in the strength of general response biases ($\beta$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$).

- (...) Likewise, eliminating the dynamic fluctuations in history-congruence as a predictor of fluctuations in stimulus-congruence yielded an increase in AIC by $\delta_{AIC}$ = $`r -delta_AIC_correlation_humans`$. These results provided model-level evidence against the null hypotheses that fluctuations in stimulus-congruence are driven exclusively by choice randomness or general response bias (see Supplemental Section 9.2 for an in-depth assessment of general response bias).

Mice:

- As in humans, fluctuations in the strength of history-congruent biases had a significant effect on stimulus-congruence ($\beta_1$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) beyond the effect of ongoing changes in general response biases ($\beta_2$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$). Eliminating the dynamic fluctuations in history-congruence as a predictor of fluctuations in stimulus-congruence resulted in an increase in AIC by $\delta_{AIC}$ = $`r -delta_AIC_correlation_mice`$. This confirmed that, in both humans and mice, perceptual performance is modulated by systematic fluctuations between externally- and internally-oriented modes of sensory processing that exist beyond general response bias (see Supplemental Section 9.2 for an in-depth assessment of general response bias).

Third, we analyzed full and history-conditioned psychometric curves in external and internal mode as well as across modes. Under our main hypothesis that periodic reductions in sensitivity to external information are driven by increases in the impact of perceptual history, one would expect (i) a history-dependent increase in biases and lapses (effects of perceptual history), and (ii), a history-independent increase in threshold (reduced sensitivity to external information). Conversely, if what we identified as internal mode processing was in fact driven by random choices, one would expect (i), a history-independent increase in lapses (choice randomness), (ii), no change in bias (no effect of perceptual history), and (iii), reduced thresholds (reduced sensitivity to external information). In both humans and mice, we observed the pattern predicted by H1. In response to the comments by Reviewer 1 and 3, we have significantly streamlined the manuscript and moved our assessment of psychometric functions to the Supplement. We now provide a summary of our results in the main manuscript to make our reasoning with respect to $H0_1$ and $H1$ more explicit:

Humans:

- To confirm that changes in the sensitivity to external information are indicative of internal mode processing, we estimated full and history-dependent psychometric curves during internal, external, and across modes. If, as we hypothesized, internal mode processing reflects an enhanced impact of perceptual history, one would expect a history-dependent increase in biases and lapses as well as a history-independent increase in threshold. Conversely, if internal mode processing were driven by random choices, one would expect a history-independent increase in lapses and threshold, and no change in bias. In line with our prediction, we found that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section 9.3.1 and Supplemental Figure S6). This confirms that internal mode processing is indeed driven by an enhanced impact of perceptual history.   

Mice:

- When fitting full and history-conditioned psychometric curves to the data from the IBL database, we observed that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section 9.3.2 and Supplemental Figure S7). This provided further evidence for the hypothesis that internal mode processing is driven by an enhanced impact of perceptual history, as opposed to increased choice randomness.   


### Comment 3

**From a mechanistic (sequential sampling) perspective, several previous papers have examined whether choice history biases influence the starting point or the drift rate of the evidence accumulation process. Under the former formulation, reliance on the evidence vs. reliance on the previous choice will be naturally anti-correlated (the less weight you place on the evidence the more impactful the choice history will be, assuming that the last choice is represented as a starting point bias). This seems to be mapping onto the computational model the authors describe, in which there is a weight on the prior, a weight on the likelihood and the assumption that these weights fluctuate in anti-phase. It is not obvious that this anti-phase relationship needs to be imposed ad-hoc. Or whether it would emerge naturally (using a mechanistic or Bayesian framework). More generally, the authors assert that without an external mechanism prior biases would be impossible to overcome, and this would misfit the data. However, it would be important to a) actually show that the results cannot be explained by a single mechanism in which the anti-phase relationship is emergent rather than ad-hoc, b) relate the current framework with previous mechanistic considerations of serial choice biases.**

We would like to thank the reviewer for pointing this out. We agree that both normative Bayesian and mechanistic drift diffusion are bound to lead to anti-correlated effects of sensory information and perceptual history *at the level of individual trials*. This, however, does not necessarily entail slow fluctuations in the impact of sensory information and perceptual history that evolve *over many consecutive trials*. We now provide a systematic model comparison and discuss our model in relation to drift diffusion models and descriptive models that assume slow changes in the latent parameters underlying perceptual decision-making (such as Roy et al., Neuron 2021 or Ashwood et al. Nature Neuroscience et al. 2022). We also discuss the ad-hoc nature of the bimodal inference model in the subsection *Limitations and open questions*. 

- We used a maximum likelihood procedure to fit the bimodal inference model (M1, Figure 1F) to the behavioral data from the Confidence database[@Rahnev2020] and the IBL database[@IBL2021], optimizing the parameters $\alpha$, $H$, $amp_{LLR}$, $amp_{\psi}$, $f$, $p$, and $\zeta$ (see Methods for details and Supplemental Table T2 for a summary of the parameters of the bimodal inference model). We validated our model in three steps: First, to show that bimodal inference does not emerge spontaneously in normative Bayesian models of evidence accumulation, but requires the ad-hoc addition of anti-phase oscillations in prior and likelihood precision, we compared the bimodal inference model to four control models (M2-5, Figure 1G). In these models, we successively removed the anti-phase oscillations (M2-M4) and the integration of information across trials (M5) from the bimodal inference model and performed a model comparison based on AIC. 

- Model M2 ($AIC_2$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "2: likelihood-oscillation-only model",]$AIC`$ in mice) and Model M3 ($AIC_3$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "3: prior-oscillation-only model",]$AIC`$ in mice) incorporated only oscillations of either likelihood or prior precision. Model M4 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice) lacked any oscillations of likelihood and prior precision and corresponded to the normative model proposed by Glaze et al.[@Glaze2015]. In model M5 ($AIC_4$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "5: no-evidence-accumulation",]$AIC`$ in mice), we furthermore removed the integration of information across trials, such that perception depended only in incoming sensory information (Figure 1G).

- The bimodal inference model achieved the lowest AIC across the full model space ($AIC_1$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC`$ in mice) and was clearly superior to the normative Bayesian model of evidence accumulation ($\delta_{AIC}$ = $`r Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "humans" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in humans and $`r Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "1: Bimodal inference",]$AIC - Sum_AIC_Models[Sum_AIC_Models$species == "mice" & Sum_AIC_Models$type == "4: normative-evidence-accumulation",]$AIC`$ in mice; Supplemental Figure S9).

- In sum, computational modeling suggested that between-mode fluctuations are best explained by two interlinked processes (Figure 1E): (i), the dynamic accumulation of information across successive trials mandated by normative Bayesian models of evidence accumulation and, (ii), ongoing anti-phase oscillations in the impact of external and internal information.

- Could bimodal inference emerge spontaneously in normative models of perceptual decision-making? In predictive processing, the relative precision of prior and likelihood determines their integration into the posterior that determines the content of perception. At the level of individual trials, the perceptual impact of internal predictions generated from perceptual history (prior precision) and external sensory information (likelihood precision) are thus necessarily anti-correlated. The same holds for mechanistic models of drift diffusion, which understand choice history biases as driven by changes in the starting point[@Glaze2015] or the drift rate of evidence accumulation[@Urai2019]. Under the former formulation, perceptual history is bound to have a stronger influence on perception when less weight is given to incoming sensory evidence, assuming that the last choice is represented as a starting point bias. The effects of choice history in normative Bayesian and mechanistic drift diffusion models can be mapped onto one another via the Bayesian formulation of drift diffusion[@Bitzer2014], where the inverse of likelihood precision determines the amount of noise in the accumulation of new evidence, and prior precision determines the absolute shift in its starting point[@Bitzer2014].

- While it is thus clear that the impact of perceptual history and sensory evidence are anti-correlated *at each individual trial*, we here introduce anti-phase oscillations as an ad-hoc modification to model slow fluctuations in prior and likelihood precision that evolve *over many consecutive trials* and are not mandated by normative Bayesian or mechanistic drift diffusion models. The bimodal inference model provides a reasonable explanation of the linked autocorrelations in stimulus- and history-congruence, as evidenced by formal model comparison, successful prediction of RTs and confidence as out-of-training variables, and a qualitative reproduction of our empirical data from posterior model parameter as evidence against over- or under-fitting. 

- Of note, similar non-stationarities have been observed in descriptive models that assume continuous[@Roy2021] or discrete[@Ashwood2022] changes in the latent states that modulate perceptual decision-making at slow timescales. A recent computational study[@Ashwood2021] has used a Hidden Markov model to investigate perceptual decision-making in the IBL database[@IBL2021]. In analogy to our findings, the authors observed that mice switch between temporally extended *strategies* that last for more than 100 trials: During *engaged* states, perception was highly sensitive to external sensory information. During *disengaged* states, in turn, choice behavior was prone to errors due to enhanced biases toward one of the two perceptual outcomes[@Ashwood2021]. Despite the conceptual differences to our approach (discrete states in a Hidden Markov model that correspond to switches between distinct decision-making strategies[@Ashwood2021] vs. gradual changes in mode that emerge from sequential Bayesian inference and ongoing fluctuations in the impact of external relative to internal information), it is tempting to speculate that engaged/disengaged states and between-mode fluctuations might tap into the same underlying phenomenon.

- A third open question concerns the computational underpinnings of bimodal inference. The addition of slow anti-phase oscillations to the integration of prior and likelihood represents an ad-hoc modification of a normative Bayesian model of evidence accumulation[@Glaze2015]. While the bimodal inference model is supported by formal model comparison, the successful prediction of out-of-training variables and the qualitative reproduction of our empirical data in simulations from posterior model parameters, it is an important task for future research to test (i), whether between-mode fluctuations can emerge spontaneously in hierarchical models of Bayesian inference, (ii), whether modes are continuous[@Weilnhammer2021a] or discrete[@Ashwood2021], and (iii), whether bimodal inference can be causally manipulated by experimental variables. We speculate that between-mode fluctuations may separate the perceptual contribution of internal predictions and external sensory data in time, creating unambiguous learning signals that benefit inference about the precision of prior and likelihood, respectively. This proposition should be tested empirically by relating the phenomenon of bimodal inference to performance in, e.g., reversal learning, probabilistic reasoning, or metacognition.

### Comment 4

**The authors need to unpack their definition of history biases since in previous work biases due to the response or the identity of the stimulus at the previous trial are treated differently. Here, the authors focus on response biases but it is not clear whether they could examine also stimulus-driven history biases (in paradigms where stimulus-response is remapped on each trial).**

We would like to thank the reviewer for raising this important point. We defined the history-biases reported in our main manuscript by comparing the *response about the perceived* stimulus category (A vs. B) at the current and at the preceding trial (choice history). An alternative would have been to define history biases by comparing the choice at the current trial to the stimulus category *presented* at the preceding trial (stimulus history). As we show below, perceptual responses tended to be biased not only toward choice history, but also (but to a lesser degree) to stimulus history. This is expected, as perception was *stimulus-congruent* on approximately 75% of trials, causing the effects of the preceding response and the preceding stimulus to be highly correlated. We therefore compared the effects on choice history and stimulus history induced by trials at which perception was *stimulus-incongruent*, since those trials lead to opposite predictions regarding the perceptual choice at the subsequent trial. 

As expected, perceptual choices were attracted toward perceptual choices at preceding stimulus-incongruent trials (i.e., a positive effect of choice history). By contrast, perceptual choices tended to be repelled away from the stimulus presented the preceding trial. This repulsion of choices away from stimuli presented at stimulus-incongruent trials confirms that the choices at stimulus-incongruent trials were the primary driver of serial effects in perception in both humans and mice.

We now refer to our analysis on stimulus history as an additional confound, which we present in detail in the Supplement 9.1 of our manuscript:

- (...) These serial biases were effects of choice history, i.e., driven by the experiences reported at the preceding trial, and could not be attributed to stimulus history, i.e., to effects of the stimuli presented at the preceding trial (Supplemental Section 9.1).

Supplement:

- The main manuscript reports the effects of perceptual history, which we defined as the impact of the choice at the preceding trial on the choice at the current trial (henceforth *choice history*). *Stimulus history*, which is defined as the impact of the stimulus presented at the preceding trial on the choice at the present trial, represents an alternative approach to this. Here, we compare the effects of choice history to the effects of stimulus history. 

- We observed a significant bias toward stimulus history (humans: `r mean_Stimulus_History`% ± `r  error_Stimulus_History`% of trials, $\beta$ = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,1]`$ ± $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,2]`$, T($`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,3]`$) = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,4]`$, p = $`r STAT.Global_Stimulus_History_Accuracy$coefficients[1,5]`$; mice: `r M_mean_Stimulus_History`% ± `r  M_error_Stimulus_History`% of trials,  T(`r M_STAT.Global_Stimulus_History_Accuracy$parameter`) = `r M_STAT.Global_Stimulus_History_Accuracy$statistic`, p = $`r M_STAT.Global_Stimulus_History_Accuracy$p.value`$). The bias toward stimulus history was smaller than the bias toward choice history (humans: $\beta$ = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,1]`$ ± $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,2]`$, T($`r STAT.Difference_Stimulus_Choice_History$coefficients[1,3]`$) = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,4]`$, p = $`r STAT.Difference_Stimulus_Choice_History$coefficients[1,5]`$; mice: T(`r M_STAT.Difference_Stimulus_Choice_History$parameter`) = `r M_STAT.Difference_Stimulus_Choice_History$statistic`, p = $`r M_STAT.Difference_Stimulus_Choice_History$p.value`$).

- The attraction of choices toward both preceding choices and stimuli is expected, as perception was *stimulus-congruent* on approximately 75% of trials, causing choices and stimuli to be highly correlated. We therefore compared the effects of choice history and stimulus history after  *stimulus-incongruent* (i.e., *error*) trials, since those trials lead to opposite predictions regarding the perceptual choice at the subsequent trial.

- As expected from the findings presented in the main manuscript, perceptual choices were attracted toward perceptual choices when the inducing trial was stimulus-incongruent (i.e., a positive effect of choice history; humans: $\beta$ = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum_Choice_History$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum_Choice_History$coefficients[3,4]`$: mice: $\beta$ = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum_Choice_History$coefficients[3,4]`$). By contrast, perceptual choices tended to be repelled away from the stimulus presented at preceding stimulus-incongruent trial (i.e., a negative effect of stimulus history; humans: $\beta$ = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum_Stimulus_History$coefficients[3,4]`$: mice: $\beta$ = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,1]`$ ± $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,2]`$, z = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,3]`$, p = $`r M_STAT.full_logit_Sum_Stimulus_History$coefficients[3,4]`$). This repulsion of choices away from stimuli presented at stimulus-incongruent trials confirmed that choices (which are anti-correlated to stimuli at stimulus-incongruent trials) were the primary driver of attracting serial effects in perception.

- In sum, the above results suggest that, in both humans and mice, serial dependencies were better explained by the effects of choice history as opposed to the effects of stimulus history. This aligns with a result recently published for the IBL database, where mice were shown to follow an *action-kernel* as opposed to a *stimulus-kernel* model when integrating information across trials[@Findling2023].

### Comment 5

**Previous work, which the authors acknowledges in their Discussion (6.5), distinguishes repetitive history biases from alternating biases. For instance, in Braun, Urai & Donner (2018, JoN) participants are split into repetitive or alternating. Shouldn't the authors define the history bias in a similar fashion? The authors point out that attracting and repelling biases operate simultaneously across different timescales. However, this is not warranted given Braun et. al and other similar papers. It is not clear how this more nuanced definition of history bias would alter the conclusions.**

Our empirical results show that, on average, history biases tend to be repetitive (Figure 2A, Figure 3A, and the biases in the psychometric functions in the Supplemental Figure S6-7). In fact, only `r sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)` of the `r nrow(Study_Behav)` experiments we included from the Confidence database[@Rahnev2020] showed significant alternating biases (Supplemental Figure S1, please note that history-congruence was not used in the inclusion algorithm). However, this does not rule out the possibility that there are periods of alternating biases in the other experiments with net repeating effects. 

Importantly, our central finding of autocorrelation in history-congruence does not distinguish between alternating and repetitive history biases. In the plot below, we show autocorrelation curves for fluctuations in history-congruence for both alternation (hazard = 0.8) and repetition (hazard = 0.2). Both were simulated for 10 blocks of a random duration between 15 and 30 trials, interleaved with 10 blocks with no history biases (hazard = 0.5). This simulation illustrates that the autocorrelation of alternating and repeating biases is identical for symmetric pairs of hazard rates. The autocorrelation of history-congruence and the associated internal mode processing is therefore not tied to repeating biases, but accommodates alternating biases as well (which seem to be, on average and in our data, less frequent). We therefore did not separate alternating from repeating biases in our analysis of mode. 

We apologize for not having recognized this aspect in the previous version of the discussion. We have rewritten the paragraph on alternating and repeating biases in the discussion, referring to the plot below, which we have added to the Supplemental Materials. 

- Second, our results point to an attraction of perception toward preceding choices. Previous work has shown that perceptual decision-making is concurrently affected by both attractive and repulsive serial biases that operate on distinct time-scales and serve complementary functions for sensory processing[@Fritsche2017; @Gekas2019; @Fritsche2020]: Short-term attraction may serve the decoding of noisy sensory inputs and increase the stability of perception, whereas long-term repulsion may enable efficient encoding and sensitivity to change[@Fritsche2020]. In the data analyzed here, history biases tended to be repetitive (Figure 2A, Figure 3A, Supplemental Figure S6 and S7), and only `r sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)` of the `r nrow(Study_Behav)` experiments of the Confidence database[@Rahnev2020] showed significant alternating biases (Supplemental Figure S1). However, as we show in Supplemental Figure S14, fluctuations in both alternating and repeating history biases generate overlapping autocorrelation curves. Our analysis of between-mode fluctuations is therefore not tied exclusively to repeating biases, but accommodates alternating biases as well, such that both may lead to internally-biased processing and reduced sensitivity to external sensory information. Future work could apply our approach to paradigms that boost alternating as opposed to repeating biases, as this would help to better understand how repetition and alternation are linked in terms of their computational function and neural implementation[@Fritsche2020].

**Supplemental Figure S14**
```{r Supplemental_Figure_S14_print, cache = TRUE}
p_Sum_Rep_Alt_Sim
```

- **Supplemental Figure S14**. Autocorrelation of history-congruence of alternating and repeating biases. Here, we simulate the autocorrelation of history-congruence in `r n_participants` synthetic participants. In the repeating regime (blue), history-congruence fluctuated between 50% and 80% (blue) in interleaved blocks (10 blocks per condition with a random duration between 15 and 30 trials). In the alternation regime (red), history-congruence fluctuated between 50% and 20%. The resulting autocorrelation curves for history-congruence overlap, indicating that our analysis is able to accommodate both repeating and alternating biases.

### Comment 6

**The arousal hypothesis seems to be ruled out too easily, merely in the presence of a non-monotonic "state" vs. RT pattern. Arousal can have an inverted U-shaped effect on behavioral performance and recent paper has demonstrated a non-monotonic effect of tonic arousal (baseline pupil) on RTs and accuracy (https://www.biorxiv.org/content/10.1101/2023.07.28.550956.abstract). More generally, the RT and confidence analyses need to be complemented, perhaps by computational modeling using sequential sampling models, as these behavioral metrics have multiple mechanistic mappings (e.g., a fast RT might correspond to high SNR or an impulsive decisions driven by a starting point bias).**

We would like to thank the reviewer for this important point. Considering this Comment and the Comment 3 by Reviewer 1, we realize that the quadratic relationships between mode and RTs/confidence do not represent a convincing defensive analysis against the potential contributions of arousal to the phenomenon that we have identified as between-mode fluctuations. Rather, we now interpret the fluctuations of RTs/confidence with mode as indicative of a scenario in which between-mode fluctuations modulate a decision-variable that determines not only the perceptual choices, but also the speed and confidence at which they are made. 

Therefore, as a first response to this comment, we have re-phrased our assessment of RT and confidence in the following way:

Humans:

- The above results point to systematic fluctuations in the *decision variable*[@Kepecs2008] that determines perceptual choices, causing enhanced sensitivity to external stimulus information during external mode and increased biases toward preceding choices during internal mode. As such, fluctuations in mode should influence downstream aspects of behavior and cognition that operate on the perceptual decision variable[@Kepecs2008]. To test this hypothesis with respect to motor behavior and metacognition, we asked how bimodal inference relates to response times (RTs) and confidence reports. (...).

- (...) In sum, the above results indicate that reporting behavior and metacognition do not map linearly onto the mode of sensory processing. Rather, they suggest that slow fluctuations in the respective impact of external and internal information are most likely to affect perception at an early level of sensory analysis[@St.John-Saaltink2016; @Cicchini2021]. Such low-level processing may thus integrate perceptual history with external inputs into a decision variable[@Kepecs2008] that influences not only perceptual choices, but also the speed and confidence at which they are made. 

- In what follows, we probe alternative explanations for between-mode fluctuations, test for the existence of modes in mice, and propose a predictive processing model that explains fluctuations in mode ongoing shifts in the precision afforded to external sensory information relative to internal predictions driven by perceptual history.

Mice:

- The above results confirm that fluctuations between internally- and externally-biased modes generalize to perceptual decision-making in mice. As in humans, we hypothesized that bimodal inference modulates the decision variable[@Kepecs2008] that determines not only perceptual choices, but also downstream aspects of mouse behavior[@Kepecs2008]. (...). When fitting full and history-conditioned psychometric curves to the data from the IBL database, we observed that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section 9.3.2 and Supplemental Figure S7). Over time, the frequency of history-congruent choices increased alongside stimulus-congruence and speed of response as mice were exposed to the experiment, arguing against the proposition that biases toward perceptual history reflected an unspecific response strategy in mice who were not sufficiently trained on the IBL task.

Second, we have re-structured the section of defensive analyses, where we contrast the phenomenon that we identify as fluctuating modes in perception with stereotypical or random responses as evidence of low task engagement, and not specifically with attention or arousal, which we discuss below. To streamline the manuscript, we report those analyses in detail in the Supplement (see Comments 12 by Reviewer 1 and Comment 9 by Reviewer 3).

- The core assumption of bimodal inference - that ongoing changes in the sensitivity to external information are driven by internal predictions induced via perceptual history - needs to be contrasted against two alternative hypotheses: When making errors, observers may not engage with the task and respond stereotypically, i.e., exhibit stronger general biases toward one of the two potential outcomes, or simply choose randomly. Logistic regression confirmed that perceptual history made a significant contribution to perception ($\beta$ = $`r STAT.full_logit_Sum$coefficients[3,1]`$ ± $`r STAT.full_logit_Sum$coefficients[3,2]`$, z = $`r STAT.full_logit_Sum$coefficients[3,3]`$, p = $`r STAT.full_logit_Sum$coefficients[3,4]`$) over and above the ongoing stream of external sensory information ($\beta$ = $`r STAT.full_logit_Sum$coefficients[2,1]`$ ± $`r STAT.full_logit_Sum$coefficients[2,2]`$, z = $`r STAT.full_logit_Sum$coefficients[2,3]`$, p = $`r STAT.full_logit_Sum$coefficients[2,4]`$) and general response biases toward ($\beta$ = $`r STAT.full_logit_Sum$coefficients[1,1]`$ ± $`r STAT.full_logit_Sum$coefficients[1,2]`$, z = $`r STAT.full_logit_Sum$coefficients[1,3]`$, p = $`r STAT.full_logit_Sum$coefficients[1,4]`$). 

- When eliminating perceptual history as a predictor of individual choices at individual trials, AIC increased by $\delta_{AIC}$ = $`r -delta_AIC_log_reg_humans`$ (see Supplemental Figure S4A-B for parameter- and model-level inference at the level of individual observers). Likewise, when eliminating slow fluctuations in history-congruence as a predictor of slow fluctuations in stimulus-congruence across trials, we observed an increase in AIC by $\delta_{AIC}$ = $`r -delta_AIC_correlation_humans`$. These results provided model-level evidence against the null hypotheses that fluctuations in stimulus-congruence are driven exclusively by choice randomness or general response bias (see Supplemental Section 9.2 for an in-depth assessment of general response bias).

- To confirm that changes in the sensitivity to external information are indicative of internal mode processing, we estimated full and history-dependent psychometric curves during internal, external, and across modes. If, as we hypothesized, internal mode processing reflects an enhanced impact of perceptual history, one would expect a history-dependent increase in biases and lapses as well as a history-independent increase in threshold. Conversely, if internal mode processing were driven by random choices, one would expect a history-independent increase in lapses and threshold, and no change in bias. In line with our prediction, we found that internal mode processing was associated with a history-dependent increase in bias and lapse as well as a history-independent increase in threshold (Supplemental Section 9.3 and Supplemental Figure S6-7). This confirmed that internal mode processing is indeed driven by an enhanced impact of perceptual history.   

- In line with this, the quadratic relationship between mode and confidence (Figure 2J) suggested that biases toward internal information do not reflect a post-perceptual strategy or repeating preceding choices when the subjective confidence in the perceptual decision is low. Moreover, while responses became faster with longer exposure to the experiments of the Confidence database, the frequency of history-congruent choices increased over time, speaking against the proposition that participants may stereotypically repeat preceding choices when not yet familiar with the experimental task (see Supplemental Section). 

- Taken together, our results thus argue against recurring intervals of low task engagement, which may be signaled by stereotypical or random responses, as an alternative explanation for the phenomenon that we identify as bimodal inference.

Third, in response the Comment 3 by Reviewer 1, we now interpret the quadratic relationship of mode to RTs/confidence in the context of predictive processing views on attention[@Feldman2010]. Based on the Bayesian formulation of drift diffusion[@Bitzer2014], we propose that the effects of likelihood on prior precision on the decision variable and, consequently, on confidence and RTs can be translated into the mechanistic framework of drift diffusion. Specifically, Bitzer et al. relate likelihood precision to noise in the accumulation process, and prior precision to the amount of shift in the starting point[@Bitzer2014]. As a third response to this comment, we have re-written our discussion of the quadratic relationship of mode to RTs and Confidence, focusing on predictive processing models attention, which we relate to mechanistic drift diffusion models: 

- How does attention relate to between-mode fluctuations? According to predictive processing, attention corresponds to the precision afforded to the probability distributions that underlie perceptual inference[@Feldman2010]. As outlined above, between-mode fluctuations can be understood as ongoing shifts in the precision afforded to likelihood (*external mode*) and prior (*internal mode*), respectively. When the precision afforded to prior or likelihood increases, posterior precision increases, which leads to faster RTs and higher confidence. When defined from the perspective of predictive processing as the precision afforded to likelihood and prior[@Feldman2010], fluctuations in attention may thus provide a plausible explanation for the quadratic relationship of mode to RTs and confidence (Figure 2H and J; Figure 3I; Figure 4I). Such effects of attention in the predictive processing sense can be directly related to mechanistic drift diffusion models[@Bitzer2014], where both larger shifts in starting point (related to increased prior precision in internal mode) and lower noise in the accumulation of evidence (related to increased likelihood precision in external mode) may explain faster and more confident responses.

Given the correspondence between normative Bayesian and mechanistic drift diffusion models[@Bitzer2014], we believe that we would not gain additional insights into the role of arousal and additional potential confounds or causes of between-mode fluctuations by fitting drift diffusion models as an alternative class of behavioral models to our data. To understand the relation of arousal to the bimodal inference, we think that it is necessary to look at data beyond behavior, such as pupillometry, video tracking of response behavior, or neural data. While this is beyond the scope of the current manuscript, we plan to do these analyses in a follow-up paper, using data published by the IBL after this paper was submitted. The IBL data now contains eye tracking, video tracking of response behavior, and neuropixel recordings across the whole mouse brain[@IBL2023]. As a fourth response to this comment, we have re-written our discussion of attention and arousal:

- Outside of the predictive processing field, attention is often understood in the context of task engagement[@Matthews2010], which varies according to the availability of cognitive resources that are modulated by factors such as tonic arousal, familiarity with the task, or fatigue[@Matthews2010]. Our results suggest that internal mode processing cannot be completely reduced to intervals of low task engagement: In addition to shorter RTs and elevated confidence, choices during internal mode were not random or globally biased, but driven by perceptual history (Supplemental Figures S6-7). Moreover, our computational model identified the dominant timescale of between-mode fluctuations at 0.11 1/$N_{trials}$, which may be compatible with fluctuations in arousal[@McGinley2015], but is faster than to be expected for the development of task familiarity or fatigue. 

- However, in interpreting the impact of between-mode fluctuations on perceptual accuracy, speed of response and confidence, it is important to consider that global modulators such as tonic arousal are known to have non-linear effects on task performance[@Beerendonk2023]: In perceptual tasks, performance seems so be highest during mid-level arousal, whereas low- and high-level arousal lead to reduced accuracy and slower responses[@Beerendonk2023]. This contrasts with the effects of bimodal inference, where accuracy increases linearly as one moves from internal to external mode, and responses become faster at both ends of the mode spectrum. 

- Of note, high phasic arousal has been shown to suppress multi-domain biases in decision-making in humans and mice[@Gee2014; @Gee2017; @Gee2020], including the biases toward perceptual history[@Urai2017] that we implicate in internal mode processing. The increase in response speed and history congruence over time (Supplemental Section 9.4) may argue against insufficient training as an alternative explanation for internal mode processing, but may also taken as a sign of waning arousal. The multiple mechanistic mappings to RTs and confidence therefore warrant more direct measures of arousal (such as pupil size[@Gee2014; @McGinley2015b; @Urai2017; @Gee2020; @Gee2022; @Beerendonk2023], motor behavior[@McGinley2015b; @Gee2022], or neural data[@IBL2023]) to better delineate bimodal inference from fluctuations in global modulators of task performance.

### Comment 7

**In several analysis the authors present an effect and then show that this effects persists when key variables/ design aspects are also taken into account (see an example at around line 70). It makes more sense to present only one single analysis in which these key variables are controlled for. Results cannot be interpreted if they are spurious factors driving them so it is not clear why some of the results are presented in two versions ("uncontrolled" and "controlled" analyses).**

We apologize for this. We have updated our manuscript accordingly and have omitted, whenever possible, reports of uncontrolled analyses (highlighted in the main text). The most significant changes are summarized here: 

- Group-level autocorrelation curves in humans: In line with previous work[@Dehaene1993], we found that the probability of stimulus-congruence was not independent across successive trials: At the group level, stimulus-congruent perceptual choices were significantly autocorrelated for up to `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` trials (Figure 2B), controlling for task difficulty and the sequence of presented stimuli (Supplemental Figure 2A-B). (...) In close analogy to stimulus-congruence, we found history-congruence to be significantly autocorrelated for up to `r min(which(Summary_acf[Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` trials (Figure 2B), while controlling for task difficulty and the sequence of presented stimuli (Supplemental Figure 2A-B).

- Group-level autocorrelation curves in mice: At the group level, we found significant autocorrelations in both stimulus-congruence (`r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_Stimulus",]$p == "> 0.05")) - 1` consecutive trials) and history-congruence (`r min(which(M_Summary_acf[M_Summary_acf$Variable == "diff_acf_History",]$p == "> 0.05")) - 1` consecutive trials (Figure 3B), while controlling for the respective autocorrelation of task difficulty and external stimulation (Supplemental Figure 2C-D).

- Correlation between stimulus- and history-congruence in humans: When perceptual choices were less biased toward external information, participants relied more strongly on internal information acquired from perceptual history (and vice versa, $\beta$ = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$, **controlling for fluctuations in general response biases**; Supplemental Section 9.2).

- Correlation between stimulus- and history-congruence in mice: Fluctuations in the strength of history-congruent biases had a significant effect on stimulus-congruence ($\beta_1$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[2,5]`$) beyond the effect of ongoing changes in general response biases ($\beta_2$ = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,1]`$ ± $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,2]`$, T($`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,3]`$) = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,4]`$, p = $`r M_STAT.slider_History_Preferred_vs_Accuracy$coefficients[3,5]`$).

### Comment 8

**The central empirical finding is potentially important but is currently shadowed by more speculative sections/ discussions. For instance, the section on the adaptive merits of the computational model is relatively weaker compared to the empirical results. In particular, the model is simulated without feedback (whereas most experiments employ trial by trial feedback) and does not outperform the baseline model in accuracy but in other secondary metrics**.

We agree with the Reviewer (see also a similar suggestion by Reviewer 1 in Comment 9). We have removed the section 5.8 and the associated subsection of the discussion from our manuscript. We will develop a model on the potential function of between-mode fluctuations in a separate publication. 

**Minor comments:**

### Comment 9
**The amount of statistical analysis and results is often overwhelming. The authors could streamline the presentation better such that the main result is brought to the foreground. Currently the manuscript resembles a technical report.**

We apologize for this. From the Results, we have moved a number of sections to the Supplemental Materials to stream-line the manuscript (i.e., our analysis of general response biases [former section 5.4], the analysis of psychometric functions [former section 5.5], and the respective paragraphs on the IBL database). From the Discussion, we have removed the section on self-organized criticality (also following the suggestion of Reviewer 1). We have also moved more peripheral statistical results to the figure legends and the method section:

- We have moved the statistics on exponential decay in the autocorrelation to the figure legends of Figure 2B and 3B.

- We have shortened the description of logistic regression models that predict the stimulus- and history-congruence at the index trial from the stimulus- and history-congruence at the preceding trials.

- The section on mouse behavior now matches the logic and presentation of results in the human section.

We hope that these changes will increase the readability of our manuscript. 

### Comment 10
**Some typos or omissions may alter the meaning in various places. Indicatively, in lines 273, 439, 649.**

Thanks a lot, we have corrected these typos. 

\newpage
# References