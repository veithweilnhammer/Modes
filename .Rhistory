scaled_directed_mode = scale(directed_mode),
strength_mode = abs(scaled_directed_mode)
)
PwData$clear_RT = PwData$RT
PwData$clear_RT = exclude_3SD(PwData$clear_RT)
PwData$clear_Confidence = PwData$Confidence
PwData$clear_Confidence = exclude_3SD(PwData$clear_Confidence)
evaluate_human = TRUE
if (evaluate_human){
type_list = c(
"fit_glaze_osc_zeta_v1",
"fit_glaze_osc_zeta_v1_one_amp",
"fit_glaze_osc_zeta_v1_LLR_amp",
"fit_glaze_osc_zeta_v1_Prior_amp",
"fit_glaze_osc_zeta_v1_no_amp",
"fit_glaze_osc_zeta_v1_no_integration"
#"fit_glaze_osc_zeta_v1_random_walk_mouse"
)
# available models
source("./Functions/fit_glaze_osc_zeta_v1.R",
local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_one_amp.R",
local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_LLR_amp.R",
local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_Prior_amp.R",
local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_no_amp.R",
local = knitr::knit_global())
source("./Functions/fit_glaze_osc_zeta_v1_no_integration.R",
local = knitr::knit_global())
block_size = 10
Optim_eval = data.frame()
n_start = 2438
#for (id in unique(PwData$subject_id)) {
#for (id in unique(PwData$subject_id[PwData$subject_id > max(PwData$subject_id)/2])) {
for (id in unique(PwData$subject_id[PwData$subject_id >= n_start])) {
print(paste("human:", id, sep = " "))
Input_Data = PwData[PwData$subject_id == id, c("Stimulus", "Response", "clear_RT", "clear_Confidence")]
Input_Data = Input_Data[!is.na(Input_Data$Stimulus) &
!is.na(Input_Data$Response), ]
if (nrow(Input_Data)  > block_size) {
for (trial_idx in seq(from = 1, to = nrow(Input_Data) - block_size  + 1, by = block_size)) {
train =  Input_Data[seq(from = trial_idx,
to = trial_idx + block_size  - 1,
by = 1), ]
for (type_idx in c(1:length(type_list))) {
#print(type_list[type_idx])
if (type_list[type_idx] == "fit_glaze_osc_zeta_v1") {
## Starting parameters
par = c(0.01, 0.01, 1, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
lower = c(0.01, 0.01, 0, 0, 1 / 40, 0, 1)
upper = c(0.99, 20, 4, 4, 1 / 5, 2 * pi - pi / 1000, 10)
}
if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_one_amp") {
## Starting parameters
par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
}
if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_LLR_amp") {
## Starting parameters
par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
}
if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_Prior_amp") {
## Starting parameters
par = c(0.01, 0.01, 1,  1 / 20, runif (1, 0, 2 * pi - pi / 1000), 5)
lower = c(0.01, 0.01, 0, 1 / 40, 0, 1)
upper = c(0.99, 20, 4, 1 / 5, 2 * pi - pi / 1000, 10)
}
if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_amp") {
## Starting parameters
par = c(0.01, 0.01, 5)
lower = c(0.01, 0.01, 1)
upper = c(0.99, 20, 10)
}
if (type_list[type_idx] == "fit_glaze_osc_zeta_v1_no_integration") {
## Starting parameters
par = c(0.01, 5)
lower = c(0.01, 1)
upper = c(20, 10)
}
suppressWarnings({
add_O =  optimx(
par,
match.fun(type_list[type_idx]),
Input_Data = train,
lower = lower,
upper = upper
)
})
add_O$subject_id = id
add_O$block_id = trial_idx
add_O$trial_n = nrow(train)
add_O$free_param = length(par)
add_O$type = type_list[type_idx]
add_O$AIC = compute_AIC(
LL = -add_O$value,
K =  add_O$free_param,
n = add_O$trial_n,
correction = 0
)
add_O$cAIC = compute_AIC(
LL = -add_O$value,
K =  add_O$free_param,
n = add_O$trial_n,
correction = 1
)
Optim_eval = rbind(Optim_eval, add_O[, c('subject_id', 'block_id','value', 'AIC', 'type', 'free_param', 'trial_n', 'convcode')])
#Optim_eval = rbind(Optim_eval, add_O)
}
}
}
write.csv(Optim_eval, paste("./Results/Optim_eval_human_interim_min_", as.character(n_start), ".csv", sep = ""), row.names = FALSE)
Sum <-   ddply(Optim_eval,
.(type),
summarise,
error = mean(value, na.rm = TRUE))
print(Sum)
}
}
##
## Latex smallest numbers
##
# smallest_value = .Machine$double.xmin
# replace p = \(0\) with p < \(\ensuremath{2.2\times 10^{-308}}\)
knitr::opts_chunk$set(echo = FALSE,
message = FALSE,
warning = FALSE)
options(scipen = -1, digits = 2)
##
## Global settings: what to do in R markdown (compute primary statistics vs. load data from disc)
##
##
## ROOT
##
#root = "/home/veithweilnhammer/Desktop/Modes Data/"
#root = "E:/Modes Data/"
root = "C:/Users/Veith Weilnhammer/Downloads/"
##
## Load/save fully preprocessed data
##
load_summary_data = TRUE
save_summary_data = FALSE
##
## Human Data
##
collect_data = FALSE
n_permutations = 100
compute_slider = FALSE
sliding_window = 10
additional_autocorrelations = FALSE
load_data = FALSE
compute_logreg = FALSE
compute_Tw_LogReg = FALSE
compute_sine_wave_fit = FALSE
compute_power_spectra = FALSE
compute_group_acf = FALSE
compute_training_history = FALSE
compute_metacognitive_sensitivity = FALSE
extract_additional_Slider_Data = FALSE
compute_sliding_log = FALSE
##
## Mouse data
##
preprocess_mouse_data = FALSE
compute_slider_mouse_data = FALSE
filter_mouse_data = FALSE
compute_pretraining_data_mouse = FALSE
load_mouse_data = FALSE
apply_mouse_exclusion_criteria = FALSE
compute_mouse_group_acf = FALSE
compute_mouse_power_spectra = FALSE
compute_mouse_logreg = FALSE
compute_slider_History_Accuracy_lmer = FALSE
compute_mouse_Tw_LogReg = FALSE
mouse_compute_RT_Accuracy_History = FALSE
compute_mouse_training_history = FALSE
extract_mouse_additional_Slider_Data = FALSE
##
## Optimization
##
run_optim_human = FALSE
evaluate_human = FALSE
run_optim_mouse = FALSE
evaluate_mouse = FALSE
## generate output from fitted data
generate_output_human = FALSE
run_logreg_Confidence_mu_minus_1 = FALSE
generate_output_mouse = FALSE
run_mouse_logreg_mu_minus_1 = FALSE
##
## Simulation
##
# visualize bias
run_visualize_bias_sim = FALSE
run_visualize_alt_sim = FALSE
# Adaptive simulation
run_adaptive_simulation = FALSE
# Posterior Parameters
run_simulation = FALSE
compute_power_spectra_simulation = FALSE
run_control_simulation = FALSE
compute_simulation_control_group_acf = FALSE
visualize_circular_inference = FALSE
#### General Markdown Settings
library(pander)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)
library(knitcitations)
cleanbib()
cite_options(citation_format = "pandoc", check.entries = FALSE)
library(bibtex)
## load human data
Optim_eval = data.frame()
human_files = list.files(path='./Results', pattern='Optim_eval_human_')
for (load_idx in human_files){
Optim_eval = rbind(Optim_eval, read.csv(paste("./Results/", load_idx, sep = "")))
}
Optim_eval$species = "humans"
Optim_eval$unique_id = as.character(Optim_eval$subject_id * rand(1))
Optim_eval$study_id = NA
for (idx in unique(Optim_eval$subject_id)){
print(idx)
Optim_eval[Optim_eval$subject_id == idx,]$study_id =  Behav[Behav$subject_id == idx,]$study_id
}
## load human data
Optim_eval = data.frame()
human_files = list.files(path='./Results', pattern='Optim_eval_human_')
for (load_idx in human_files){
Optim_eval = rbind(Optim_eval, read.csv(paste("./Results/", load_idx, sep = "")))
}
Optim_eval$species = "humans"
Optim_eval$unique_id = as.character(Optim_eval$subject_id * rand(1))
Optim_eval$study_id = NA
for (idx in unique(Optim_eval$subject_id)){
Optim_eval[Optim_eval$subject_id == idx,]$study_id =  Behav[Behav$subject_id == idx,]$study_id
}
## load mouse data
Optim_eval_mice = data.frame()
mice_files = list.files(path='./Results', pattern='Optim_eval_mouse_')
for (load_idx in mice_files){
Optim_eval_mice = rbind(Optim_eval_mice, read.csv(paste("./Results/", load_idx, sep = "")))
}
Optim_eval_mice$species = "mice"
Optim_eval_mice$unique_id = as.character(Optim_eval_mice$subject_id * Optim_eval_mice$session_id * rand(1))
Optim_eval_mice$study_id = NA
## group and rename
Optim_eval = rbind(Optim_eval[,c("value", "type", "free_param", "trial_n", "species", "convcode", "study_id", "unique_id")], Optim_eval_mice[,c("value", "type", "free_param", "trial_n", "species", "convcode", "study_id", "unique_id")])
Optim_eval$type = gsub("_mouse", "", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_no_integration", "5: no-evidence-accumulation", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_no_amp", "4: normative-evidence-accumulation", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_Prior_amp", "3: prior-oscillation-only model", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_LLR_amp", "2: likelihood-oscillation-only model", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1_one_amp", "Model X: Only one amplitude", Optim_eval$type)
Optim_eval$type = gsub("fit_glaze_osc_zeta_v1" , "1: Bimodal inference", Optim_eval$type)
## summarize
Sum_AIC_Models <-   ddply(Optim_eval[Optim_eval$convcode == 0,],
.(species, type),
summarise,
median_error = median(value, na.rm = TRUE),
sum_error = sum(value, na.rm = TRUE),
k = mean(free_param),
AIC = 2*k + 2 * sum(value, na.rm = TRUE)
)
Sum_Subj_Model_AIC <-   ddply(Optim_eval[Optim_eval$convcode == 0,],
.(unique_id, type, species),
summarise,
median_error = median(value, na.rm = TRUE),
sum_error = sum(value, na.rm = TRUE),
k = mean(free_param),
AIC = 2*k + 2 * sum(value, na.rm = TRUE),
study_id = unique(study_id)
)
STAT.M1_M2 = summary(lmer(AIC ~ as.factor(type) + (1|study_id),
data = Sum_Subj_Model_AIC[(Sum_Subj_Model_AIC$type == "1: Bimodal inference" | Sum_Subj_Model_AIC$type == "2: likelihood-oscillation-only model") & Sum_Subj_Model_AIC$species == "humans", ]))
M_STAT.M1_M2 = t.test(Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type == "1: Bimodal inference" & Sum_Subj_Model_AIC$species == "mice", ]$AIC - Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type == "2: likelihood-oscillation-only model" & Sum_Subj_Model_AIC$species == "mice", ]$AIC)
print(Sum_AIC_Models)
p_distribution_Sum_Subj_Model_AIC <-
ggplot(Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type != "Model X: Only one amplitude", ],
aes(
x = round_any(AIC, 25),
y = ..density..,
color = type,
fill = type
)) +
geom_histogram(position = "dodge", alpha = 0.5) +
theme_classic(base_size = 6) + xlim(-10, max(Sum_Subj_Model_AIC[Sum_Subj_Model_AIC$type != "Model X: Only one amplitude", ]$AIC)) +
labs(x = "AIC (subject-level)", y = "Density", color = "Model", fill = "Model", subtitle = "B") +
scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +
theme(legend.position = "none") + facet_wrap( ~ species, ncol = 2)
p_distribution_Sum_Model_AIC <-
ggplot(Sum_AIC_Models[Sum_AIC_Models$type != "Model X: Only one amplitude", ],
aes(
y = type,
x = AIC,
color = type,
fill = type
)) +
geom_col(alpha = 0.5, width = 0.1) +
theme_classic(base_size = 6) +
scale_y_discrete(labels=c(" M1", " M2", " M3", " M4", " M5"))+
labs(x = "AIC (group-level)", y = "Model", color = "Model", fill = "Model", subtitle = "A") +
scale_color_brewer(palette = "Dark2", direction = +1) + scale_fill_brewer(palette = "Dark2", direction = +1) +
theme(legend.position = "top") + facet_wrap( ~ species, ncol = 2)
lay <- rbind(c(1), c(2))
grid.arrange(
p_distribution_Sum_Model_AIC,
p_distribution_Sum_Subj_Model_AIC,
layout_matrix = lay,
heights = c(0.75,1),
widths = c(1))
STAT.M1_M2
M_STAT.M1_M2
load("./Summary_Data/spectral_analysis_mouse.Rdata")
min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]$p_value > 0.05))
load("./Summary_Data/log_reg_autocorrelation_mouse.Rdata")
min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]$p_value > 0.05))
M_Tw_LogReg <- read.csv("./Results/M_Tw_LogReg_new_preproc_100back.csv")
gathercol = colnames(M_Tw_LogReg[, c(4, 6)])
M_Tw_LogReg_long <-
gather(M_Tw_LogReg[, c(1:4, 6)],
"Variable",
"beta",
gathercol,
factor_key = TRUE)
M_Tw_LogReg_long$Variable <-
gsub("History_weight",
"Weight: History-congruence",
M_Tw_LogReg_long$Variable)
M_Tw_LogReg_long$Variable <-
gsub("Stimulus_weight",
"Weight: Stimulus-congruence",
M_Tw_LogReg_long$Variable)
M_Group_Tw_LogReg_long <-  ddply(
M_Tw_LogReg_long,
.(lag, Variable),
summarise,
Mean = mean(exclude_3SD(beta), na.rm = TRUE),
Error = sd(exclude_3SD(beta), na.rm = TRUE) / sqrt(length(exclude_3SD(beta)))
)
## trial-wise stats
M_Group_Tw_LogReg_long$p_value = NA
for (lag_idx in c(1:n_back)) {
M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: History-congruence" &
M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
summary(lmer(exclude_3SD(History_weight) ~ 1 + (1 |
session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence" &
M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
summary(lmer(exclude_3SD(Stimulus_weight) ~ 1 + (1 |
session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
}
min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]$p_value > 0.05))
min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]
a
M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]
M_Tw_LogReg <- read.csv("./Results/M_Tw_LogReg_new_preproc_100back.csv")
head(M_Tw_LogReg)
n_back
n_back = 100
gathercol = colnames(M_Tw_LogReg[, c(4, 6)])
M_Tw_LogReg_long <-
gather(M_Tw_LogReg[, c(1:4, 6)],
"Variable",
"beta",
gathercol,
factor_key = TRUE)
M_Tw_LogReg_long$Variable <-
gsub("History_weight",
"Weight: History-congruence",
M_Tw_LogReg_long$Variable)
M_Tw_LogReg_long$Variable <-
gsub("Stimulus_weight",
"Weight: Stimulus-congruence",
M_Tw_LogReg_long$Variable)
M_Group_Tw_LogReg_long <-  ddply(
M_Tw_LogReg_long,
.(lag, Variable),
summarise,
Mean = mean(exclude_3SD(beta), na.rm = TRUE),
Error = sd(exclude_3SD(beta), na.rm = TRUE) / sqrt(length(exclude_3SD(beta)))
)
## trial-wise stats
M_Group_Tw_LogReg_long$p_value = NA
for (lag_idx in c(1:n_back)) {
M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: History-congruence" &
M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
summary(lmer(exclude_3SD(History_weight) ~ 1 + (1 |
session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence" &
M_Group_Tw_LogReg_long$lag == lag_idx, ]$p_value <-
summary(lmer(exclude_3SD(Stimulus_weight) ~ 1 + (1 |
session_id) , data = M_Tw_LogReg[M_Tw_LogReg$lag == lag_idx, ]))$coefficients[5]
}
min(which(M_Group_Tw_LogReg_long[M_Group_Tw_LogReg_long$Variable == "Weight: Stimulus-congruence",]$p_value > 0.05))
save(M_Group_Tw_LogReg_long,
M_Tw_LogReg_long,
M_Tw_LogReg,
M_Group_Tw_LogReg_long,
n_back,
file = "./Summary_Data/log_reg_autocorrelation_mouse.Rdata")
sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)/sum(!is.na(Study_Behav$study_id))
sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)
sum(Study_Behav$p < 0.05 & Study_Behav$estimate < 0, na.rm = TRUE)
nrow(Study_Behav)
uninstall.packages("ggplot2")
remove.packages("ggplot2")
devtools::install_version("ggplot2", "3.3.5")
devtools::install_version("ggplot2", "3.4.0")
##
## Latex smallest numbers
##
# smallest_value = .Machine$double.xmin
# replace p = \(0\) with p < \(\ensuremath{2.2\times 10^{-308}}\)
knitr::opts_chunk$set(echo = FALSE,
message = FALSE,
warning = FALSE)
options(scipen = -1, digits = 2)
##
## Global settings: what to do in R markdown (compute primary statistics vs. load data from disc)
##
##
## ROOT
##
#root = "/home/veithweilnhammer/Desktop/Modes Data/"
#root = "E:/Modes Data/"
root = "C:/Users/Veith Weilnhammer/Downloads/"
##
## Load/save fully preprocessed data
##
load_summary_data = TRUE
save_summary_data = FALSE
##
## Human Data
##
collect_data = FALSE
n_permutations = 100
compute_slider = FALSE
sliding_window = 10
additional_autocorrelations = FALSE
load_data = FALSE
compute_logreg = FALSE
compute_Tw_LogReg = FALSE
compute_sine_wave_fit = FALSE
compute_power_spectra = FALSE
compute_group_acf = FALSE
compute_training_history = FALSE
compute_metacognitive_sensitivity = FALSE
extract_additional_Slider_Data = FALSE
compute_sliding_log = FALSE
##
## Mouse data
##
preprocess_mouse_data = FALSE
compute_slider_mouse_data = FALSE
filter_mouse_data = FALSE
compute_pretraining_data_mouse = FALSE
load_mouse_data = FALSE
apply_mouse_exclusion_criteria = FALSE
compute_mouse_group_acf = FALSE
compute_mouse_power_spectra = FALSE
compute_mouse_logreg = FALSE
compute_slider_History_Accuracy_lmer = FALSE
compute_mouse_Tw_LogReg = FALSE
mouse_compute_RT_Accuracy_History = FALSE
compute_mouse_training_history = FALSE
extract_mouse_additional_Slider_Data = FALSE
##
## Optimization
##
run_optim_human = FALSE
evaluate_human = FALSE
run_optim_mouse = FALSE
evaluate_mouse = FALSE
## generate output from fitted data
generate_output_human = FALSE
run_logreg_Confidence_mu_minus_1 = FALSE
generate_output_mouse = FALSE
run_mouse_logreg_mu_minus_1 = FALSE
##
## Simulation
##
# visualize bias
run_visualize_bias_sim = FALSE
run_visualize_alt_sim = FALSE
# Adaptive simulation
run_adaptive_simulation = FALSE
# Posterior Parameters
run_simulation = FALSE
compute_power_spectra_simulation = FALSE
run_control_simulation = FALSE
compute_simulation_control_group_acf = FALSE
visualize_circular_inference = FALSE
#### General Markdown Settings
library(pander)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)
library(knitcitations)
cleanbib()
cite_options(citation_format = "pandoc", check.entries = FALSE)
library(bibtex)
library(kableExtra)
detach("package:bibtex", unload = TRUE)
library(ggplot2)
tinytex::install_tinytex()
library(xelatex)
install.packages(c("askpass", "datawizard", "effectsize", "insight", "knitr", "markdown", "Matrix", "minqa", "parameters", "performance", "rmarkdown"))
install.packages(c("askpass", "datawizard", "effectsize", "insight", "knitr", "markdown", "Matrix", "minqa", "parameters", "performance", "rmarkdown"))
install.packages(c("askpass", "datawizard", "effectsize", "insight", "knitr", "markdown", "Matrix", "minqa", "parameters", "performance", "rmarkdown"))
